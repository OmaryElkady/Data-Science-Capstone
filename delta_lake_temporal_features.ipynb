{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "037d2e9a",
   "metadata": {},
   "source": [
    "# Advanced Temporal Feature Engineering on Delta Lake Data\n",
    "\n",
    "This notebook demonstrates how to apply the same temporal feature engineering we did on CSV data, but using PySpark on Delta Lake tables. This approach is more scalable for large datasets and integrates with your existing data lake architecture.\n",
    "\n",
    "**Source Table:** `default.silver_flights_processed`\n",
    "**Output Table:** `default.gold_flights_temporal`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876b8312",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73462b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, when, count, isnan, to_date, dayofweek, dayofmonth, month, year,\n",
    "    weekofyear, date_format, datediff, expr, lit, array, array_contains,\n",
    "    udf, collect_list, explode\n",
    ")\n",
    "from pyspark.sql.types import StringType, IntegerType, DateType\n",
    "import holidays\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(\"âœ… PySpark and temporal feature engineering libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fc4416",
   "metadata": {},
   "source": [
    "## 2. Load Data from Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a4eed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Silver table from Delta Lake\n",
    "print(\"Loading Silver table from Delta Lake...\")\n",
    "df_silver = spark.table(\"default.silver_flights_processed\")\n",
    "\n",
    "print(f\"âœ… Loaded {df_silver.count():,} flights from Delta Lake\")\n",
    "print(f\"Columns: {len(df_silver.columns)}\")\n",
    "\n",
    "# Show schema\n",
    "print(\"\\nSchema:\")\n",
    "df_silver.printSchema()\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\nSample data:\")\n",
    "df_silver.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a21259",
   "metadata": {},
   "source": [
    "## 3. Apply Temporal Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3583142c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Applying temporal feature engineering...\")\n",
    "\n",
    "# First, ensure we have a proper date column\n",
    "df_temporal = df_silver.withColumn(\"flight_date\", to_date(col(\"FL_DATE\")))\n",
    "\n",
    "# Basic temporal features using PySpark SQL functions\n",
    "df_temporal = df_temporal.withColumn(\"day_of_week\", dayofweek(col(\"flight_date\"))) \\\n",
    "                       .withColumn(\"day_of_week_name\", date_format(col(\"flight_date\"), \"EEEE\")) \\\n",
    "                       .withColumn(\"week_of_year\", weekofyear(col(\"flight_date\"))) \\\n",
    "                       .withColumn(\"is_weekend\", when(col(\"day_of_week\").isin([1, 7]), 1).otherwise(0))\n",
    "\n",
    "print(\"âœ… Basic temporal features added\")\n",
    "\n",
    "# Create holiday detection UDFs\n",
    "def get_us_holidays_for_years(years):\n",
    "    \"\"\"Get all US holidays for given years\"\"\"\n",
    "    all_holidays = {}\n",
    "    for year in years:\n",
    "        us_holidays = holidays.US(years=year)\n",
    "        for date, name in us_holidays.items():\n",
    "            if any(keyword in name.upper() for keyword in [\n",
    "                'NEW YEAR', 'MARTIN LUTHER KING', 'WASHINGTON', 'MEMORIAL',\n",
    "                'INDEPENDENCE', 'LABOR', 'COLUMBUS', 'VETERANS', 'THANKSGIVING', 'CHRISTMAS'\n",
    "            ]):\n",
    "                all_holidays[date.strftime('%Y-%m-%d')] = name\n",
    "    return all_holidays\n",
    "\n",
    "# Get unique years from the data\n",
    "years_list = df_temporal.select(year(col(\"flight_date\"))).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "print(f\"Years in dataset: {sorted(years_list)}\")\n",
    "\n",
    "# Get holidays for all years\n",
    "holidays_dict = get_us_holidays_for_years(years_list)\n",
    "print(f\"Total holidays identified: {len(holidays_dict)}\")\n",
    "\n",
    "# Create broadcast variable for holidays (efficient for distributed processing)\n",
    "holidays_broadcast = spark.sparkContext.broadcast(holidays_dict)\n",
    "\n",
    "# Create UDFs for holiday detection\n",
    "def is_near_holiday_udf(date_str):\n",
    "    \"\"\"Check if date is near a holiday (within 3 days)\"\"\"\n",
    "    if not date_str:\n",
    "        return 0\n",
    "\n",
    "    try:\n",
    "        date = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "        holidays_dict = holidays_broadcast.value\n",
    "\n",
    "        for holiday_str in holidays_dict.keys():\n",
    "            holiday_date = datetime.strptime(holiday_str, '%Y-%m-%d')\n",
    "            if abs((date - holiday_date).days) <= 3:\n",
    "                return 1\n",
    "        return 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def nearest_holiday_udf(date_str):\n",
    "    \"\"\"Get name of nearest holiday\"\"\"\n",
    "    if not date_str:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        date = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "        holidays_dict = holidays_broadcast.value\n",
    "\n",
    "        min_distance = float('inf')\n",
    "        nearest_holiday = None\n",
    "\n",
    "        for holiday_str, holiday_name in holidays_dict.items():\n",
    "            holiday_date = datetime.strptime(holiday_str, '%Y-%m-%d')\n",
    "            distance = abs((date - holiday_date).days)\n",
    "            if distance <= 3 and distance < min_distance:\n",
    "                min_distance = distance\n",
    "                nearest_holiday = holiday_name\n",
    "\n",
    "        return nearest_holiday\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def is_holiday_period_udf(date_str):\n",
    "    \"\"\"Check if date is in holiday period (Â±7 pre, +3 post)\"\"\"\n",
    "    if not date_str:\n",
    "        return 0\n",
    "\n",
    "    try:\n",
    "        date = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "        holidays_dict = holidays_broadcast.value\n",
    "\n",
    "        for holiday_str in holidays_dict.keys():\n",
    "            holiday_date = datetime.strptime(holiday_str, '%Y-%m-%d')\n",
    "            days_diff = (date - holiday_date).days\n",
    "            if -7 <= days_diff <= 3:\n",
    "                return 1\n",
    "        return 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def holiday_period_name_udf(date_str):\n",
    "    \"\"\"Get name of holiday in period\"\"\"\n",
    "    if not date_str:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        date = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "        holidays_dict = holidays_broadcast.value\n",
    "\n",
    "        for holiday_str, holiday_name in holidays_dict.items():\n",
    "            holiday_date = datetime.strptime(holiday_str, '%Y-%m-%d')\n",
    "            days_diff = (date - holiday_date).days\n",
    "            if -7 <= days_diff <= 3:\n",
    "                return holiday_name\n",
    "        return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def days_to_holiday_udf(date_str):\n",
    "    \"\"\"Get days to nearest holiday in period\"\"\"\n",
    "    if not date_str:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        date = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "        holidays_dict = holidays_broadcast.value\n",
    "\n",
    "        min_distance = float('inf')\n",
    "        for holiday_str in holidays_dict.keys():\n",
    "            holiday_date = datetime.strptime(holiday_str, '%Y-%m-%d')\n",
    "            days_diff = (date - holiday_date).days\n",
    "            if -7 <= days_diff <= 3 and abs(days_diff) < abs(min_distance):\n",
    "                min_distance = days_diff\n",
    "\n",
    "        return int(min_distance) if min_distance != float('inf') else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Register UDFs\n",
    "is_near_holiday_spark_udf = udf(is_near_holiday_udf, IntegerType())\n",
    "nearest_holiday_spark_udf = udf(nearest_holiday_udf, StringType())\n",
    "is_holiday_period_spark_udf = udf(is_holiday_period_udf, IntegerType())\n",
    "holiday_period_name_spark_udf = udf(holiday_period_name_udf, StringType())\n",
    "days_to_holiday_spark_udf = udf(days_to_holiday_udf, IntegerType())\n",
    "\n",
    "# Apply holiday UDFs\n",
    "date_str_col = date_format(col(\"flight_date\"), \"yyyy-MM-dd\")\n",
    "\n",
    "df_temporal = df_temporal.withColumn(\"date_str\", date_str_col) \\\n",
    "                       .withColumn(\"is_near_holiday\", is_near_holiday_spark_udf(col(\"date_str\"))) \\\n",
    "                       .withColumn(\"nearest_holiday\", nearest_holiday_spark_udf(col(\"date_str\"))) \\\n",
    "                       .withColumn(\"is_holiday_period\", is_holiday_period_spark_udf(col(\"date_str\"))) \\\n",
    "                       .withColumn(\"holiday_period_name\", holiday_period_name_spark_udf(col(\"date_str\"))) \\\n",
    "                       .withColumn(\"days_to_holiday\", days_to_holiday_spark_udf(col(\"date_str\"))) \\\n",
    "                       .drop(\"date_str\")\n",
    "\n",
    "print(\"âœ… Holiday features added using PySpark UDFs\")\n",
    "\n",
    "# Show sample of new features\n",
    "print(\"\\nSample of temporal features:\")\n",
    "df_temporal.select(\n",
    "    \"flight_date\", \"day_of_week\", \"day_of_week_name\", \"week_of_year\", \"is_weekend\",\n",
    "    \"is_near_holiday\", \"nearest_holiday\", \"is_holiday_period\", \"days_to_holiday\"\n",
    ").show(10)\n",
    "\n",
    "# Show statistics\n",
    "print(\"\\nTemporal feature statistics:\")\n",
    "df_temporal.select(\n",
    "    \"is_weekend\", \"is_near_holiday\", \"is_holiday_period\"\n",
    ").summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa3ae3e",
   "metadata": {},
   "source": [
    "## 4. Save Enhanced Data to Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df45093a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for the Gold table with temporal features\n",
    "GOLD_TEMPORAL_PATH = \"/Volumes/workspace/default/ds_capstone/gold/flights_temporal\"\n",
    "GOLD_TEMPORAL_TABLE_NAME = \"default.gold_flights_temporal\"\n",
    "DATABASE_NAME = \"default\"\n",
    "\n",
    "def path_exists(path):\n",
    "    \"\"\"Check if a path exists\"\"\"\n",
    "    try:\n",
    "        dbutils.fs.ls(path)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def create_directory_if_not_exists(path):\n",
    "    \"\"\"Create directory if it doesn't exist\"\"\"\n",
    "    if not path_exists(path):\n",
    "        dbutils.fs.mkdirs(path)\n",
    "        print(f\"âœ… Created directory: {path}\")\n",
    "    else:\n",
    "        print(f\"â„¹ï¸  Directory already exists: {path}\")\n",
    "\n",
    "# Check if Gold temporal path exists and clean if needed\n",
    "print(f\"\\nðŸ“ Checking Gold temporal path: {GOLD_TEMPORAL_PATH}\")\n",
    "if path_exists(GOLD_TEMPORAL_PATH):\n",
    "    print(f\"âš ï¸  Path already exists. Will overwrite existing table\")\n",
    "else:\n",
    "    print(f\"âœ… Path is clear, ready to create new table\")\n",
    "\n",
    "# Create parent directory if needed\n",
    "gold_temporal_parent = \"/\".join(GOLD_TEMPORAL_PATH.split(\"/\")[:-1])\n",
    "create_directory_if_not_exists(gold_temporal_parent)\n",
    "\n",
    "# Write Gold temporal Delta table\n",
    "print(f\"\\nðŸ’¾ Writing Gold temporal Delta table...\")\n",
    "try:\n",
    "    df_temporal.write.format(\"delta\").mode(\"overwrite\").save(GOLD_TEMPORAL_PATH)\n",
    "    print(f\"âœ… Delta table written to: {GOLD_TEMPORAL_PATH}\")\n",
    "    print(f\"âœ… Records written: {df_temporal.count():,}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ERROR: Could not write Delta table\")\n",
    "    print(f\"   Error: {str(e)}\")\n",
    "    print(f\"\\nðŸ’¡ Trying to clean and retry...\")\n",
    "    try:\n",
    "        dbutils.fs.rm(GOLD_TEMPORAL_PATH, recurse=True)\n",
    "        df_temporal.write.format(\"delta\").mode(\"overwrite\").save(GOLD_TEMPORAL_PATH)\n",
    "        print(f\"âœ… Successfully wrote Delta table after cleanup\")\n",
    "    except Exception as e2:\n",
    "        print(f\"âŒ Still failed: {str(e2)}\")\n",
    "        raise\n",
    "\n",
    "# Register Delta table\n",
    "print(f\"\\nðŸ“Œ Registering Delta table as: {GOLD_TEMPORAL_TABLE_NAME}\")\n",
    "try:\n",
    "    # Ensure database exists\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {DATABASE_NAME}\")\n",
    "    print(f\"âœ… Database '{DATABASE_NAME}' ready\")\n",
    "\n",
    "    # Drop table if it exists (to avoid conflicts)\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {GOLD_TEMPORAL_TABLE_NAME}\")\n",
    "    print(f\"   Dropped existing table (if any)\")\n",
    "\n",
    "    # Create managed table\n",
    "    df_for_table = spark.read.format(\"delta\").load(GOLD_TEMPORAL_PATH)\n",
    "    df_for_table.write.format(\"delta\").mode(\"overwrite\").saveAsTable(GOLD_TEMPORAL_TABLE_NAME)\n",
    "\n",
    "    print(f\"âœ… Table registered successfully as '{GOLD_TEMPORAL_TABLE_NAME}'!\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Could not create table with saveAsTable, trying alternative method...\")\n",
    "    try:\n",
    "        # Alternative: Create external table with explicit LOCATION\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {GOLD_TEMPORAL_TABLE_NAME}\n",
    "            USING DELTA\n",
    "            LOCATION '{GOLD_TEMPORAL_PATH}'\n",
    "        \"\"\")\n",
    "        print(f\"âœ… Table registered with LOCATION clause!\")\n",
    "    except Exception as e2:\n",
    "        print(f\"âš ï¸  Table registration failed: {str(e2)}\")\n",
    "        print(f\"ðŸ’¡ You can still access the data directly using:\")\n",
    "        print(f\"   spark.read.format('delta').load('{GOLD_TEMPORAL_PATH}')\")\n",
    "\n",
    "print(\"\n",
    "ðŸŽ‰ **SUCCESS!** Delta Lake temporal feature engineering completed!\"print(f\"ðŸ“Š **Final Dataset Summary:**\")\n",
    "print(f\"   - Total flights: {df_temporal.count():,}\")\n",
    "print(f\"   - Original features: {len(df_silver.columns)}\")\n",
    "print(f\"   - New temporal features: 9\")\n",
    "print(f\"   - Total features: {len(df_temporal.columns)}\")\n",
    "print(f\"   - Table location: {GOLD_TEMPORAL_TABLE_NAME}\")\n",
    "print(f\"   - Delta path: {GOLD_TEMPORAL_PATH}\")\n",
    "\n",
    "# Show final schema\n",
    "print(\"\n",
    "ðŸ“‹ **Final Schema:**\"df_temporal.printSchema()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
