{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7f05ebf-7daa-4dd9-84f2-bb8a008afbca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 4. ML Model Training (Distributed)\n",
    "\n",
    "**Purpose:** This notebook loads the pre-processed Gold ML table (`gold_ml_features_experimental`) and uses the distributed `pyspark.ml` library to train and evaluate classification models.\n",
    "\n",
    "**Method:**\n",
    "1.  Load the vectorized Gold table.\n",
    "2.  Split the data into training and test sets.\n",
    "3.  Define the ML models (e.g., Decision Tree, Random Forest) using `pyspark.ml`.\n",
    "4.  Run MLflow experiments to track results.\n",
    "5.  Save the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aebb8ebb-f667-4cca-ab78-13042d756e31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "TEMP_VOLUME_PATH = \"/Volumes/workspace/default/ds_capstone/_mlflow_temp\"\n",
    "\n",
    "# Create this directory if it doesn't exist\n",
    "try:\n",
    "    dbutils.fs.mkdirs(TEMP_VOLUME_PATH)\n",
    "    print(f\"✅ Created temporary directory: {TEMP_VOLUME_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Could not create directory (it may already exist): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d39e33c0-cf4f-463a-bed4-893017b2bb69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 1. Load Your ML-Ready Gold Table ---\n",
    "\n",
    "# The data is already vectorized and has a 'features' and 'label' column\n",
    "gold_table_name = \"default.gold_ml_features_experimental\"\n",
    "\n",
    "try:\n",
    "    df_ml = spark.table(gold_table_name)\n",
    "    print(f\"✅ Successfully loaded ML-ready table: {gold_table_name}\")\n",
    "    print(f\"Total rows: {df_ml.count():,}\")\n",
    "    \n",
    "    print(\"\\nSchema:\")\n",
    "    df_ml.printSchema()\n",
    "    \n",
    "    print(\"\\nSample data:\")\n",
    "    df_ml.show(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR: Could not load table '{gold_table_name}'.\")\n",
    "    print(f\"   Did you run the Gold ETL notebook first?\")\n",
    "    print(f\"   Error: {e}\")\n",
    "\n",
    "# --- 2. Split the Data ---\n",
    "# This is the standard, reliable way to split your data\n",
    "(training_data, test_data) = df_ml.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"\\nData split complete:\")\n",
    "print(f\"Training set: {training_data.count():,} rows\")\n",
    "print(f\"Test set:     {test_data.count():,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cdb5eee-0c6b-4cac-a9e2-7e5f9e52a6ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.ml.classification import (\n",
    "    DecisionTreeClassifier,\n",
    "    RandomForestClassifier,\n",
    "    GBTClassifier\n",
    ")\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from datetime import datetime\n",
    "\n",
    "# --- 1. Set up MLflow Experiment ---\n",
    "# (This can be in its own cell if you prefer)\n",
    "\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "experiment_name = \"/Users/kshitijmishra231@gmail.com/Flightmasters_Prediction_SparkML\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f\"MLflow experiment set to: {experiment_name}\")\n",
    "\n",
    "# --- 2. Define our Evaluator ---\n",
    "# We'll re-use this for all models.\n",
    "# It will calculate all these metrics at once.\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\" # We can ask for f1, accuracy, etc.\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69b59c5f-3098-4dac-82fb-c90fbd3c878e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 3. Experiment 1: Decision Tree ---\n",
    "print(\"\\n=== Experiment 1: Decision Tree ===\")\n",
    "with mlflow.start_run(run_name=f\"DecisionTree_{datetime.now().strftime('%Y%m%d_%H%M%S')}\") as run:\n",
    "    \n",
    "    # --- a. Configure and Train ---\n",
    "    dt = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"label\", maxDepth=5)\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"model_type\", \"DecisionTreeClassifier\")\n",
    "    mlflow.log_param(\"max_depth\", 5)\n",
    "\n",
    "    # Train the model\n",
    "    dt_model = dt.fit(training_data)\n",
    "\n",
    "    # --- b. Evaluate ---\n",
    "    predictions = dt_model.transform(test_data)\n",
    "    \n",
    "    # Get metrics\n",
    "    f1_score = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "    accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "    precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "    recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"f1_score\", f1_score)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "\n",
    "    # --- c. Log Model ---\n",
    "    mlflow.spark.log_model(dt_model, \"spark-model\", dfs_tmpdir=TEMP_VOLUME_PATH)\n",
    "    \n",
    "    print(f\"Decision Tree F1 Score: {f1_score:.4f}\")\n",
    "    print(f\"Run ID: {run.info.run_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22505fbf-fbf1-4cb6-a51f-74af4a984ab0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 4. Experiment 2: Random Forest ---\n",
    "print(\"\\n=== Experiment 2: Random Forest ===\")\n",
    "with mlflow.start_run(run_name=f\"RandomForest_{datetime.now().strftime('%Y%m%d_%H%M%S')}\") as run:\n",
    "    \n",
    "    # --- a. Configure and Train ---\n",
    "    rf = RandomForestClassifier(\n",
    "        featuresCol=\"features\", \n",
    "        labelCol=\"label\", \n",
    "        numTrees=100, \n",
    "        maxDepth=5\n",
    "    )\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"model_type\", \"RandomForestClassifier\")\n",
    "    mlflow.log_param(\"num_trees\", 100)\n",
    "    mlflow.log_param(\"max_depth\", 5)\n",
    "\n",
    "    # Train the model\n",
    "    rf_model = rf.fit(training_data)\n",
    "\n",
    "    # --- b. Evaluate ---\n",
    "    predictions = rf_model.transform(test_data)\n",
    "    \n",
    "    # Get metrics\n",
    "    f1_score = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "    accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "    precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "    recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"f1_score\", f1_score)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "\n",
    "    # --- c. Log Model ---\n",
    "    mlflow.spark.log_model(rf_model, \"spark-model\", dfs_tmpdir=TEMP_VOLUME_PATH)\n",
    "    \n",
    "    print(f\"Random Forest F1 Score: {f1_score:.4f}\")\n",
    "    print(f\"Run ID: {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09fe11a3-e73c-41f6-bde2-01192a573250",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 5. Experiment 3: Gradient Boosting (GBT) ---\n",
    "print(\"\\n=== Experiment 3: Gradient Boosting (GBT) ===\")\n",
    "with mlflow.start_run(run_name=f\"GBT_{datetime.now().strftime('%Y%m%d_%H%M%S')}\") as run:\n",
    "    \n",
    "    # --- a. Configure and Train ---\n",
    "    gbt = GBTClassifier(\n",
    "        featuresCol=\"features\", \n",
    "        labelCol=\"label\", \n",
    "        maxIter=10,\n",
    "        maxDepth=5\n",
    "    )\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"model_type\", \"GBTClassifier\")\n",
    "    mlflow.log_param(\"max_iter\", 10)\n",
    "    mlflow.log_param(\"max_depth\", 5)\n",
    "\n",
    "    # Train the model\n",
    "    gbt_model = gbt.fit(training_data)\n",
    "\n",
    "    # --- b. Evaluate ---\n",
    "    predictions = gbt_model.transform(test_data)\n",
    "    \n",
    "    # Get metrics\n",
    "    f1_score = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "    accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "    precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "    recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"f1_score\", f1_score)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "\n",
    "    # --- c. Log Model ---\n",
    "    mlflow.spark.log_model(gbt_model, \"spark-model\", dfs_tmpdir=TEMP_VOLUME_PATH)\n",
    "    \n",
    "    print(f\"GBT F1 Score: {f1_score:.4f}\")\n",
    "    print(f\"Run ID: {run.info.run_id}\")\n",
    "\n",
    "print(\"\\n✅ All experiments complete. Check the MLflow UI!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ML FLow with pyspark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
