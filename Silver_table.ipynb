{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0768220b-3df7-4dd7-9061-7bb786f70d16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ü•à Silver Layer: Cleanse and Transform Flights Data\n",
    "\n",
    "**Purpose:** This notebook takes the raw, unprocessed data from the Bronze table (`default.bronze_flights_data`) and transforms it into a clean, conformed, and enriched Silver table (`default.flights_processed`).\n",
    "\n",
    "The goal of this Silver table is to be the \"single source of truth\" for analysts. It will be de-duplicated, have correct data types, and include new, engineered features to make analytics easier.\n",
    "\n",
    "**Source Table:** `default.bronze_flights_data`\n",
    "**Output Table:** `default.flights_processed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "612205c7-43e4-4eac-bd64-24332cc3dbb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, month, year, isnan, when, count, upper, trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67c277a0-8991-4a2a-8f44-73f2bcc99dc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_bronze = spark.table(\"default.bronze_flights_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc1b94a8-1c6f-4348-8d0a-289dd5680a57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "column_count = len(df_bronze.columns)\n",
    "\n",
    "print(f\"The bronze DataFrame has {column_count} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ec21766-f084-4ef7-a364-26a4b5faf054",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"üìã Bronze Table Schema:\")\n",
    "df_bronze.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe8c1c70-72dd-4035-b83b-0d51a564ddc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List of columns to drop, as you provided\n",
    "columns_to_drop = [\n",
    "    \"AIRLINE_DOT\", \"DOT_CODE\", \"FL_NUMBER\", \"ORIGIN_CITY\", \"DEST_CITY\", \n",
    "    \"CRS_DEP_TIME\", \"DEP_TIME\", \"DEP_DELAY\", \"TAXI_OUT\", \"WHEELS_OFF\", \n",
    "    \"WHEELS_ON\", \"TAXI_IN\", \"CRS_ARR_TIME\", \"ARR_TIME\", \"CANCELLED\", \n",
    "    \"CANCELLATION_CODE\", \"DIVERTED\", \"CRS_ELAPSED_TIME\", \"ELAPSED_TIME\", \n",
    "    \"AIR_TIME\", \"DISTANCE\", \"DELAY_DUE_CARRIER\", \"DELAY_DUE_WEATHER\", \n",
    "    \"DELAY_DUE_NAS\", \"DELAY_DUE_SECURITY\", \"DELAY_DUE_LATE_AIRCRAFT\",\"bronze_ingestion_timestamp\"\n",
    "]\n",
    "\n",
    "df_silver = df_bronze.drop(*columns_to_drop)\n",
    "\n",
    "# 1. Print the new schema to see what's left\n",
    "print(\"üìã New Silver Table Schema (after dropping columns):\")\n",
    "df_silver.printSchema()\n",
    "\n",
    "# 2. Show a sample of the new DataFrame\n",
    "print(\"\\nüîé Sample data from the new Silver Table:\")\n",
    "df_silver.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b392ded0-738e-4d7d-838c-fd29c5e3a51f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "column_count = len(df_silver.columns)\n",
    "\n",
    "print(f\"The silver DataFrame has {column_count} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daf285c9-74ed-4d4e-a113-7f98b70bf2ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver_transformed = df_silver.withColumn(\"flight_date\", to_date(col(\"FL_DATE\")))\n",
    "\n",
    "# 2. Extract month and year into new columns\n",
    "df_silver_transformed = df_silver_transformed.withColumn(\"flight_month\", month(col(\"flight_date\")))\n",
    "df_silver_transformed = df_silver_transformed.withColumn(\"flight_year\", year(col(\"flight_date\")))\n",
    "\n",
    "# 3. Drop the original string column\n",
    "df_silver_transformed = df_silver_transformed.drop(\"FL_DATE\")\n",
    "\n",
    "# --- Let's verify the result ---\n",
    "print(\"üìã New Silver Table Schema (with date columns):\")\n",
    "df_silver_transformed.printSchema()\n",
    "\n",
    "df_silver = df_silver_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3357d0a-4a91-495e-9ffe-f1127b3d5ee7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_columns = df_silver.columns\n",
    "# Find just the float/double columns\n",
    "numeric_cols = [\n",
    "    c_name for (c_name, c_type) in df_silver.dtypes \n",
    "    if c_type in ('float', 'double')\n",
    "]\n",
    "\n",
    "# Get all *other* columns\n",
    "other_cols = [\n",
    "    c_name for c_name in all_columns \n",
    "    if c_name not in numeric_cols\n",
    "]\n",
    "\n",
    "# Create expressions for numeric columns (check for null OR nan)\n",
    "numeric_expressions = [\n",
    "    count(when(col(c).isNull() | isnan(c), c)).alias(c) \n",
    "    for c in numeric_cols\n",
    "]\n",
    "\n",
    "# Create expressions for all other columns (check for null only)\n",
    "other_expressions = [\n",
    "    count(when(col(c).isNull(), c)).alias(c) \n",
    "    for c in other_cols\n",
    "]\n",
    "\n",
    "# Combine the lists of expressions\n",
    "all_expressions = numeric_expressions + other_expressions\n",
    "\n",
    "# Run the counts and show the result\n",
    "print(\"Missing value counts per column:\")\n",
    "df_silver.select(*all_expressions).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87c97d8b-ef37-4af2-aa42-0ee3919352d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver= df_silver.fillna(0, subset=[\"ARR_DELAY\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b8a2297-2cef-4076-ab9b-99a95f35e95c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver_final = df_silver \\\n",
    "    .withColumnRenamed(\"AIRLINE\", \"airline_name\") \\\n",
    "    .withColumnRenamed(\"AIRLINE_CODE\", \"airline_code\") \\\n",
    "    .withColumn(\"airline_code\", trim(upper(col(\"airline_code\")))) \\\n",
    "    .withColumnRenamed(\"ORIGIN\", \"origin_airport_code\") \\\n",
    "    .withColumn(\"origin_airport_code\", trim(upper(col(\"origin_airport_code\")))) \\\n",
    "    .withColumnRenamed(\"DEST\", \"destination_airport_code\") \\\n",
    "    .withColumn(\"destination_airport_code\", trim(upper(col(\"destination_airport_code\"))))\\\n",
    "    .withColumnRenamed(\"ARR_DELAY\", \"arrival_delay\") \\\n",
    "\n",
    "# Let's check the final schema and data\n",
    "print(\"üìã Final Silver Table Schema:\")\n",
    "df_silver_final.printSchema()\n",
    "\n",
    "print(\"\\nüîé Final Silver Table Sample Data:\")\n",
    "df_silver_final.show(10)\n",
    "\n",
    "# Assign this to df_silver for the final save\n",
    "df_silver = df_silver_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee00ed52-b7dd-4745-ad8c-206d98223b72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def path_exists(path):\n",
    "    \"\"\"Check if a path exists\"\"\"\n",
    "    try:\n",
    "        dbutils.fs.ls(path)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def create_directory_if_not_exists(path):\n",
    "    \"\"\"Create directory if it doesn't exist\"\"\"\n",
    "    if not path_exists(path):\n",
    "        dbutils.fs.mkdirs(path)\n",
    "        print(f\"‚úÖ Created directory: {path}\")\n",
    "    else:\n",
    "        print(f\"‚ÑπÔ∏è  Directory already exists: {path}\")\n",
    "\n",
    "def table_exists(table_name):\n",
    "    \"\"\"Check if a table exists\"\"\"\n",
    "    try:\n",
    "        spark.table(table_name)\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4c6cdf3-3947-4305-9e5e-0a2ec48d07dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assert df_silver, \"The DataFrame 'df_silver' does not exist.\"\n",
    "\n",
    "# Define the paths for your new Silver table\n",
    "SILVER_PATH = \"/Volumes/workspace/default/ds_capstone/silver/flights_processed\"\n",
    "SILVER_TABLE_NAME = \"default.silver_flights_processed\"\n",
    "DATABASE_NAME = \"default\"\n",
    "\n",
    "\n",
    "assert DATABASE_NAME, \"DATABASE_NAME is not defined.\"\n",
    "\n",
    "print(f\"\\nüìÅ Checking Silver path: {SILVER_PATH}\")\n",
    "if path_exists(SILVER_PATH):\n",
    "    print(f\"‚ö†Ô∏è  Path already exists. Checking if it's a valid Delta table...\")\n",
    "    try:\n",
    "        # Try to read as Delta\n",
    "        test_df = spark.read.format(\"delta\").load(SILVER_PATH)\n",
    "        print(f\"‚úÖ Valid Delta table found with {test_df.count()} records\")\n",
    "        print(f\"üí° Will overwrite existing table\")\n",
    "    except:\n",
    "        print(f\"‚ö†Ô∏è  Path exists but is not a valid Delta table\")\n",
    "        print(f\"üßπ Cleaning up old data...\")\n",
    "        dbutils.fs.rm(SILVER_PATH, recurse=True)\n",
    "        print(f\"‚úÖ Old data removed\")\n",
    "else:\n",
    "    print(f\"‚úÖ Path is clear, ready to create new table\")\n",
    "\n",
    "# Create parent directory if needed\n",
    "silver_parent = \"/\".join(SILVER_PATH.split(\"/\")[:-1])\n",
    "create_directory_if_not_exists(silver_parent)\n",
    "\n",
    "print(f\"\\nüíæ Writing Silver Delta table...\")\n",
    "try:\n",
    "    df_silver.write.format(\"delta\").mode(\"overwrite\").save(SILVER_PATH)\n",
    "    print(f\"‚úÖ Delta table written to: {SILVER_PATH}\")\n",
    "    print(f\"‚úÖ Records written: {df_silver.count():,}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR: Could not write Delta table\")\n",
    "    print(f\"   Error: {str(e)}\")\n",
    "    print(f\"\\nüí° Trying to clean and retry...\")\n",
    "    try:\n",
    "        dbutils.fs.rm(SILVER_PATH, recurse=True)\n",
    "        df_silver.write.format(\"delta\").mode(\"overwrite\").save(SILVER_PATH)\n",
    "        print(f\"‚úÖ Successfully wrote Delta table after cleanup\")\n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Still failed: {str(e2)}\")\n",
    "        raise\n",
    "\n",
    "print(f\"\\nüìå Registering Delta table as: {SILVER_TABLE_NAME}\")\n",
    "try:\n",
    "    # Ensure database exists\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {DATABASE_NAME}\")\n",
    "    print(f\"‚úÖ Database '{DATABASE_NAME}' ready\")\n",
    "    \n",
    "    # Drop table if it exists (to avoid conflicts)\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {SILVER_TABLE_NAME}\")\n",
    "    print(f\"   Dropped existing table (if any)\")\n",
    "    \n",
    "    # Create managed table \n",
    "    # This reads the data you JUST wrote and saves it as a managed table\n",
    "    df_for_table = spark.read.format(\"delta\").load(SILVER_PATH)\n",
    "    df_for_table.write.format(\"delta\").mode(\"overwrite\").saveAsTable(SILVER_TABLE_NAME)\n",
    "    \n",
    "    print(f\"‚úÖ Table registered successfully as '{SILVER_TABLE_NAME}'!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not create table with saveAsTable, trying alternative method...\")\n",
    "    try:\n",
    "        # Alternative: Create external table with explicit LOCATION\n",
    "        # This just points the table name to the files you saved in Step 7\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {SILVER_TABLE_NAME}\n",
    "            USING DELTA\n",
    "            LOCATION '{SILVER_PATH}'\n",
    "        \"\"\")\n",
    "        print(f\"‚úÖ Table registered with LOCATION clause!\")\n",
    "    except Exception as e2:\n",
    "        print(f\"‚ö†Ô∏è  Table registration failed: {str(e2)}\")\n",
    "        print(f\"üí° You can still access the data directly using:\")\n",
    "        print(f\"   spark.read.format('delta').load('{SILVER_PATH}')\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver_table",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
