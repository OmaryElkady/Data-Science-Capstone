{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0768220b-3df7-4dd7-9061-7bb786f70d16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ü•à Silver Layer: Enhanced Data Engineering with Temporal Features\n",
    "\n",
    "**Purpose:** This notebook transforms Bronze data into a clean, enriched Silver table with advanced temporal features for comprehensive analytics and ML readiness.\n",
    "\n",
    "**Key Features:**\n",
    "- ‚úÖ Data cleaning and column standardization\n",
    "- ‚úÖ Advanced temporal features (day of week, holidays, seasons)\n",
    "- ‚úÖ US holiday detection system\n",
    "- ‚úÖ Weekend and seasonal classifications\n",
    "- ‚úÖ Single source of truth for all analytics\n",
    "\n",
    "**Pipeline:** Bronze (33 cols) ‚Üí **Enhanced Silver (15 cols)** ‚Üí Gold (ML)\n",
    "\n",
    "**Source Table:** `default.bronze_flights_data`\n",
    "**Output Table:** `default.silver_flights_processed` (Enhanced with temporal intelligence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46412f6a-74e3-4476-b362-49fb0e747c19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "612205c7-43e4-4eac-bd64-24332cc3dbb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Core PySpark imports\n",
    "# Holiday detection\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import holidays\n",
    "from pyspark.sql.functions import (  # Temporal feature functions\n",
    "    broadcast,\n",
    "    col,\n",
    "    avg,\n",
    "    count,\n",
    "    datediff,\n",
    "    dayofmonth,\n",
    "    dayofweek,\n",
    "    expr,\n",
    "    isnan,\n",
    "    lit,\n",
    "    month,\n",
    "    to_date,\n",
    "    trim,\n",
    "    upper,\n",
    "    weekofyear,\n",
    "    when,\n",
    "    year,\n",
    ")\n",
    "from pyspark.sql.types import BooleanType, DateType\n",
    "\n",
    "print(\"Extra Silver imports loaded (data cleaning + temporal feature engineering)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67c277a0-8991-4a2a-8f44-73f2bcc99dc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_bronze = spark.table(\"default.bronze_flights_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc1b94a8-1c6f-4348-8d0a-289dd5680a57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "column_count = len(df_bronze.columns)\n",
    "\n",
    "print(f\"The bronze DataFrame has {column_count} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ec21766-f084-4ef7-a364-26a4b5faf054",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"üìã Bronze Table Schema:\")\n",
    "df_bronze.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe8c1c70-72dd-4035-b83b-0d51a564ddc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List of columns to drop, as you provided\n",
    "columns_to_drop = [\n",
    "    \"AIRLINE_DOT\",\n",
    "    \"DOT_CODE\",\n",
    "    \"FL_NUMBER\",\n",
    "    \"ORIGIN_CITY\",\n",
    "    \"DEST_CITY\",\n",
    "    \"CRS_DEP_TIME\",\n",
    "    \"DEP_TIME\",\n",
    "    \"DEP_DELAY\",\n",
    "    \"TAXI_OUT\",\n",
    "    \"WHEELS_OFF\",\n",
    "    \"WHEELS_ON\",\n",
    "    \"TAXI_IN\",\n",
    "    \"CRS_ARR_TIME\",\n",
    "    \"ARR_TIME\",\n",
    "    \"CANCELLED\",\n",
    "    \"CANCELLATION_CODE\",\n",
    "    \"DIVERTED\",\n",
    "    \"CRS_ELAPSED_TIME\",\n",
    "    \"ELAPSED_TIME\",\n",
    "    \"AIR_TIME\",\n",
    "    \"DISTANCE\",\n",
    "    \"DELAY_DUE_CARRIER\",\n",
    "    \"DELAY_DUE_WEATHER\",\n",
    "    \"DELAY_DUE_NAS\",\n",
    "    \"DELAY_DUE_SECURITY\",\n",
    "    \"DELAY_DUE_LATE_AIRCRAFT\",\n",
    "    \"bronze_ingestion_timestamp\",\n",
    "]\n",
    "\n",
    "df_silver = df_bronze.drop(*columns_to_drop)\n",
    "\n",
    "# 1. Print the new schema to see what's left\n",
    "print(\"üìã New Silver Table Schema (after dropping columns):\")\n",
    "df_silver.printSchema()\n",
    "\n",
    "# 2. Show a sample of the new DataFrame\n",
    "print(\"\\nüîé Sample data from the new Silver Table:\")\n",
    "df_silver.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b392ded0-738e-4d7d-838c-fd29c5e3a51f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "column_count = len(df_silver.columns)\n",
    "\n",
    "print(f\"The silver DataFrame has {column_count} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daf285c9-74ed-4d4e-a113-7f98b70bf2ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver_data = df_silver.withColumn(\"flight_date\", to_date(col(\"FL_DATE\")))\n",
    "\n",
    "# 2. Extract month and year into new columns\n",
    "df_silver_data = df_silver_data.withColumn(\"flight_month\", month(col(\"flight_date\")))\n",
    "df_silver_data = df_silver_data.withColumn(\"flight_year\", year(col(\"flight_date\")))\n",
    "\n",
    "# 3. Drop the original string column\n",
    "df_silver_data = df_silver_data.drop(\"FL_DATE\")\n",
    "\n",
    "print(\"New Silver Table Schema (with date columns):\")\n",
    "df_silver_data.printSchema()\n",
    "\n",
    "df_silver = df_silver_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3357d0a-4a91-495e-9ffe-f1127b3d5ee7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_columns = df_silver.columns\n",
    "# Find just the float/double columns\n",
    "numeric_cols = [c_name for (c_name, c_type) in df_silver.dtypes if c_type in (\"float\", \"double\")]\n",
    "\n",
    "# Get all *other* columns\n",
    "other_cols = [c_name for c_name in all_columns if c_name not in numeric_cols]\n",
    "\n",
    "# Create expressions for numeric columns (check for null OR nan)\n",
    "numeric_expressions = [count(when(col(c).isNull() | isnan(c), c)).alias(c) for c in numeric_cols]\n",
    "\n",
    "# Create expressions for all other columns (check for null only)\n",
    "other_expressions = [count(when(col(c).isNull(), c)).alias(c) for c in other_cols]\n",
    "\n",
    "# Combine the lists of expressions\n",
    "all_expressions = numeric_expressions + other_expressions\n",
    "\n",
    "# Run the counts and show the result\n",
    "print(\"Missing value counts per column:\")\n",
    "df_silver.select(*all_expressions).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87c97d8b-ef37-4af2-aa42-0ee3919352d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_silver = df_silver.fillna(0, subset=[\"ARR_DELAY\"])\n",
    "# We will NOT be dropping missing values in the arrival delay column as they indicate that the flight was cancelled or otherwise did not arrive. We will handle these values when we make the columns we are predicting on in the Gold_table notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b8a2297-2cef-4076-ab9b-99a95f35e95c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 4: Clean and rename columns\n",
    "df_silver_clean = (\n",
    "    df_silver.withColumnRenamed(\"AIRLINE\", \"airline_name\")\n",
    "    .withColumnRenamed(\"AIRLINE_CODE\", \"airline_code\")\n",
    "    .withColumn(\"airline_code\", trim(upper(col(\"airline_code\"))))\n",
    "    .withColumnRenamed(\"ORIGIN\", \"origin_airport_code\")\n",
    "    .withColumn(\"origin_airport_code\", trim(upper(col(\"origin_airport_code\"))))\n",
    "    .withColumnRenamed(\"DEST\", \"destination_airport_code\")\n",
    "    .withColumn(\"destination_airport_code\", trim(upper(col(\"destination_airport_code\"))))\n",
    "    .withColumnRenamed(\"ARR_DELAY\", \"arrival_delay\")\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Basic data cleaning completed\")\n",
    "print(f\"Clean columns: {len(df_silver_clean.columns)}\")\n",
    "print(\"\\nüìã Clean Silver Schema:\")\n",
    "df_silver_clean.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be0875b7-e0ec-4dd6-901a-93180e7f9f18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üéÑ Advanced Temporal Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af1529c2-0d1b-4cc9-9055-c7c4948cec16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"üéÑ Creating US holiday detection system...\")\n",
    "\n",
    "# Get year range from data for holiday generation\n",
    "year_stats = df_silver_clean.agg({\"flight_year\": \"min\", \"flight_year\": \"max\"}).collect()[0]\n",
    "min_year_row = df_silver_clean.agg({\"flight_year\": \"min\"}).collect()[0]\n",
    "max_year_row = df_silver_clean.agg({\"flight_year\": \"max\"}).collect()[0]\n",
    "min_year, max_year = int(min_year_row[0]), int(max_year_row[0])\n",
    "print(f\"Data spans: {min_year} to {max_year}\")\n",
    "\n",
    "# Generate US holidays for all years in dataset\n",
    "all_holidays = []\n",
    "for year in range(min_year, max_year + 1):\n",
    "    year_holidays = holidays.UnitedStates(years=year)\n",
    "    all_holidays.extend(list(year_holidays.keys()))\n",
    "\n",
    "print(f\"‚úÖ Generated {len(all_holidays)} US federal holiday dates\")\n",
    "\n",
    "# Create holiday DataFrames for efficient joins\n",
    "holidays_df = spark.createDataFrame([(holiday_date,) for holiday_date in all_holidays], [\"holiday_date\"])\n",
    "\n",
    "# Create extended holiday periods for proximity detection\n",
    "near_holidays = []\n",
    "period_holidays = []\n",
    "\n",
    "for holiday in all_holidays:\n",
    "    # Near holiday (¬±3 days)\n",
    "    for offset in range(-3, 4):\n",
    "        near_holidays.append(holiday + timedelta(days=offset))\n",
    "\n",
    "    # Holiday period (¬±7 days)\n",
    "    for offset in range(-7, 8):\n",
    "        period_holidays.append(holiday + timedelta(days=offset))\n",
    "\n",
    "# Remove duplicates and create DataFrames\n",
    "near_holidays_df = spark.createDataFrame([(date,) for date in set(near_holidays)], [\"near_holiday_date\"])\n",
    "\n",
    "period_holidays_df = spark.createDataFrame([(date,) for date in set(period_holidays)], [\"period_holiday_date\"])\n",
    "\n",
    "print(f\"‚úÖ Holiday proximity periods created\")\n",
    "print(f\"Near-holiday dates: {len(set(near_holidays))}\")\n",
    "print(f\"Holiday-period dates: {len(set(period_holidays))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d842eae-9d6b-4e0a-a1d7-e0dc2ed831aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"üìÖ Adding comprehensive temporal features...\")\n",
    "\n",
    "# Step 1: Basic temporal features\n",
    "df_temporal = (\n",
    "    df_silver_clean.withColumn(\"day_of_week\", dayofweek(col(\"flight_date\")))\n",
    "    .withColumn(\"week_of_year\", weekofyear(col(\"flight_date\")))\n",
    "    .withColumn(\"day_of_month\", dayofmonth(col(\"flight_date\")))\n",
    "    .withColumn(\"is_weekend\", when((col(\"day_of_week\") == 1) | (col(\"day_of_week\") == 7), True).otherwise(False))\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Basic temporal features added\")\n",
    "\n",
    "# Step 2: Holiday detection using broadcast joins\n",
    "df_with_holidays = (\n",
    "    df_temporal.join(broadcast(holidays_df), col(\"flight_date\") == col(\"holiday_date\"), \"left\")\n",
    "    .withColumn(\"is_holiday\", when(col(\"holiday_date\").isNotNull(), True).otherwise(False))\n",
    "    .drop(\"holiday_date\")\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Holiday detection completed\")\n",
    "\n",
    "# Step 3: Holiday proximity features\n",
    "df_enhanced = (\n",
    "    df_with_holidays.join(broadcast(near_holidays_df), col(\"flight_date\") == col(\"near_holiday_date\"), \"left\")\n",
    "    .withColumn(\"is_near_holiday\", when(col(\"near_holiday_date\").isNotNull(), True).otherwise(False))\n",
    "    .drop(\"near_holiday_date\")\n",
    ")\n",
    "\n",
    "df_enhanced = (\n",
    "    df_enhanced.join(broadcast(period_holidays_df), col(\"flight_date\") == col(\"period_holiday_date\"), \"left\")\n",
    "    .withColumn(\"is_holiday_period\", when(col(\"period_holiday_date\").isNotNull(), True).otherwise(False))\n",
    "    .drop(\"period_holiday_date\")\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Holiday proximity features completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e80dc104-347b-467c-90a2-fc70b2ca51fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"üåø Adding seasonal and quarterly features...\")\n",
    "\n",
    "# Step 4: Seasonal features\n",
    "df_final_enhanced = df_enhanced.withColumn(\n",
    "    \"season\",\n",
    "    when(col(\"flight_month\").isin([12, 1, 2]), \"Winter\")\n",
    "    .when(col(\"flight_month\").isin([3, 4, 5]), \"Spring\")\n",
    "    .when(col(\"flight_month\").isin([6, 7, 8]), \"Summer\")\n",
    "    .when(col(\"flight_month\").isin([9, 10, 11]), \"Fall\")\n",
    "    .otherwise(\"Unknown\"),\n",
    ").withColumn(\n",
    "    \"quarter\",\n",
    "    when(col(\"flight_month\").isin([1, 2, 3]), 1)\n",
    "    .when(col(\"flight_month\").isin([4, 5, 6]), 2)\n",
    "    .when(col(\"flight_month\").isin([7, 8, 9]), 3)\n",
    "    .when(col(\"flight_month\").isin([10, 11, 12]), 4)\n",
    "    .otherwise(0),\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Seasonal features completed\")\n",
    "print(f\"Enhanced Silver columns: {len(df_final_enhanced.columns)}\")\n",
    "\n",
    "# Assign final DataFrame\n",
    "df_silver = df_final_enhanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b654dc0a-cf9d-4964-8cc9-b9bf489842af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Based on EDA, the distribution of arrival delay times for 2020 is anomalously low due to COVID-19 disruptions. These patterns do not generalize to other years and may distort model training. Therefore, we remove all flights with flight_year = 2020 from the silver table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "609d646e-c0eb-4285-8b98-2fc0e093fd13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optional Cell for dropping 2020 data from the dataset\n",
    "# --- Compute averages ---\n",
    "\n",
    "avg_2020 = (\n",
    "    df_silver.filter(col(\"flight_year\") == 2020)\n",
    "             .agg(avg(\"arrival_delay\").alias(\"avg_delay_2020\"))\n",
    "             .collect()[0][\"avg_delay_2020\"]\n",
    ")\n",
    "\n",
    "avg_non2020 = (\n",
    "    df_silver.filter(col(\"flight_year\") != 2020)\n",
    "             .agg(avg(\"arrival_delay\").alias(\"avg_delay_non2020\"))\n",
    "             .collect()[0][\"avg_delay_non2020\"]\n",
    ")\n",
    "\n",
    "avg_overall = (\n",
    "    df_silver.agg(avg(\"arrival_delay\").alias(\"avg_delay_overall\"))\n",
    "             .collect()[0][\"avg_delay_overall\"]\n",
    ")\n",
    "\n",
    "# --- Print results ---\n",
    "print(\"Average arrival delay for 2020:       \", round(avg_2020, 2))\n",
    "print(\"Average arrival delay for other years:\", round(avg_non2020, 2))\n",
    "print(\"Overall average arrival delay:        \", round(avg_overall, 2))\n",
    "\n",
    "# Drop all flights from 2020\n",
    "df_silver_no2020 = df_silver.filter(df_silver.flight_year != 2020)\n",
    "\n",
    "print(\"Original row count:\", df_silver.count())\n",
    "print(\"Row count after dropping 2020:\", df_silver_no2020.count())\n",
    "\n",
    "\n",
    "# Assign final DataFrame\n",
    "df_silver = df_silver_no2020\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7cba2b1f-ea6c-4b03-891d-79ee4fe2255a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìä Enhanced Silver Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f9a03e8-18c5-4f6c-871c-db20d32a87c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"üîç Validating Enhanced Silver table...\")\n",
    "\n",
    "# Show final schema\n",
    "print(\"\\nüìã Enhanced Silver Schema (15 columns):\")\n",
    "df_silver.printSchema()\n",
    "\n",
    "# Show sample with temporal features\n",
    "print(\"\\nüîé Sample Enhanced Silver Data:\")\n",
    "df_silver.select(\n",
    "    \"flight_date\",\n",
    "    \"airline_name\",\n",
    "    \"origin_airport_code\",\n",
    "    \"day_of_week\",\n",
    "    \"week_of_year\",\n",
    "    \"is_weekend\",\n",
    "    \"is_holiday\",\n",
    "    \"is_near_holiday\",\n",
    "    \"season\",\n",
    "    \"quarter\",\n",
    ").show(5, truncate=False)\n",
    "\n",
    "# Feature statistics\n",
    "print(\"\\nüìä Temporal Feature Statistics:\")\n",
    "total_flights = df_silver.count()\n",
    "weekend_flights = df_silver.filter(col(\"is_weekend\")).count()\n",
    "holiday_flights = df_silver.filter(col(\"is_holiday\")).count()\n",
    "near_holiday_flights = df_silver.filter(col(\"is_near_holiday\")).count()\n",
    "\n",
    "print(f\"Total flights: {total_flights:,}\")\n",
    "print(f\"Weekend flights: {weekend_flights:,} ({weekend_flights/total_flights*100:.1f}%)\")\n",
    "print(f\"Holiday flights: {holiday_flights:,} ({holiday_flights/total_flights*100:.1f}%)\")\n",
    "print(f\"Near holiday flights: {near_holiday_flights:,} ({near_holiday_flights/total_flights*100:.1f}%)\")\n",
    "\n",
    "# Column summary\n",
    "original_cols = [\n",
    "    \"airline_name\",\n",
    "    \"airline_code\",\n",
    "    \"origin_airport_code\",\n",
    "    \"destination_airport_code\",\n",
    "    \"arrival_delay\",\n",
    "    \"flight_date\",\n",
    "    \"flight_month\",\n",
    "    \"flight_year\",\n",
    "]\n",
    "temporal_cols = [\n",
    "    \"day_of_week\",\n",
    "    \"week_of_year\",\n",
    "    \"day_of_month\",\n",
    "    \"is_weekend\",\n",
    "    \"is_holiday\",\n",
    "    \"is_near_holiday\",\n",
    "    \"is_holiday_period\",\n",
    "    \"season\",\n",
    "    \"quarter\",\n",
    "]\n",
    "\n",
    "print(f\"\\n‚úÖ Enhanced Silver Success:\")\n",
    "print(f\"Original business columns: {len(original_cols)}\")\n",
    "print(f\"New temporal columns: {len(temporal_cols)}\")\n",
    "print(f\"Total columns: {len(df_silver.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee00ed52-b7dd-4745-ad8c-206d98223b72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def path_exists(path):\n",
    "    \"\"\"Check if a path exists\"\"\"\n",
    "    try:\n",
    "        dbutils.fs.ls(path)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def create_directory_if_not_exists(path):\n",
    "    \"\"\"Create directory if it doesn't exist\"\"\"\n",
    "    if not path_exists(path):\n",
    "        dbutils.fs.mkdirs(path)\n",
    "        print(f\"‚úÖ Created directory: {path}\")\n",
    "    else:\n",
    "        print(f\"‚ÑπÔ∏è  Directory already exists: {path}\")\n",
    "\n",
    "\n",
    "def table_exists(table_name):\n",
    "    \"\"\"Check if a table exists\"\"\"\n",
    "    try:\n",
    "        spark.table(table_name)\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4c6cdf3-3947-4305-9e5e-0a2ec48d07dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assert df_silver, \"The DataFrame 'df_silver' does not exist.\"\n",
    "\n",
    "# Define the paths for your new Silver table\n",
    "SILVER_PATH = \"/Volumes/workspace/default/ds-capstone/silver/flights_processed\" # This path was updated, as in the Bronze_table notebook, to use the convention \"ds-capstone\" rather than \"ds_capstone\". \n",
    "# The old path here was \"/Volumes/workspace/default/ds_capstone/silver/flights_processed\"\n",
    "SILVER_TABLE_NAME = \"default.silver_flights_processed\"\n",
    "DATABASE_NAME = \"default\"\n",
    "\n",
    "\n",
    "assert DATABASE_NAME, \"DATABASE_NAME is not defined.\"\n",
    "\n",
    "print(f\"\\nüìÅ Checking Silver path: {SILVER_PATH}\")\n",
    "if path_exists(SILVER_PATH):\n",
    "    print(f\"‚ö†Ô∏è  Path already exists. Checking if it's a valid Delta table...\")\n",
    "    try:\n",
    "        # Try to read as Delta\n",
    "        test_df = spark.read.format(\"delta\").load(SILVER_PATH)\n",
    "        print(f\"‚úÖ Valid Delta table found with {test_df.count()} records\")\n",
    "        print(f\"üí° Will overwrite existing table\")\n",
    "    except:\n",
    "        print(f\"‚ö†Ô∏è  Path exists but is not a valid Delta table\")\n",
    "        print(f\"üßπ Cleaning up old data...\")\n",
    "        dbutils.fs.rm(SILVER_PATH, recurse=True)\n",
    "        print(f\"‚úÖ Old data removed\")\n",
    "else:\n",
    "    print(f\"‚úÖ Path is clear, ready to create new table\")\n",
    "\n",
    "# Create parent directory if needed\n",
    "silver_parent = \"/\".join(SILVER_PATH.split(\"/\")[:-1])\n",
    "create_directory_if_not_exists(silver_parent)\n",
    "\n",
    "print(f\"\\nüíæ Writing Silver Delta table...\")\n",
    "try:\n",
    "    df_silver.write.format(\"delta\").mode(\"overwrite\").save(SILVER_PATH)\n",
    "    print(f\"‚úÖ Delta table written to: {SILVER_PATH}\")\n",
    "    print(f\"‚úÖ Records written: {df_silver.count():,}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR: Could not write Delta table\")\n",
    "    print(f\"   Error: {str(e)}\")\n",
    "    print(f\"\\nüí° Trying to clean and retry...\")\n",
    "    try:\n",
    "        dbutils.fs.rm(SILVER_PATH, recurse=True)\n",
    "        df_silver.write.format(\"delta\").mode(\"overwrite\").save(SILVER_PATH)\n",
    "        print(f\"‚úÖ Successfully wrote Delta table after cleanup\")\n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Still failed: {str(e2)}\")\n",
    "        raise\n",
    "\n",
    "print(f\"\\nüìå Registering Delta table as: {SILVER_TABLE_NAME}\")\n",
    "try:\n",
    "    # Ensure database exists\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {DATABASE_NAME}\")\n",
    "    print(f\"‚úÖ Database '{DATABASE_NAME}' ready\")\n",
    "\n",
    "    # Drop table if it exists (to avoid conflicts)\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {SILVER_TABLE_NAME}\")\n",
    "    print(f\"   Dropped existing table (if any)\")\n",
    "\n",
    "    # Create managed table\n",
    "    # This reads the data you JUST wrote and saves it as a managed table\n",
    "    df_for_table = spark.read.format(\"delta\").load(SILVER_PATH)\n",
    "    df_for_table.write.format(\"delta\").mode(\"overwrite\").saveAsTable(SILVER_TABLE_NAME)\n",
    "\n",
    "    print(f\"‚úÖ Table registered successfully as '{SILVER_TABLE_NAME}'!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not create table with saveAsTable, trying alternative method...\")\n",
    "    try:\n",
    "        # Alternative: Create external table with explicit LOCATION\n",
    "        # This just points the table name to the files you saved in Step 7\n",
    "        spark.sql(\n",
    "            f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {SILVER_TABLE_NAME}\n",
    "            USING DELTA\n",
    "            LOCATION '{SILVER_PATH}'\n",
    "        \"\"\"\n",
    "        )\n",
    "        print(f\"‚úÖ Table registered with LOCATION clause!\")\n",
    "    except Exception as e2:\n",
    "        print(f\"‚ö†Ô∏è  Table registration failed: {str(e2)}\")\n",
    "        print(f\"üí° You can still access the data directly using:\")\n",
    "        print(f\"   spark.read.format('delta').load('{SILVER_PATH}')\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver_table",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
