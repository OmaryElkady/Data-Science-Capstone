{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0768220b-3df7-4dd7-9061-7bb786f70d16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ü•à Silver Layer: Enhanced Data Engineering with Temporal Features\n",
    "\n",
    "**Purpose:** This notebook transforms Bronze data into a clean, enriched Silver table with advanced temporal features for comprehensive analytics and ML readiness.\n",
    "\n",
    "**Key Features:**\n",
    "- ‚úÖ Data cleaning and column standardization\n",
    "- ‚úÖ Advanced temporal features (day of week, holidays, seasons)\n",
    "- ‚úÖ US holiday detection system\n",
    "- ‚úÖ Weekend and seasonal classifications\n",
    "- ‚úÖ Single source of truth for all analytics\n",
    "\n",
    "**Pipeline:** Bronze (33 cols) ‚Üí **Enhanced Silver (15 cols)** ‚Üí Gold (ML)\n",
    "\n",
    "**Source Table:** `default.bronze_flights_data`\n",
    "**Output Table:** `default.silver_flights_processed` (Enhanced with temporal intelligence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "612205c7-43e4-4eac-bd64-24332cc3dbb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Core PySpark imports\n",
    "from pyspark.sql.functions import (\n",
    "    col, to_date, month, year, isnan, when, count, upper, trim,\n",
    "    # Temporal feature functions\n",
    "    dayofweek, weekofyear, dayofmonth, broadcast, lit, expr, datediff\n",
    ")\n",
    "from pyspark.sql.types import BooleanType, DateType\n",
    "\n",
    "# Holiday detection\n",
    "from datetime import datetime, timedelta\n",
    "import holidays\n",
    "\n",
    "print(\"‚úÖ Enhanced Silver imports loaded (data cleaning + temporal features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67c277a0-8991-4a2a-8f44-73f2bcc99dc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_bronze = spark.table(\"default.bronze_flights_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc1b94a8-1c6f-4348-8d0a-289dd5680a57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "column_count = len(df_bronze.columns)\n",
    "\n",
    "print(f\"The bronze DataFrame has {column_count} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ec21766-f084-4ef7-a364-26a4b5faf054",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"üìã Bronze Table Schema:\")\n",
    "df_bronze.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe8c1c70-72dd-4035-b83b-0d51a564ddc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List of columns to drop, as you provided\n",
    "columns_to_drop = [\n",
    "    \"AIRLINE_DOT\", \"DOT_CODE\", \"FL_NUMBER\", \"ORIGIN_CITY\", \"DEST_CITY\", \n",
    "    \"CRS_DEP_TIME\", \"DEP_TIME\", \"DEP_DELAY\", \"TAXI_OUT\", \"WHEELS_OFF\", \n",
    "    \"WHEELS_ON\", \"TAXI_IN\", \"CRS_ARR_TIME\", \"ARR_TIME\", \"CANCELLED\", \n",
    "    \"CANCELLATION_CODE\", \"DIVERTED\", \"CRS_ELAPSED_TIME\", \"ELAPSED_TIME\", \n",
    "    \"AIR_TIME\", \"DISTANCE\", \"DELAY_DUE_CARRIER\", \"DELAY_DUE_WEATHER\", \n",
    "    \"DELAY_DUE_NAS\", \"DELAY_DUE_SECURITY\", \"DELAY_DUE_LATE_AIRCRAFT\",\"bronze_ingestion_timestamp\"\n",
    "]\n",
    "\n",
    "df_silver = df_bronze.drop(*columns_to_drop)\n",
    "\n",
    "# 1. Print the new schema to see what's left\n",
    "print(\"üìã New Silver Table Schema (after dropping columns):\")\n",
    "df_silver.printSchema()\n",
    "\n",
    "# 2. Show a sample of the new DataFrame\n",
    "print(\"\\nüîé Sample data from the new Silver Table:\")\n",
    "df_silver.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b392ded0-738e-4d7d-838c-fd29c5e3a51f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "column_count = len(df_silver.columns)\n",
    "\n",
    "print(f\"The silver DataFrame has {column_count} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daf285c9-74ed-4d4e-a113-7f98b70bf2ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver_data = df_silver.withColumn(\"flight_date\", to_date(col(\"FL_DATE\")))\n",
    "\n",
    "# 2. Extract month and year into new columns\n",
    "df_silver_data = df_silver_data.withColumn(\"flight_month\", month(col(\"flight_date\")))\n",
    "df_silver_data = df_silver_data.withColumn(\"flight_year\", year(col(\"flight_date\")))\n",
    "\n",
    "# 3. Drop the original string column\n",
    "df_silver_data = df_silver_data.drop(\"FL_DATE\")\n",
    "\n",
    "# --- Let's verify the result ---\n",
    "print(\"üìã New Silver Table Schema (with date columns):\")\n",
    "df_silver_data.printSchema()\n",
    "\n",
    "df_silver = df_silver_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3357d0a-4a91-495e-9ffe-f1127b3d5ee7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_columns = df_silver.columns\n",
    "# Find just the float/double columns\n",
    "numeric_cols = [\n",
    "    c_name for (c_name, c_type) in df_silver.dtypes \n",
    "    if c_type in ('float', 'double')\n",
    "]\n",
    "\n",
    "# Get all *other* columns\n",
    "other_cols = [\n",
    "    c_name for c_name in all_columns \n",
    "    if c_name not in numeric_cols\n",
    "]\n",
    "\n",
    "# Create expressions for numeric columns (check for null OR nan)\n",
    "numeric_expressions = [\n",
    "    count(when(col(c).isNull() | isnan(c), c)).alias(c) \n",
    "    for c in numeric_cols\n",
    "]\n",
    "\n",
    "# Create expressions for all other columns (check for null only)\n",
    "other_expressions = [\n",
    "    count(when(col(c).isNull(), c)).alias(c) \n",
    "    for c in other_cols\n",
    "]\n",
    "\n",
    "# Combine the lists of expressions\n",
    "all_expressions = numeric_expressions + other_expressions\n",
    "\n",
    "# Run the counts and show the result\n",
    "print(\"Missing value counts per column:\")\n",
    "df_silver.select(*all_expressions).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87c97d8b-ef37-4af2-aa42-0ee3919352d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver= df_silver.fillna(0, subset=[\"ARR_DELAY\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b8a2297-2cef-4076-ab9b-99a95f35e95c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 4: Clean and rename columns\n",
    "df_silver_clean = df_silver \\\n",
    "    .withColumnRenamed(\"AIRLINE\", \"airline_name\") \\\n",
    "    .withColumnRenamed(\"AIRLINE_CODE\", \"airline_code\") \\\n",
    "    .withColumn(\"airline_code\", trim(upper(col(\"airline_code\")))) \\\n",
    "    .withColumnRenamed(\"ORIGIN\", \"origin_airport_code\") \\\n",
    "    .withColumn(\"origin_airport_code\", trim(upper(col(\"origin_airport_code\")))) \\\n",
    "    .withColumnRenamed(\"DEST\", \"destination_airport_code\") \\\n",
    "    .withColumn(\"destination_airport_code\", trim(upper(col(\"destination_airport_code\")))) \\\n",
    "    .withColumnRenamed(\"ARR_DELAY\", \"arrival_delay\")\n",
    "\n",
    "print(\"‚úÖ Basic data cleaning completed\")\n",
    "print(f\"Clean columns: {len(df_silver_clean.columns)}\")\n",
    "print(\"\\nüìã Clean Silver Schema:\")\n",
    "df_silver_clean.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéÑ Advanced Temporal Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéÑ Creating US holiday detection system...\")\n",
    "\n",
    "# Get year range from data for holiday generation\n",
    "year_stats = df_silver_clean.agg({\"flight_year\": \"min\", \"flight_year\": \"max\"}).collect()[0]\n",
    "min_year, max_year = int(year_stats[0]), int(year_stats[1])\n",
    "print(f\"Data spans: {min_year} to {max_year}\")\n",
    "\n",
    "# Generate US holidays for all years in dataset\n",
    "all_holidays = []\n",
    "for year in range(min_year, max_year + 1):\n",
    "    year_holidays = holidays.UnitedStates(years=year)\n",
    "    all_holidays.extend(list(year_holidays.keys()))\n",
    "\n",
    "print(f\"‚úÖ Generated {len(all_holidays)} US federal holiday dates\")\n",
    "\n",
    "# Create holiday DataFrames for efficient joins\n",
    "holidays_df = spark.createDataFrame(\n",
    "    [(holiday_date,) for holiday_date in all_holidays],\n",
    "    [\"holiday_date\"]\n",
    ")\n",
    "\n",
    "# Create extended holiday periods for proximity detection\n",
    "near_holidays = []\n",
    "period_holidays = []\n",
    "\n",
    "for holiday in all_holidays:\n",
    "    # Near holiday (¬±3 days)\n",
    "    for offset in range(-3, 4):\n",
    "        near_holidays.append(holiday + timedelta(days=offset))\n",
    "    \n",
    "    # Holiday period (¬±7 days)  \n",
    "    for offset in range(-7, 8):\n",
    "        period_holidays.append(holiday + timedelta(days=offset))\n",
    "\n",
    "# Remove duplicates and create DataFrames\n",
    "near_holidays_df = spark.createDataFrame(\n",
    "    [(date,) for date in set(near_holidays)],\n",
    "    [\"near_holiday_date\"]\n",
    ")\n",
    "\n",
    "period_holidays_df = spark.createDataFrame(\n",
    "    [(date,) for date in set(period_holidays)],\n",
    "    [\"period_holiday_date\"]\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Holiday proximity periods created\")\n",
    "print(f\"Near-holiday dates: {len(set(near_holidays))}\")\n",
    "print(f\"Holiday-period dates: {len(set(period_holidays))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìÖ Adding comprehensive temporal features...\")\n",
    "\n",
    "# Step 1: Basic temporal features\n",
    "df_temporal = df_silver_clean \\\n",
    "    .withColumn(\"day_of_week\", dayofweek(col(\"flight_date\"))) \\\n",
    "    .withColumn(\"week_of_year\", weekofyear(col(\"flight_date\"))) \\\n",
    "    .withColumn(\"day_of_month\", dayofmonth(col(\"flight_date\"))) \\\n",
    "    .withColumn(\"is_weekend\", \n",
    "        when((col(\"day_of_week\") == 1) | (col(\"day_of_week\") == 7), True).otherwise(False))\n",
    "\n",
    "print(\"‚úÖ Basic temporal features added\")\n",
    "\n",
    "# Step 2: Holiday detection using broadcast joins\n",
    "df_with_holidays = df_temporal.join(\n",
    "    broadcast(holidays_df),\n",
    "    col(\"flight_date\") == col(\"holiday_date\"),\n",
    "    \"left\"\n",
    ").withColumn(\n",
    "    \"is_holiday\",\n",
    "    when(col(\"holiday_date\").isNotNull(), True).otherwise(False)\n",
    ").drop(\"holiday_date\")\n",
    "\n",
    "print(\"‚úÖ Holiday detection completed\")\n",
    "\n",
    "# Step 3: Holiday proximity features\n",
    "df_enhanced = df_with_holidays.join(\n",
    "    broadcast(near_holidays_df),\n",
    "    col(\"flight_date\") == col(\"near_holiday_date\"),\n",
    "    \"left\"\n",
    ").withColumn(\n",
    "    \"is_near_holiday\",\n",
    "    when(col(\"near_holiday_date\").isNotNull(), True).otherwise(False)\n",
    ").drop(\"near_holiday_date\")\n",
    "\n",
    "df_enhanced = df_enhanced.join(\n",
    "    broadcast(period_holidays_df),\n",
    "    col(\"flight_date\") == col(\"period_holiday_date\"),\n",
    "    \"left\"\n",
    ").withColumn(\n",
    "    \"is_holiday_period\",\n",
    "    when(col(\"period_holiday_date\").isNotNull(), True).otherwise(False)\n",
    ").drop(\"period_holiday_date\")\n",
    "\n",
    "print(\"‚úÖ Holiday proximity features completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üåø Adding seasonal and quarterly features...\")\n",
    "\n",
    "# Step 4: Seasonal features\n",
    "df_final_enhanced = df_enhanced.withColumn(\n",
    "    \"season\",\n",
    "    when(col(\"flight_month\").isin([12, 1, 2]), \"Winter\")\n",
    "    .when(col(\"flight_month\").isin([3, 4, 5]), \"Spring\") \n",
    "    .when(col(\"flight_month\").isin([6, 7, 8]), \"Summer\")\n",
    "    .when(col(\"flight_month\").isin([9, 10, 11]), \"Fall\")\n",
    "    .otherwise(\"Unknown\")\n",
    ").withColumn(\n",
    "    \"quarter\",\n",
    "    when(col(\"flight_month\").isin([1, 2, 3]), 1)\n",
    "    .when(col(\"flight_month\").isin([4, 5, 6]), 2)\n",
    "    .when(col(\"flight_month\").isin([7, 8, 9]), 3)\n",
    "    .when(col(\"flight_month\").isin([10, 11, 12]), 4)\n",
    "    .otherwise(0)\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Seasonal features completed\")\n",
    "print(f\"Enhanced Silver columns: {len(df_final_enhanced.columns)}\")\n",
    "\n",
    "# Assign final DataFrame\n",
    "df_silver = df_final_enhanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Enhanced Silver Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Validating Enhanced Silver table...\")\n",
    "\n",
    "# Show final schema\n",
    "print(\"\\nüìã Enhanced Silver Schema (15 columns):\")\n",
    "df_silver.printSchema()\n",
    "\n",
    "# Show sample with temporal features\n",
    "print(\"\\nüîé Sample Enhanced Silver Data:\")\n",
    "df_silver.select(\n",
    "    \"flight_date\", \"airline_name\", \"origin_airport_code\", \n",
    "    \"day_of_week\", \"week_of_year\", \"is_weekend\", \"is_holiday\",\n",
    "    \"is_near_holiday\", \"season\", \"quarter\"\n",
    ").show(5, truncate=False)\n",
    "\n",
    "# Feature statistics\n",
    "print(\"\\nüìä Temporal Feature Statistics:\")\n",
    "total_flights = df_silver.count()\n",
    "weekend_flights = df_silver.filter(col(\"is_weekend\")).count()\n",
    "holiday_flights = df_silver.filter(col(\"is_holiday\")).count()\n",
    "near_holiday_flights = df_silver.filter(col(\"is_near_holiday\")).count()\n",
    "\n",
    "print(f\"Total flights: {total_flights:,}\")\n",
    "print(f\"Weekend flights: {weekend_flights:,} ({weekend_flights/total_flights*100:.1f}%)\")\n",
    "print(f\"Holiday flights: {holiday_flights:,} ({holiday_flights/total_flights*100:.1f}%)\")\n",
    "print(f\"Near holiday flights: {near_holiday_flights:,} ({near_holiday_flights/total_flights*100:.1f}%)\")\n",
    "\n",
    "# Column summary\n",
    "original_cols = [\"airline_name\", \"airline_code\", \"origin_airport_code\", \"destination_airport_code\", \"arrival_delay\", \"flight_date\", \"flight_month\", \"flight_year\"]\n",
    "temporal_cols = [\"day_of_week\", \"week_of_year\", \"day_of_month\", \"is_weekend\", \"is_holiday\", \"is_near_holiday\", \"is_holiday_period\", \"season\", \"quarter\"]\n",
    "\n",
    "print(f\"\\n‚úÖ Enhanced Silver Success:\")\n",
    "print(f\"Original business columns: {len(original_cols)}\")\n",
    "print(f\"New temporal columns: {len(temporal_cols)}\")\n",
    "print(f\"Total columns: {len(df_silver.columns)} (vs 6 in old Silver)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee00ed52-b7dd-4745-ad8c-206d98223b72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def path_exists(path):\n",
    "    \"\"\"Check if a path exists\"\"\"\n",
    "    try:\n",
    "        dbutils.fs.ls(path)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def create_directory_if_not_exists(path):\n",
    "    \"\"\"Create directory if it doesn't exist\"\"\"\n",
    "    if not path_exists(path):\n",
    "        dbutils.fs.mkdirs(path)\n",
    "        print(f\"‚úÖ Created directory: {path}\")\n",
    "    else:\n",
    "        print(f\"‚ÑπÔ∏è  Directory already exists: {path}\")\n",
    "\n",
    "def table_exists(table_name):\n",
    "    \"\"\"Check if a table exists\"\"\"\n",
    "    try:\n",
    "        spark.table(table_name)\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4c6cdf3-3947-4305-9e5e-0a2ec48d07dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assert df_silver, \"The DataFrame 'df_silver' does not exist.\"\n",
    "\n",
    "# Define the paths for your new Silver table\n",
    "SILVER_PATH = \"/Volumes/workspace/default/ds_capstone/silver/flights_processed\"\n",
    "SILVER_TABLE_NAME = \"default.silver_flights_processed\"\n",
    "DATABASE_NAME = \"default\"\n",
    "\n",
    "\n",
    "assert DATABASE_NAME, \"DATABASE_NAME is not defined.\"\n",
    "\n",
    "print(f\"\\nüìÅ Checking Silver path: {SILVER_PATH}\")\n",
    "if path_exists(SILVER_PATH):\n",
    "    print(f\"‚ö†Ô∏è  Path already exists. Checking if it's a valid Delta table...\")\n",
    "    try:\n",
    "        # Try to read as Delta\n",
    "        test_df = spark.read.format(\"delta\").load(SILVER_PATH)\n",
    "        print(f\"‚úÖ Valid Delta table found with {test_df.count()} records\")\n",
    "        print(f\"üí° Will overwrite existing table\")\n",
    "    except:\n",
    "        print(f\"‚ö†Ô∏è  Path exists but is not a valid Delta table\")\n",
    "        print(f\"üßπ Cleaning up old data...\")\n",
    "        dbutils.fs.rm(SILVER_PATH, recurse=True)\n",
    "        print(f\"‚úÖ Old data removed\")\n",
    "else:\n",
    "    print(f\"‚úÖ Path is clear, ready to create new table\")\n",
    "\n",
    "# Create parent directory if needed\n",
    "silver_parent = \"/\".join(SILVER_PATH.split(\"/\")[:-1])\n",
    "create_directory_if_not_exists(silver_parent)\n",
    "\n",
    "print(f\"\\nüíæ Writing Silver Delta table...\")\n",
    "try:\n",
    "    df_silver.write.format(\"delta\").mode(\"overwrite\").save(SILVER_PATH)\n",
    "    print(f\"‚úÖ Delta table written to: {SILVER_PATH}\")\n",
    "    print(f\"‚úÖ Records written: {df_silver.count():,}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR: Could not write Delta table\")\n",
    "    print(f\"   Error: {str(e)}\")\n",
    "    print(f\"\\nüí° Trying to clean and retry...\")\n",
    "    try:\n",
    "        dbutils.fs.rm(SILVER_PATH, recurse=True)\n",
    "        df_silver.write.format(\"delta\").mode(\"overwrite\").save(SILVER_PATH)\n",
    "        print(f\"‚úÖ Successfully wrote Delta table after cleanup\")\n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Still failed: {str(e2)}\")\n",
    "        raise\n",
    "\n",
    "print(f\"\\nüìå Registering Delta table as: {SILVER_TABLE_NAME}\")\n",
    "try:\n",
    "    # Ensure database exists\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {DATABASE_NAME}\")\n",
    "    print(f\"‚úÖ Database '{DATABASE_NAME}' ready\")\n",
    "    \n",
    "    # Drop table if it exists (to avoid conflicts)\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {SILVER_TABLE_NAME}\")\n",
    "    print(f\"   Dropped existing table (if any)\")\n",
    "    \n",
    "    # Create managed table \n",
    "    # This reads the data you JUST wrote and saves it as a managed table\n",
    "    df_for_table = spark.read.format(\"delta\").load(SILVER_PATH)\n",
    "    df_for_table.write.format(\"delta\").mode(\"overwrite\").saveAsTable(SILVER_TABLE_NAME)\n",
    "    \n",
    "    print(f\"‚úÖ Table registered successfully as '{SILVER_TABLE_NAME}'!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not create table with saveAsTable, trying alternative method...\")\n",
    "    try:\n",
    "        # Alternative: Create external table with explicit LOCATION\n",
    "        # This just points the table name to the files you saved in Step 7\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {SILVER_TABLE_NAME}\n",
    "            USING DELTA\n",
    "            LOCATION '{SILVER_PATH}'\n",
    "        \"\"\")\n",
    "        print(f\"‚úÖ Table registered with LOCATION clause!\")\n",
    "    except Exception as e2:\n",
    "        print(f\"‚ö†Ô∏è  Table registration failed: {str(e2)}\")\n",
    "        print(f\"üí° You can still access the data directly using:\")\n",
    "        print(f\"   spark.read.format('delta').load('{SILVER_PATH}')\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver_table",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
