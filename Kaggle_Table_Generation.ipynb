{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43cad212-8e24-4516-950b-45235ade66ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üß© Prerequisites\n",
    "\n",
    "1. In Databricks, open the **Catalog** panel on the left sidebar.  \n",
    "2. Navigate to **Workspace ‚Üí default**.  \n",
    "3. Click **Create** (top-right corner) and select **Volume**.  \n",
    "4. Name the volume **`ds-capstone`** and click **Create**.  \n",
    "5. Once created, open the new volume and click **Upload to this volume**.  \n",
    "6. Upload the Kaggle dataset file: **`flights_sample_3m.csv`**.  \n",
    "\n",
    "After completing these steps, you‚Äôll be ready to run the code below without any errors.  \n",
    "If you encounter any issues, please reach out to me for assistance - Omar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "688d2a74-7183-4de5-a81c-770c5dccc5b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### BRONZE TABLE CREATION SCRIPT\n",
    "#### Creates a Bronze Delta table from your raw data file\n",
    "#### Automatically creates directories and tables if they don't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "088ab11c-1d45-4dba-ba04-62a964b77a42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "BASE_PATH = \"/Volumes/workspace/default/ds_capstone\" # Had to edit this line to the correct file path. Previously it was \"/Volumes/workspace/default/ds-caspstone\" and has since been corrected\n",
    "PROJECT_NAME = \"flights_analysis\"\n",
    "SOURCE_FILE_NAME = \"flights_sample_3m.csv\"\n",
    "\n",
    "# Table configuration\n",
    "DATABASE_NAME = \"default\"  # Change this if you want to use a different database\n",
    "TABLE_NAME = \"bronze_flights_data\"\n",
    "BRONZE_TABLE_NAME = f\"{DATABASE_NAME}.{TABLE_NAME}\"\n",
    "\n",
    "# Define paths\n",
    "SOURCE_FILE = f\"{BASE_PATH}/{SOURCE_FILE_NAME}\"\n",
    "BRONZE_PATH = f\"{BASE_PATH}/bronze/flights_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a60aeb3-c28d-40b4-92f4-2bd484b0a882",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c46b225-e3bb-46be-b742-a2866a9bc20b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def path_exists(path):\n",
    "    \"\"\"Check if a path exists\"\"\"\n",
    "    try:\n",
    "        dbutils.fs.ls(path)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def create_directory_if_not_exists(path):\n",
    "    \"\"\"Create directory if it doesn't exist\"\"\"\n",
    "    if not path_exists(path):\n",
    "        dbutils.fs.mkdirs(path)\n",
    "        print(f\"‚úÖ Created directory: {path}\")\n",
    "    else:\n",
    "        print(f\"‚ÑπÔ∏è  Directory already exists: {path}\")\n",
    "\n",
    "def table_exists(table_name):\n",
    "    \"\"\"Check if a table exists\"\"\"\n",
    "    try:\n",
    "        spark.table(table_name)\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e906176-9705-4add-8e23-0df7d38d8a4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### BRONZE LAYER CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fd7238f-32e2-44b1-bc5d-6e232c45d65f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ü•â BRONZE TABLE CREATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Step 1: List available files to help you verify the filename\n",
    "print(f\"\\nüìÇ Available files in {BASE_PATH}:\")\n",
    "try:\n",
    "    files = dbutils.fs.ls(BASE_PATH)\n",
    "    for f in files:\n",
    "        icon = \"üìÅ\" if f.isDir() else \"üìÑ\"\n",
    "        print(f\"  {icon} {f.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not list files: {str(e)}\")\n",
    "\n",
    "# Step 2: Check if source file exists\n",
    "print(f\"\\nüîç Checking for source file: {SOURCE_FILE}\")\n",
    "if not path_exists(SOURCE_FILE):\n",
    "    print(f\"‚ùå ERROR: Source file not found!\")\n",
    "    print(f\"   Expected: {SOURCE_FILE}\")\n",
    "    print(f\"\\nüí° Tips:\")\n",
    "    print(f\"   1. Check the filename spelling above\")\n",
    "    print(f\"   2. Update SOURCE_FILE_NAME in the configuration section\")\n",
    "    print(f\"   3. Make sure your file is uploaded to {BASE_PATH}\")\n",
    "    raise FileNotFoundError(f\"Source file not found: {SOURCE_FILE}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Source file found!\")\n",
    "\n",
    "# Step 3: Read the source file (auto-detect format)\n",
    "print(f\"\\nüìñ Reading source file...\")\n",
    "try:\n",
    "    if SOURCE_FILE.endswith('.csv'):\n",
    "        df_raw = spark.read.csv(SOURCE_FILE, header=True, inferSchema=True)\n",
    "        print(\"   Format: CSV\")\n",
    "    elif SOURCE_FILE.endswith('.json'):\n",
    "        df_raw = spark.read.json(SOURCE_FILE)\n",
    "        print(\"   Format: JSON\")\n",
    "    elif SOURCE_FILE.endswith('.parquet'):\n",
    "        df_raw = spark.read.parquet(SOURCE_FILE)\n",
    "        print(\"   Format: Parquet\")\n",
    "    elif SOURCE_FILE.endswith('.txt'):\n",
    "        df_raw = spark.read.text(SOURCE_FILE)\n",
    "        print(\"   Format: Text\")\n",
    "    else:\n",
    "        # Try to read as Delta table\n",
    "        df_raw = spark.read.format(\"delta\").load(SOURCE_FILE)\n",
    "        print(\"   Format: Delta\")\n",
    "    \n",
    "    print(f\"‚úÖ Successfully read file: {df_raw.count():,} records\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR: Could not read file\")\n",
    "    print(f\"   Error: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Step 4: Show data preview\n",
    "print(f\"\\nüìä Data Preview (first 5 rows):\")\n",
    "display(df_raw.limit(5))\n",
    "\n",
    "print(f\"\\nüìã Schema:\")\n",
    "df_raw.printSchema()\n",
    "\n",
    "# Step 5: Add Bronze metadata (optional - tracks when data was ingested)\n",
    "print(f\"\\nüè∑Ô∏è  Adding Bronze metadata...\")\n",
    "df_bronze = df_raw.withColumn(\"bronze_ingestion_timestamp\", current_timestamp())\n",
    "print(f\"‚úÖ Added ingestion timestamp\")\n",
    "\n",
    "# Step 6: Check if Bronze path exists and clean if needed\n",
    "print(f\"\\nüìÅ Checking Bronze path: {BRONZE_PATH}\")\n",
    "if path_exists(BRONZE_PATH):\n",
    "    print(f\"‚ö†Ô∏è  Path already exists. Checking if it's a valid Delta table...\")\n",
    "    try:\n",
    "        # Try to read as Delta\n",
    "        test_df = spark.read.format(\"delta\").load(BRONZE_PATH)\n",
    "        print(f\"‚úÖ Valid Delta table found with {test_df.count()} records\")\n",
    "        print(f\"üí° Will overwrite existing table\")\n",
    "    except:\n",
    "        print(f\"‚ö†Ô∏è  Path exists but is not a valid Delta table\")\n",
    "        print(f\"üßπ Cleaning up old data...\")\n",
    "        dbutils.fs.rm(BRONZE_PATH, recurse=True)\n",
    "        print(f\"‚úÖ Old data removed\")\n",
    "else:\n",
    "    print(f\"‚úÖ Path is clear, ready to create new table\")\n",
    "\n",
    "# Create parent directory if needed\n",
    "bronze_parent = \"/\".join(BRONZE_PATH.split(\"/\")[:-1])\n",
    "create_directory_if_not_exists(bronze_parent)\n",
    "\n",
    "# Step 7: Write Bronze Delta table\n",
    "print(f\"\\nüíæ Writing Bronze Delta table...\")\n",
    "try:\n",
    "    df_bronze.write.format(\"delta\").mode(\"overwrite\").save(BRONZE_PATH)\n",
    "    print(f\"‚úÖ Delta table written to: {BRONZE_PATH}\")\n",
    "    print(f\"‚úÖ Records written: {df_bronze.count():,}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR: Could not write Delta table\")\n",
    "    print(f\"   Error: {str(e)}\")\n",
    "    print(f\"\\nüí° Trying to clean and retry...\")\n",
    "    try:\n",
    "        dbutils.fs.rm(BRONZE_PATH, recurse=True)\n",
    "        df_bronze.write.format(\"delta\").mode(\"overwrite\").save(BRONZE_PATH)\n",
    "        print(f\"‚úÖ Successfully wrote Delta table after cleanup\")\n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Still failed: {str(e2)}\")\n",
    "        raise\n",
    "\n",
    "# Step 8: Create table reference (register the Delta table)\n",
    "print(f\"\\nüìå Registering Delta table as: {BRONZE_TABLE_NAME}\")\n",
    "try:\n",
    "    # Ensure database exists\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {DATABASE_NAME}\")\n",
    "    print(f\"‚úÖ Database '{DATABASE_NAME}' ready\")\n",
    "    \n",
    "    # Drop table if it exists (to avoid conflicts)\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {BRONZE_TABLE_NAME}\")\n",
    "    print(f\"   Dropped existing table (if any)\")\n",
    "    \n",
    "    # Create external table pointing to the Delta location\n",
    "    # Using a simple CREATE TABLE without location might work better in Community Edition\n",
    "    df_for_table = spark.read.format(\"delta\").load(BRONZE_PATH)\n",
    "    df_for_table.write.format(\"delta\").mode(\"overwrite\").saveAsTable(BRONZE_TABLE_NAME)\n",
    "    \n",
    "    print(f\"‚úÖ Table registered successfully as '{BRONZE_TABLE_NAME}'!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not create table with saveAsTable, trying alternative method...\")\n",
    "    try:\n",
    "        # Alternative: Create table with explicit LOCATION\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {BRONZE_TABLE_NAME}\n",
    "            USING DELTA\n",
    "            LOCATION '{BRONZE_PATH}'\n",
    "        \"\"\")\n",
    "        print(f\"‚úÖ Table registered with LOCATION clause!\")\n",
    "    except Exception as e2:\n",
    "        print(f\"‚ö†Ô∏è  Table registration failed: {str(e2)}\")\n",
    "        print(f\"üí° You can still access the data directly using:\")\n",
    "        print(f\"   spark.read.format('delta').load('{BRONZE_PATH}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce413dc5-6c34-46be-9abf-1c4b898ab8ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### VERIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29a5a6ca-a34d-40d6-9566-51fa98a767d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ BRONZE DELTA TABLE CREATED!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Verify by reading the Delta table directly\n",
    "print(f\"\\nüîç Verification:\")\n",
    "try:\n",
    "    verify_df = spark.read.format(\"delta\").load(BRONZE_PATH)\n",
    "    record_count = verify_df.count()\n",
    "    print(f\"‚úÖ Delta table exists at: {BRONZE_PATH}\")\n",
    "    print(f\"‚úÖ Record count: {record_count:,}\")\n",
    "    \n",
    "    # Show columns\n",
    "    columns = verify_df.columns\n",
    "    print(f\"‚úÖ Columns ({len(columns)}): {', '.join(columns[:5])}{'...' if len(columns) > 5 else ''}\")\n",
    "    \n",
    "    # Check if table name is registered\n",
    "    if table_exists(BRONZE_TABLE_NAME):\n",
    "        print(f\"‚úÖ Table '{BRONZE_TABLE_NAME}' is registered and queryable\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Table name '{BRONZE_TABLE_NAME}' not registered, but data is accessible via path\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Verification failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "513e70a4-52b6-42ee-a67b-056feb9a7159",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Options to call data from tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3985cc70-0870-4c99-8e12-ca1a3ac6e2c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Uncomment to viewdata\n",
    "# df = spark.read.format('delta').load('/Volumes/workspace/default/ds-caspstone/bronze/flights_data')\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "860be974-a2a1-4f4d-8c6f-c7cc11ed5925",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Uncomment to viewdata\n",
    "# display(\n",
    "#     spark.table(\n",
    "#         BRONZE_TABLE_NAME\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dae4ca95-5f10-4b25-8e53-c7335589244d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Uncomment to viewdata\n",
    "# display(\n",
    "#     spark.sql(\n",
    "#         f\"SELECT * FROM {BRONZE_TABLE_NAME} LIMIT 10\"\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a6715e4-f22d-4eeb-aae8-3c031c70a704",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Kaggle_Table_Generation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
