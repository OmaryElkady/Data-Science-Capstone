{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33eda753-6a47-49b8-ac40-e2f0e44f00a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Dual Model System - Complete Implementation Guide\n",
    "\n",
    "**Overview:**\n",
    "\n",
    "Flightmasters system now uses TWO separate models:\n",
    "\n",
    "-Pre-Departure Model :- Predicts delays BEFORE flight takes off\n",
    "\n",
    "-In-Flight Model :- Predicts delays AFTER takeoff (more accurate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ad9ec54-3d54-4ae8-abd7-f6eeb9f61682",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Why create 2 models?\n",
    "\n",
    "dep_delay is one of the STRONGEST predictors of arr_delay\n",
    "\n",
    "If a flight departs 20 minutes late, it's likely to arrive late too\n",
    "\n",
    "Pre-departure model must rely on other signals (time of day, route, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de21fdec-40a6-4cbc-9c1a-49719cc88b47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Description and details:\n",
    "\n",
    "Trains 4 models:\n",
    "\n",
    "1. Random Forest (Pre-Departure)\n",
    "2. GBT (Pre-Departure)\n",
    "3. Random Forest (In-Flight)\n",
    "4. GBT (In-Flight)\n",
    "\n",
    "Features:\n",
    "\n",
    "1. Bayesian optimization (smart parameter tuning)\n",
    "2. 2-fold cross-validation (reliable metrics)\n",
    "3. MLflow tracking (experiment management)\n",
    "\n",
    "Runtime: ~1.5 hours total\n",
    "\n",
    "Pre-Departure models: ~6 hours\n",
    "In-Flight models: ~9 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4de3be55-59ed-408c-ab5d-502b69c697e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Steps to run:\n",
    "Run all the cells in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a187f13-5f39-4540-8d76-7e02458a4e94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f76f3ca-98d6-4496-8cb1-87e22b956217",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "import os\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "from pyspark.ml.classification import GBTClassifier, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "# Signature imports\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types.schema import Schema, TensorSpec\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Optimized for Databricks Community Edition\"\"\"\n",
    "    \n",
    "    TOP_K_FEATURES = 40 \n",
    "    CV_FOLDS = 2\n",
    "    BAYES_MAX_EVALS = 4\n",
    "    TEST_RATIO = 0.2\n",
    "    RANDOM_SEED = 42\n",
    "    \n",
    "    GOLD_TABLE = \"default.gold_ml_features_experimental\"\n",
    "    EXPERIMENT_NAME = \"/Shared/Flightmasters_Optimized_Experiments\"\n",
    "    \n",
    "    # CRITICAL CHANGE: Use workspace registry for initial logging\n",
    "    MLFLOW_TRACKING_URI = \"databricks\"\n",
    "    MLFLOW_REGISTRY_URI = \"databricks\"  # Changed from databricks-uc\n",
    "    \n",
    "    # Model names (without UC prefix for workspace registry)\n",
    "    MODEL_RF_PRE = \"model_rf_pre\"\n",
    "    MODEL_GBT_PRE = \"model_gbt_pre\"\n",
    "    MODEL_RF_IN = \"model_rf_in\"\n",
    "    MODEL_GBT_IN = \"model_gbt_in\"\n",
    "    \n",
    "    # UC settings (for manual registration later)\n",
    "    UC_CATALOG = \"workspace\"\n",
    "    UC_SCHEMA = \"default\"\n",
    "    UC_VOLUME_NAME = \"mlflow_shared_tmp\"\n",
    "    \n",
    "    USE_CHECKPOINTING = True\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# HELPERS\n",
    "# =============================================================================\n",
    "\n",
    "def setup_uc_volume(spark):\n",
    "    \"\"\"Setup Unity Catalog Volume for artifacts\"\"\"\n",
    "    catalog = Config.UC_CATALOG\n",
    "    schema = Config.UC_SCHEMA\n",
    "    volume_name = Config.UC_VOLUME_NAME\n",
    "    \n",
    "    volume_path = f\"{catalog}.{schema}.{volume_name}\"\n",
    "    env_path = f\"/Volumes/{catalog}/{schema}/{volume_name}\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"UNITY CATALOG VOLUME SETUP\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Volume Target: {volume_path}\")\n",
    "    \n",
    "    try:\n",
    "        volume_exists = spark.sql(f\"SHOW VOLUMES IN {catalog}.{schema}\").filter(\n",
    "            col(\"volume_name\") == volume_name\n",
    "        ).count() > 0\n",
    "        \n",
    "        if not volume_exists:\n",
    "            spark.sql(f\"CREATE VOLUME {volume_path}\")\n",
    "            print(\"‚úÖ Volume created successfully.\")\n",
    "        else:\n",
    "            print(\"‚úÖ Volume already exists.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  WARNING: Could not check/create volume: {e}\")\n",
    "        pass\n",
    "        \n",
    "    os.environ['MLFLOW_DFS_TMP'] = env_path\n",
    "    os.environ['SPARKML_TEMP_DFS_PATH'] = env_path\n",
    "    \n",
    "    print(f\"‚úÖ Environment paths set to: {env_path}\")\n",
    "    return volume_path\n",
    "\n",
    "\n",
    "def create_safe_vector_slicer(indices_to_keep):\n",
    "    \"\"\"Create UDF to slice vectors\"\"\"\n",
    "    indices_list = list(indices_to_keep)\n",
    "    \n",
    "    @udf(returnType=VectorUDT())\n",
    "    def safe_slicer(features):\n",
    "        if features is None: \n",
    "            return None\n",
    "        max_idx = features.size - 1\n",
    "        selected_values = [float(features[i]) for i in indices_list if i <= max_idx]\n",
    "        return Vectors.dense(selected_values)\n",
    "    \n",
    "    return safe_slicer\n",
    "\n",
    "\n",
    "def checkpoint_if_enabled(df, eager=True):\n",
    "    \"\"\"Checkpoint data to prevent OOM\"\"\"\n",
    "    if Config.USE_CHECKPOINTING:\n",
    "        return df.localCheckpoint(eager=eager)\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1. FEATURE IMPORTANCE\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_feature_importance(train_data, top_k):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PHASE 1: FEATURE IMPORTANCE ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    sample = train_data.select(\"features\").first()\n",
    "    if not sample:\n",
    "        raise ValueError(\"Training data is empty or features column is missing.\")\n",
    "        \n",
    "    total_features = sample.features.size\n",
    "    print(f\"\\nüìä Original features: {total_features}\")\n",
    "    \n",
    "    print(\"\\nüå≤ Training Random Forest for feature ranking...\")\n",
    "    rf = RandomForestClassifier(\n",
    "        featuresCol=\"features\", labelCol=\"label\",\n",
    "        numTrees=30, maxDepth=8, seed=Config.RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    rf_model = rf.fit(train_data)\n",
    "    importances = rf_model.featureImportances.toArray()\n",
    "    \n",
    "    top_k_indices = np.argsort(importances)[-top_k:][::-1]\n",
    "    top_k_scores = importances[top_k_indices]\n",
    "    \n",
    "    selected_importance = np.sum(top_k_scores)\n",
    "    total_importance = np.sum(importances)\n",
    "    retention = (selected_importance / total_importance) * 100\n",
    "    \n",
    "    print(f\"\\nüìà Feature Selection Summary:\")\n",
    "    print(f\"   Original: {total_features} -> Selected: {top_k}\")\n",
    "    print(f\"   Information retained: {retention:.1f}%\")\n",
    "    \n",
    "    return total_features, top_k_indices.tolist(), retention\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. CREATE DUAL DATASETS\n",
    "# =============================================================================\n",
    "\n",
    "def create_dual_datasets(df_gold, selected_indices, dep_delay_orig_index=11):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PHASE 2: CREATING DUAL DATASETS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    dep_delay_in_selected = dep_delay_orig_index in selected_indices\n",
    "    if dep_delay_in_selected: \n",
    "        print(f\"‚úÖ dep_delay found at original index {dep_delay_orig_index}\")\n",
    "    else: \n",
    "        print(f\"‚ö†Ô∏è  dep_delay (index {dep_delay_orig_index}) not in top-K\")\n",
    "    \n",
    "    print(f\"\\nüîß Applying feature selection...\")\n",
    "    slicer_udf = create_safe_vector_slicer(selected_indices)\n",
    "    df_selected = df_gold.withColumn(\"features\", slicer_udf(col(\"features\"))).select(\"features\", \"label\")\n",
    "    df_selected = df_selected.withColumn(\"label\", col(\"label\").cast(DoubleType()))\n",
    "    df_selected = checkpoint_if_enabled(df_selected)\n",
    "    \n",
    "    if dep_delay_in_selected:\n",
    "        print(f\"\\nüìã Creating Pre-Departure (removing dep_delay)...\")\n",
    "        dep_delay_new_index = selected_indices.index(dep_delay_orig_index)\n",
    "        pre_dep_indices = [i for i in range(len(selected_indices)) if i != dep_delay_new_index]\n",
    "        pre_dep_slicer = create_safe_vector_slicer(pre_dep_indices)\n",
    "        df_pre_dep = df_selected.withColumn(\"features\", pre_dep_slicer(col(\"features\"))).select(\"features\", \"label\")\n",
    "    else:\n",
    "        df_pre_dep = df_selected\n",
    "    \n",
    "    df_in_flight = df_selected\n",
    "    \n",
    "    df_pre_dep = checkpoint_if_enabled(df_pre_dep, eager=False)\n",
    "    df_in_flight = checkpoint_if_enabled(df_in_flight, eager=False)\n",
    "    \n",
    "    return df_pre_dep, df_in_flight, dep_delay_in_selected\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. TRAIN/TEST SPLIT\n",
    "# =============================================================================\n",
    "\n",
    "def split_and_checkpoint(df, name):\n",
    "    print(f\"\\nüîÄ Splitting {name} dataset...\")\n",
    "    train, test = df.randomSplit([1.0 - Config.TEST_RATIO, Config.TEST_RATIO], seed=Config.RANDOM_SEED)\n",
    "    print(f\"   Train count: {train.count():,}\")\n",
    "    print(f\"   Test count: {test.count():,}\")\n",
    "    \n",
    "    train = checkpoint_if_enabled(train, eager=True)\n",
    "    test = checkpoint_if_enabled(test, eager=True)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4. MODEL TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "def train_model_optimized(train_data, test_data, model_name, model_type):\n",
    "    \"\"\"Bayesian optimization with reduced CV folds\"\"\"\n",
    "    \n",
    "    print(f\"\\n\\tüéØ Training {model_name} ({model_type})\")\n",
    "    print(f\"\\t   {Config.CV_FOLDS}-fold CV + {Config.BAYES_MAX_EVALS} Bayesian evals\")\n",
    "    \n",
    "    if model_name == \"RandomForest\":\n",
    "        space = {\n",
    "            'numTrees': hp.choice('numTrees', [50, 100]),\n",
    "            'maxDepth': hp.choice('maxDepth', [10, 15]),\n",
    "            'minInstancesPerNode': hp.choice('minInstancesPerNode', [25, 50])\n",
    "        }\n",
    "        ModelClass = RandomForestClassifier\n",
    "        param_map = {'numTrees': [50, 100], 'maxDepth': [10, 15], 'minInstancesPerNode': [25, 50]}\n",
    "    else:\n",
    "        space = {\n",
    "            'maxIter': hp.choice('maxIter', [50, 100]),\n",
    "            'maxDepth': hp.choice('maxDepth', [4, 6]),\n",
    "            'stepSize': hp.uniform('stepSize', 0.05, 0.15)\n",
    "        }\n",
    "        ModelClass = GBTClassifier\n",
    "        param_map = {'maxIter': [50, 100], 'maxDepth': [4, 6]}\n",
    "    \n",
    "    def objective(params):\n",
    "        if model_name == \"RandomForest\":\n",
    "            model = ModelClass(\n",
    "                featuresCol=\"features\", labelCol=\"label\",\n",
    "                numTrees=int(params['numTrees']),\n",
    "                maxDepth=int(params['maxDepth']),\n",
    "                minInstancesPerNode=int(params['minInstancesPerNode']),\n",
    "                seed=Config.RANDOM_SEED\n",
    "            )\n",
    "        else:\n",
    "            model = ModelClass(\n",
    "                featuresCol=\"features\", labelCol=\"label\",\n",
    "                maxIter=int(params['maxIter']),\n",
    "                maxDepth=int(params['maxDepth']),\n",
    "                stepSize=float(params['stepSize']),\n",
    "                seed=Config.RANDOM_SEED\n",
    "            )\n",
    "        \n",
    "        cv = CrossValidator(\n",
    "            estimator=model,\n",
    "            estimatorParamMaps=[{}],\n",
    "            evaluator=BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\"),\n",
    "            numFolds=Config.CV_FOLDS,\n",
    "            seed=Config.RANDOM_SEED,\n",
    "            parallelism=1\n",
    "        )\n",
    "        \n",
    "        cv_model = cv.fit(train_data)\n",
    "        avg_auc = cv_model.avgMetrics[0]\n",
    "        return {'loss': -avg_auc, 'status': STATUS_OK}\n",
    "    \n",
    "    print(f\"\\t   Optimizing over {Config.BAYES_MAX_EVALS} iterations...\")\n",
    "    trials = Trials()\n",
    "    \n",
    "    best = fmin(\n",
    "        fn=objective, space=space, algo=tpe.suggest,\n",
    "        max_evals=Config.BAYES_MAX_EVALS, trials=trials,\n",
    "        rstate=np.random.default_rng(Config.RANDOM_SEED),\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    best_params_actual = {}\n",
    "    for k, v in best.items():\n",
    "        if k in param_map:\n",
    "            best_params_actual[k] = param_map[k][v]\n",
    "        else:\n",
    "            best_params_actual[k] = v\n",
    "    \n",
    "    print(f\"\\t   Best params: {best_params_actual}\")\n",
    "    \n",
    "    final_model = ModelClass(\n",
    "        featuresCol=\"features\", labelCol=\"label\",\n",
    "        **best_params_actual, seed=Config.RANDOM_SEED\n",
    "    ).fit(train_data)\n",
    "    \n",
    "    predictions = final_model.transform(test_data)\n",
    "    \n",
    "    metrics = {\n",
    "        \"auc_roc\": BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\").evaluate(predictions),\n",
    "        \"accuracy\": MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\").evaluate(predictions),\n",
    "        \"f1_score\": MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"f1\").evaluate(predictions),\n",
    "        \"precision\": MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"precisionByLabel\").evaluate(predictions),\"recall\": MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"recallByLabel\").evaluate(predictions)\n",
    "    }\n",
    "    \n",
    "    cv_score = -min([t['result']['loss'] for t in trials.trials])\n",
    "    print(f\"\\t   Best CV Score: {cv_score:.4f}\")\n",
    "    print(f\"\\t   Test AUC-ROC: {metrics['auc_roc']:.4f}\")\n",
    "    \n",
    "    return final_model, metrics, best_params_actual, cv_score\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 5. MAIN PIPELINE (WORKSPACE REGISTRY LOGGING)\n",
    "# =============================================================================\n",
    "\n",
    "def run_complete_experiments():\n",
    "    \"\"\"Main execution pipeline - logs to workspace registry\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"FLIGHTMASTERS OPTIMIZED EXPERIMENTS\")\n",
    "    print(\"Strategy: Log to Workspace Registry (bypasses MLeap issue)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Setup MLflow for workspace registry\n",
    "    mlflow.set_tracking_uri(Config.MLFLOW_TRACKING_URI)\n",
    "    mlflow.set_registry_uri(Config.MLFLOW_REGISTRY_URI)\n",
    "    mlflow.set_experiment(Config.EXPERIMENT_NAME)\n",
    "    \n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    setup_uc_volume(spark)\n",
    "    \n",
    "    mlflow.end_run()\n",
    "    \n",
    "    # === MASTER PARENT RUN ===\n",
    "    with mlflow.start_run(run_name=\"Flight_Experiment_Master_Workspace\"):\n",
    "        \n",
    "        # Load data\n",
    "        print(f\"\\nüì• Loading Gold table...\")\n",
    "        df_gold = spark.table(Config.GOLD_TABLE)\n",
    "        df_gold = df_gold.withColumn(\"label\", col(\"label\").cast(DoubleType())).filter(\n",
    "            (col(\"label\") == 0.0) | (col(\"label\") == 1.0)\n",
    "        )\n",
    "        df_gold = checkpoint_if_enabled(df_gold, eager=True)\n",
    "        \n",
    "        train_full, test_full = df_gold.randomSplit([0.8, 0.2], seed=Config.RANDOM_SEED)\n",
    "        train_full = checkpoint_if_enabled(train_full, eager=False)\n",
    "        test_full = checkpoint_if_enabled(test_full, eager=False)\n",
    "\n",
    "        # Feature selection\n",
    "        with mlflow.start_run(run_name=\"Feature_Selection\", nested=True):\n",
    "            orig_features, selected_indices, retention = analyze_feature_importance(\n",
    "                train_full, Config.TOP_K_FEATURES\n",
    "            )\n",
    "            mlflow.log_params({\n",
    "                \"step\": \"Feature_Selection\",\n",
    "                \"original_features\": orig_features,\n",
    "                \"selected_features_count\": Config.TOP_K_FEATURES\n",
    "            })\n",
    "            mlflow.log_metric(\"information_retained_pct\", retention)\n",
    "            \n",
    "            temp_file = \"selected_feature_indices.txt\"\n",
    "            with open(temp_file, \"w\") as f:\n",
    "                # Write indices as a comma-separated list\n",
    "                f.write(\",\".join(map(str, selected_indices)))\n",
    "            mlflow.log_artifact(temp_file)\n",
    "            os.remove(temp_file)\n",
    "\n",
    "        # Create datasets\n",
    "        df_pre, df_in, _ = create_dual_datasets(df_gold, selected_indices)\n",
    "        train_pre, test_pre = split_and_checkpoint(df_pre, \"Pre-Departure\")\n",
    "        train_in, test_in = split_and_checkpoint(df_in, \"In-Flight\")\n",
    "        results = {}\n",
    "        \n",
    "        # Define signatures using TensorSpec (UC-compatible format)\n",
    "        pre_dep_feature_count = train_pre.select(\"features\").first().features.size\n",
    "        in_flight_feature_count = train_in.select(\"features\").first().features.size\n",
    "        \n",
    "        pre_dep_input_schema = Schema([TensorSpec(type=np.dtype('float64'), shape=(-1,), name=\"features\")])\n",
    "        in_flight_input_schema = Schema([TensorSpec(type=np.dtype('float64'), shape=(-1,), name=\"features\")])\n",
    "        output_schema = Schema([TensorSpec(type=np.dtype('float64'), shape=(-1, 2), name=\"probability\")])\n",
    "        \n",
    "        pre_dep_signature = ModelSignature(inputs=pre_dep_input_schema, outputs=output_schema)\n",
    "        in_flight_signature = ModelSignature(inputs=in_flight_input_schema, outputs=output_schema)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Signatures defined. Features: Pre-Dep={pre_dep_feature_count}, In-Flight={in_flight_feature_count}\")\n",
    "\n",
    "        # Get sample inputs - convert DenseVector to list/array for JSON serialization\n",
    "        sample_pre_row = train_pre.limit(1).select(\"features\").collect()[0]\n",
    "        sample_pre = pd.DataFrame([sample_pre_row.features.toArray()])\n",
    "        \n",
    "        sample_in_row = train_in.limit(1).select(\"features\").collect()[0]\n",
    "        sample_in = pd.DataFrame([sample_in_row.features.toArray()])\n",
    "        \n",
    "        # === TRAIN & LOG MODELS ===\n",
    "        \n",
    "        # 1. Pre-Departure Random Forest\n",
    "        with mlflow.start_run(run_name=\"RF_Pre_Departure\", nested=True):\n",
    "            m, metrics, p, cv = train_model_optimized(train_pre, test_pre, \"RandomForest\", \"Pre-Departure\")\n",
    "            mlflow.log_params(p)\n",
    "            mlflow.log_metric(\"cv_score\", cv)\n",
    "            mlflow.log_metric(\"auc_roc\", metrics[\"auc_roc\"])\n",
    "            mlflow.log_metric(\"accuracy\", metrics[\"accuracy\"])\n",
    "            mlflow.log_metric(\"f1_score\", metrics[\"f1_score\"])\n",
    "            mlflow.log_metric(\"precision\", metrics[\"precision\"])\n",
    "            mlflow.log_metric(\"recall\", metrics[\"recall\"])\n",
    "            # CRITICAL FIX: Log WITHOUT registered_model_name parameter\n",
    "            mlflow.spark.log_model(\n",
    "                spark_model=m,\n",
    "                artifact_path=\"model\",\n",
    "                signature=pre_dep_signature,\n",
    "                input_example=sample_pre\n",
    "            )\n",
    "            results[\"RF_Pre\"] = metrics\n",
    "            print(f\":white_check_mark: RF Pre-Departure logged to workspace registry.\")\n",
    "        # 2. Pre-Departure GBT\n",
    "        with mlflow.start_run(run_name=\"GBT_Pre_Departure\", nested=True):\n",
    "            m, metrics, p, cv = train_model_optimized(train_pre, test_pre, \"GBT\", \"Pre-Departure\")\n",
    "            mlflow.log_params(p)\n",
    "            mlflow.log_metric(\"cv_score\", cv)\n",
    "            mlflow.log_metric(\"auc_roc\", metrics[\"auc_roc\"])\n",
    "            mlflow.log_metric(\"accuracy\", metrics[\"accuracy\"])\n",
    "            mlflow.log_metric(\"f1_score\", metrics[\"f1_score\"])\n",
    "            mlflow.log_metric(\"precision\", metrics[\"precision\"])\n",
    "            mlflow.log_metric(\"recall\", metrics[\"recall\"])\n",
    "            mlflow.spark.log_model(\n",
    "                spark_model=m,\n",
    "                artifact_path=\"model\",\n",
    "                signature=pre_dep_signature,\n",
    "                input_example=sample_pre\n",
    "            )\n",
    "            results[\"GBT_Pre\"] = metrics\n",
    "            print(f\":white_check_mark: GBT Pre-Departure logged to workspace registry.\")\n",
    "        # 3. In-Flight Random Forest\n",
    "        with mlflow.start_run(run_name=\"RF_In_Flight\", nested=True):\n",
    "            m, metrics, p, cv = train_model_optimized(train_in, test_in, \"RandomForest\", \"In-Flight\")\n",
    "            mlflow.log_params(p)\n",
    "            mlflow.log_metric(\"cv_score\", cv)\n",
    "            mlflow.log_metric(\"auc_roc\", metrics[\"auc_roc\"])\n",
    "            mlflow.log_metric(\"accuracy\", metrics[\"accuracy\"])\n",
    "            mlflow.log_metric(\"f1_score\", metrics[\"f1_score\"])\n",
    "            mlflow.log_metric(\"precision\", metrics[\"precision\"])\n",
    "            mlflow.log_metric(\"recall\", metrics[\"recall\"])\n",
    "            mlflow.spark.log_model(\n",
    "                spark_model=m,\n",
    "                artifact_path=\"model\",\n",
    "                signature=in_flight_signature,\n",
    "                input_example=sample_in\n",
    "            )\n",
    "            results[\"RF_In\"] = metrics\n",
    "            print(f\":white_check_mark: RF In-Flight logged to workspace registry.\")\n",
    "        # 4. In-Flight GBT\n",
    "        with mlflow.start_run(run_name=\"GBT_In_Flight\", nested=True):\n",
    "            m, metrics, p, cv = train_model_optimized(train_in, test_in, \"GBT\", \"In-Flight\")\n",
    "            mlflow.log_params(p)\n",
    "            mlflow.log_metric(\"cv_score\", cv)\n",
    "            mlflow.log_metric(\"auc_roc\", metrics[\"auc_roc\"])\n",
    "            mlflow.log_metric(\"accuracy\", metrics[\"accuracy\"])\n",
    "            mlflow.log_metric(\"f1_score\", metrics[\"f1_score\"])\n",
    "            mlflow.log_metric(\"precision\", metrics[\"precision\"])\n",
    "            mlflow.log_metric(\"recall\", metrics[\"recall\"])\n",
    "            mlflow.spark.log_model(\n",
    "                spark_model=m,\n",
    "                artifact_path=\"model\",\n",
    "                signature=in_flight_signature,\n",
    "                input_example=sample_in\n",
    "            )\n",
    "            results[\"GBT_In\"] = metrics\n",
    "            print(f\":white_check_mark: GBT In-Flight logged to workspace registry.\")\n",
    "            \n",
    "        # === FINAL SUMMARY ===\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"FINAL RESULTS SUMMARY\")\n",
    "        print(\"=\" * 80)\n",
    "        for k, v in results.items():\n",
    "            print(f\"Model: {k:<15} | Test AUC: {v['auc_roc']:.4f}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"NEXT STEPS: Manual Unity Catalog Registration\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your models are now logged in the workspace registry with signatures.\")\n",
    "        print(\"\\nTo register to Unity Catalog:\")\n",
    "        print(\"1. Go to MLflow UI (Experiments page)\")\n",
    "        print(\"2. Find your runs and click on each model\")\n",
    "        print(\"3. Click 'Register Model' button\")\n",
    "        print(f\"4. Select Unity Catalog and use format: {Config.UC_CATALOG}.{Config.UC_SCHEMA}.model_name\")\n",
    "        print(\"\\nAlternatively, use the MLflow Client API to register programmatically\")\n",
    "        print(\"after models are logged (example code in next message).\")\n",
    "            \n",
    "        return results, selected_indices\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    try:\n",
    "        run_complete_experiments()\n",
    "        print(\"\\n‚úÖ All experiments complete and MLflow runs closed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå An error occurred during the pipeline execution: {e}\")\n",
    "        raise e\n",
    "        \n",
    "    finally:\n",
    "        mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a94f5cc-f6d8-4153-ab68-d0faf5cb8363",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Useful Information:\n",
    "\n",
    "You can find the ML models and information regarding feature selection in the Experiments tab on the left hand side.\n",
    "\n",
    "You can see the indexs of what features were used for training in the Artifacts tab of Feature Engineering within experiments (indexes refrence the Gold table).\n",
    "\n",
    "Once done head on to Flightmasters_Delay_Prediction notebook where you will need to fix and run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cdb58c3-89c1-4f87-9d05-0bc960c94e16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Advanced_Experiment_Tracking_and_Models",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
