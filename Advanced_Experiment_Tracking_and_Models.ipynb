{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33eda753-6a47-49b8-ac40-e2f0e44f00a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Dual Model System - Complete Implementation Guide\n",
    "\n",
    "**Overview:**\n",
    "\n",
    "Flightmasters system now uses TWO separate models:\n",
    "\n",
    "-Pre-Departure Model :- Predicts delays BEFORE flight takes off\n",
    "\n",
    "-In-Flight Model :- Predicts delays AFTER takeoff (more accurate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ad9ec54-3d54-4ae8-abd7-f6eeb9f61682",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Why create 2 models?\n",
    "\n",
    "dep_delay is one of the STRONGEST predictors of arr_delay\n",
    "\n",
    "If a flight departs 20 minutes late, it's likely to arrive late too\n",
    "\n",
    "Pre-departure model must rely on other signals (time of day, route, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de21fdec-40a6-4cbc-9c1a-49719cc88b47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Description and details:\n",
    "\n",
    "Trains 4 models:\n",
    "\n",
    "1. Random Forest (Pre-Departure)\n",
    "2. GBT (Pre-Departure)\n",
    "3. Random Forest (In-Flight)\n",
    "4. GBT (In-Flight)\n",
    "\n",
    "Features:\n",
    "\n",
    "1. Bayesian optimization (smart parameter tuning)\n",
    "2. 5-fold cross-validation (reliable metrics)\n",
    "3. MLflow tracking (experiment management)\n",
    "\n",
    "Runtime: ~15 hours total\n",
    "\n",
    "Pre-Departure models: ~6 hours\n",
    "In-Flight models: ~9 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4de3be55-59ed-408c-ab5d-502b69c697e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Steps to run:\n",
    "Run all the cells in the notebook after you change your email at the bottom of the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a187f13-5f39-4540-8d76-7e02458a4e94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hyperopt in /local_disk0/.ephemeral_nfs/envs/pythonEnv-3b6f5673-d2fc-417a-8640-653164380a7e/lib/python3.12/site-packages (0.2.7)\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.12/site-packages (from hyperopt) (2.1.3)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.12/site-packages (from hyperopt) (1.15.1)\nRequirement already satisfied: six in /usr/lib/python3/dist-packages (from hyperopt) (1.16.0)\nRequirement already satisfied: networkx>=2.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-3b6f5673-d2fc-417a-8640-653164380a7e/lib/python3.12/site-packages (from hyperopt) (3.6)\nRequirement already satisfied: future in /local_disk0/.ephemeral_nfs/envs/pythonEnv-3b6f5673-d2fc-417a-8640-653164380a7e/lib/python3.12/site-packages (from hyperopt) (1.0.0)\nRequirement already satisfied: tqdm in /local_disk0/.ephemeral_nfs/envs/pythonEnv-3b6f5673-d2fc-417a-8640-653164380a7e/lib/python3.12/site-packages (from hyperopt) (4.67.1)\nRequirement already satisfied: cloudpickle in /databricks/python3/lib/python3.12/site-packages (from hyperopt) (3.0.0)\nRequirement already satisfied: py4j in /databricks/python3/lib/python3.12/site-packages (from hyperopt) (0.10.9.9)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35843a79-4a4d-44ba-9db9-3d9edd4f002c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\nFLIGHTMASTERS OPTIMIZED EXPERIMENTS\nStrategy: Log to Workspace Registry (bypasses MLeap issue)\n================================================================================\n\n================================================================================\nUNITY CATALOG VOLUME SETUP\n================================================================================\nVolume Target: workspace.default.mlflow_shared_tmp\n‚úÖ Volume already exists.\n‚úÖ Environment paths set to: /Volumes/workspace/default/mlflow_shared_tmp\n\nüì• Loading Gold table...\n\n================================================================================\nPHASE 1: FEATURE IMPORTANCE ANALYSIS\n================================================================================\n\nüìä Original features: 819\n\nüå≤ Training Random Forest for feature ranking...\n\nüìà Feature Selection Summary:\n   Original: 819 -> Selected: 40\n   Information retained: 99.3%\n\n================================================================================\nPHASE 2: CREATING DUAL DATASETS\n================================================================================\n‚úÖ dep_delay found at original index 11\n\nüîß Applying feature selection...\n\nüìã Creating Pre-Departure (removing dep_delay)...\n\nüîÄ Splitting Pre-Departure dataset...\n   Train count: 1,971,675\n   Test count: 492,304\n\nüîÄ Splitting In-Flight dataset...\n   Train count: 1,971,675\n   Test count: 492,304\n\n‚úÖ Signatures defined. Features: Pre-Dep=39, In-Flight=40\n\n\tüéØ Training RandomForest (Pre-Departure)\n\t   2-fold CV + 4 Bayesian evals\n\t   Optimizing over 4 iterations...\n\t   Best params: {'maxDepth': 15, 'minInstancesPerNode': 50, 'numTrees': 50}\n\t   Best CV Score: 0.9179\n\t   Test AUC-ROC: 0.9190\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/30 04:56:02 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2025/11/30 04:56:07 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-3b6f5673-d2fc-417a-8640-65/tmplzgn_ew6/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f89e69654064cef8371b127ed85cc44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RF Pre-Departure logged to workspace registry.\n\n\tüéØ Training GBT (Pre-Departure)\n\t   2-fold CV + 4 Bayesian evals\n\t   Optimizing over 4 iterations...\n\t   Best params: {'maxDepth': 6, 'maxIter': 100, 'stepSize': np.float64(0.1313655407640288)}\n\t   Best CV Score: 0.9328\n\t   Test AUC-ROC: 0.9318\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/30 05:17:07 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2025/11/30 05:17:10 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-3b6f5673-d2fc-417a-8640-65/tmpci5vki8_/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a98ae69eef14d1981605af8017d5ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GBT Pre-Departure logged to workspace registry.\n\n\tüéØ Training RandomForest (In-Flight)\n\t   2-fold CV + 4 Bayesian evals\n\t   Optimizing over 4 iterations...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2025-11-30 05:27:44.395\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_MultiThreadedRendezvous\", \"msg\": \"<_MultiThreadedRendezvous of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:44.394535673+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"_execute_and_fetch_as_iterator\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2019\"}, {\"class\": null, \"method\": \"__next__\", \"file\": \"<frozen _collections_abc>\", \"line\": \"356\"}, {\"class\": null, \"method\": \"send\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"141\"}, {\"class\": null, \"method\": \"_has_next\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"202\"}, {\"class\": null, \"method\": \"_has_next\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"193\"}, {\"class\": null, \"method\": \"_call_iter\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"311\"}, {\"class\": null, \"method\": \"_call_iter\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"283\"}, {\"class\": null, \"method\": \"<lambda>\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"194\"}, {\"class\": null, \"method\": \"__iter__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"686\"}, {\"class\": null, \"method\": \"__next__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"543\"}, {\"class\": null, \"method\": \"_next\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"969\"}]}}\n{\"ts\": \"2025-11-30 05:27:44.395\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_MultiThreadedRendezvous\", \"msg\": \"<_MultiThreadedRendezvous of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:44.394535673+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"_execute_and_fetch_as_iterator\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2019\"}, {\"class\": null, \"method\": \"__next__\", \"file\": \"<frozen _collections_abc>\", \"line\": \"356\"}, {\"class\": null, \"method\": \"send\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"141\"}, {\"class\": null, \"method\": \"_has_next\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"202\"}, {\"class\": null, \"method\": \"_has_next\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"193\"}, {\"class\": null, \"method\": \"_call_iter\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"311\"}, {\"class\": null, \"method\": \"_call_iter\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"283\"}, {\"class\": null, \"method\": \"<lambda>\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"194\"}, {\"class\": null, \"method\": \"__iter__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"686\"}, {\"class\": null, \"method\": \"__next__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"543\"}, {\"class\": null, \"method\": \"_next\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"969\"}]}}\n{\"ts\": \"2025-11-30 05:27:44.395\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_MultiThreadedRendezvous\", \"msg\": \"<_MultiThreadedRendezvous of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:44.394535673+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"_execute_and_fetch_as_iterator\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2019\"}, {\"class\": null, \"method\": \"__next__\", \"file\": \"<frozen _collections_abc>\", \"line\": \"356\"}, {\"class\": null, \"method\": \"send\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"141\"}, {\"class\": null, \"method\": \"_has_next\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"202\"}, {\"class\": null, \"method\": \"_has_next\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"193\"}, {\"class\": null, \"method\": \"_call_iter\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"311\"}, {\"class\": null, \"method\": \"_call_iter\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"283\"}, {\"class\": null, \"method\": \"<lambda>\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"194\"}, {\"class\": null, \"method\": \"__iter__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"686\"}, {\"class\": null, \"method\": \"__next__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"543\"}, {\"class\": null, \"method\": \"_next\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"969\"}]}}\n{\"ts\": \"2025-11-30 05:27:44.414\", \"level\": \"WARNING\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"ReleaseExecute failed with exception: <_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\", grpc_status:9, created_time:\\\"2025-11-30T05:27:44.414216608+00:00\\\"}\\\"\\n>.\", \"context\": {}}\n{\"ts\": \"2025-11-30 05:27:44.414\", \"level\": \"WARNING\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"ReleaseExecute failed with exception: <_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\", grpc_status:9, created_time:\\\"2025-11-30T05:27:44.414216608+00:00\\\"}\\\"\\n>.\", \"context\": {}}\n{\"ts\": \"2025-11-30 05:27:44.414\", \"level\": \"WARNING\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"ReleaseExecute failed with exception: <_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\", grpc_status:9, created_time:\\\"2025-11-30T05:27:44.414216608+00:00\\\"}\\\"\\n>.\", \"context\": {}}\njob exception: (DENY_NEW_AND_EXISTING_RESOURCES) BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå An error occurred during the pipeline execution: (DENY_NEW_AND_EXISTING_RESOURCES) BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2025-11-30 05:27:46.135\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.115777946+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.135\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.115777946+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.135\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.115777946+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.283\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.282716754+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.283\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.282716754+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.283\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.282716754+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.339\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.338666126+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.339\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.338666126+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.339\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.338666126+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.386\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.385772623+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.386\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.385772623+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.386\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.385772623+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.429\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.429062254+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.429\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.429062254+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.429\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.429062254+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.471\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.471221903+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.471\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.471221903+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.471\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.471221903+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.518\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.518120291+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.518\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.518120291+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.518\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.518120291+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.584\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.583388579+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.584\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.583388579+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.584\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.583388579+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.660\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.659852007+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.660\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.659852007+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.660\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.659852007+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.721\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\", grpc_status:9, created_time:\\\"2025-11-30T05:27:46.720931327+00:00\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.721\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\", grpc_status:9, created_time:\\\"2025-11-30T05:27:46.720931327+00:00\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.721\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\", grpc_status:9, created_time:\\\"2025-11-30T05:27:46.720931327+00:00\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.764\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.762225942+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.764\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.762225942+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.764\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.762225942+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.805\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.804994198+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.805\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.804994198+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.805\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.804994198+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.845\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.845154948+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.845\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.845154948+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.845\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.845154948+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.881\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.881561235+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.881\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.881561235+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.881\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.881561235+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.920\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\", grpc_status:9, created_time:\\\"2025-11-30T05:27:46.919676073+00:00\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.920\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\", grpc_status:9, created_time:\\\"2025-11-30T05:27:46.919676073+00:00\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.920\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\", grpc_status:9, created_time:\\\"2025-11-30T05:27:46.919676073+00:00\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.958\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.957915214+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.958\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.957915214+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.958\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.957915214+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.992\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.992443895+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.992\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.992443895+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:46.992\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:46.992443895+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:47.032\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\", grpc_status:9, created_time:\\\"2025-11-30T05:27:47.031658709+00:00\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:47.032\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\", grpc_status:9, created_time:\\\"2025-11-30T05:27:47.031658709+00:00\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:47.032\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\", grpc_status:9, created_time:\\\"2025-11-30T05:27:47.031658709+00:00\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:47.070\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\", grpc_status:9, created_time:\\\"2025-11-30T05:27:47.070327398+00:00\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:47.070\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\", grpc_status:9, created_time:\\\"2025-11-30T05:27:47.070327398+00:00\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:47.070\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\", grpc_status:9, created_time:\\\"2025-11-30T05:27:47.070327398+00:00\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:47.110\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:47.110174271+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:47.110\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:47.110174271+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2025-11-30 05:27:47.110\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-11-30T05:27:47.110174271+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"interrupt_tag\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2219\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mUnknownException\u001B[0m                          Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-8141152329916617>, line 484\u001B[0m\n",
       "\u001B[1;32m    482\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    483\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m‚ùå An error occurred during the pipeline execution: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m--> 484\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
       "\u001B[1;32m    486\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m    487\u001B[0m     mlflow\u001B[38;5;241m.\u001B[39mend_run()\n",
       "\n",
       "File \u001B[0;32m<command-8141152329916617>, line 479\u001B[0m\n",
       "\u001B[1;32m    476\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[1;32m    478\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 479\u001B[0m         run_complete_experiments()\n",
       "\u001B[1;32m    480\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m‚úÖ All experiments complete and MLflow runs closed successfully!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m    482\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\n",
       "File \u001B[0;32m<command-8141152329916617>, line 422\u001B[0m, in \u001B[0;36mrun_complete_experiments\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m    420\u001B[0m \u001B[38;5;66;03m# 3. In-Flight Random Forest\u001B[39;00m\n",
       "\u001B[1;32m    421\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m mlflow\u001B[38;5;241m.\u001B[39mstart_run(run_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRF_In_Flight\u001B[39m\u001B[38;5;124m\"\u001B[39m, nested\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n",
       "\u001B[0;32m--> 422\u001B[0m     m, metrics, p, cv \u001B[38;5;241m=\u001B[39m train_model_optimized(train_in, test_in, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRandomForest\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIn-Flight\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m    423\u001B[0m     mlflow\u001B[38;5;241m.\u001B[39mlog_params(p)\n",
       "\u001B[1;32m    424\u001B[0m     mlflow\u001B[38;5;241m.\u001B[39mlog_metric(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcv_score\u001B[39m\u001B[38;5;124m\"\u001B[39m, cv)\n",
       "\n",
       "File \u001B[0;32m<command-8141152329916617>, line 279\u001B[0m, in \u001B[0;36mtrain_model_optimized\u001B[0;34m(train_data, test_data, model_name, model_type)\u001B[0m\n",
       "\u001B[1;32m    276\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m   Optimizing over \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mConfig\u001B[38;5;241m.\u001B[39mBAYES_MAX_EVALS\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m iterations...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m    277\u001B[0m trials \u001B[38;5;241m=\u001B[39m Trials()\n",
       "\u001B[0;32m--> 279\u001B[0m best \u001B[38;5;241m=\u001B[39m fmin(\n",
       "\u001B[1;32m    280\u001B[0m     fn\u001B[38;5;241m=\u001B[39mobjective, space\u001B[38;5;241m=\u001B[39mspace, algo\u001B[38;5;241m=\u001B[39mtpe\u001B[38;5;241m.\u001B[39msuggest,\n",
       "\u001B[1;32m    281\u001B[0m     max_evals\u001B[38;5;241m=\u001B[39mConfig\u001B[38;5;241m.\u001B[39mBAYES_MAX_EVALS, trials\u001B[38;5;241m=\u001B[39mtrials,\n",
       "\u001B[1;32m    282\u001B[0m     rstate\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mdefault_rng(Config\u001B[38;5;241m.\u001B[39mRANDOM_SEED),\n",
       "\u001B[1;32m    283\u001B[0m     verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m\n",
       "\u001B[1;32m    284\u001B[0m )\n",
       "\u001B[1;32m    286\u001B[0m best_params_actual \u001B[38;5;241m=\u001B[39m {}\n",
       "\u001B[1;32m    287\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m best\u001B[38;5;241m.\u001B[39mitems():\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-3b6f5673-d2fc-417a-8640-653164380a7e/lib/python3.12/site-packages/hyperopt/fmin.py:540\u001B[0m, in \u001B[0;36mfmin\u001B[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001B[0m\n",
       "\u001B[1;32m    537\u001B[0m     fn \u001B[38;5;241m=\u001B[39m __objective_fmin_wrapper(fn)\n",
       "\u001B[1;32m    539\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m allow_trials_fmin \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(trials, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfmin\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\u001B[0;32m--> 540\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m trials\u001B[38;5;241m.\u001B[39mfmin(\n",
       "\u001B[1;32m    541\u001B[0m         fn,\n",
       "\u001B[1;32m    542\u001B[0m         space,\n",
       "\u001B[1;32m    543\u001B[0m         algo\u001B[38;5;241m=\u001B[39malgo,\n",
       "\u001B[1;32m    544\u001B[0m         max_evals\u001B[38;5;241m=\u001B[39mmax_evals,\n",
       "\u001B[1;32m    545\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mtimeout,\n",
       "\u001B[1;32m    546\u001B[0m         loss_threshold\u001B[38;5;241m=\u001B[39mloss_threshold,\n",
       "\u001B[1;32m    547\u001B[0m         max_queue_len\u001B[38;5;241m=\u001B[39mmax_queue_len,\n",
       "\u001B[1;32m    548\u001B[0m         rstate\u001B[38;5;241m=\u001B[39mrstate,\n",
       "\u001B[1;32m    549\u001B[0m         pass_expr_memo_ctrl\u001B[38;5;241m=\u001B[39mpass_expr_memo_ctrl,\n",
       "\u001B[1;32m    550\u001B[0m         verbose\u001B[38;5;241m=\u001B[39mverbose,\n",
       "\u001B[1;32m    551\u001B[0m         catch_eval_exceptions\u001B[38;5;241m=\u001B[39mcatch_eval_exceptions,\n",
       "\u001B[1;32m    552\u001B[0m         return_argmin\u001B[38;5;241m=\u001B[39mreturn_argmin,\n",
       "\u001B[1;32m    553\u001B[0m         show_progressbar\u001B[38;5;241m=\u001B[39mshow_progressbar,\n",
       "\u001B[1;32m    554\u001B[0m         early_stop_fn\u001B[38;5;241m=\u001B[39mearly_stop_fn,\n",
       "\u001B[1;32m    555\u001B[0m         trials_save_file\u001B[38;5;241m=\u001B[39mtrials_save_file,\n",
       "\u001B[1;32m    556\u001B[0m     )\n",
       "\u001B[1;32m    558\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m trials \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m    559\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(trials_save_file):\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-3b6f5673-d2fc-417a-8640-653164380a7e/lib/python3.12/site-packages/hyperopt/base.py:671\u001B[0m, in \u001B[0;36mTrials.fmin\u001B[0;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001B[0m\n",
       "\u001B[1;32m    666\u001B[0m \u001B[38;5;66;03m# -- Stop-gap implementation!\u001B[39;00m\n",
       "\u001B[1;32m    667\u001B[0m \u001B[38;5;66;03m#    fmin should have been a Trials method in the first place\u001B[39;00m\n",
       "\u001B[1;32m    668\u001B[0m \u001B[38;5;66;03m#    but for now it's still sitting in another file.\u001B[39;00m\n",
       "\u001B[1;32m    669\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfmin\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m fmin\n",
       "\u001B[0;32m--> 671\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m fmin(\n",
       "\u001B[1;32m    672\u001B[0m     fn,\n",
       "\u001B[1;32m    673\u001B[0m     space,\n",
       "\u001B[1;32m    674\u001B[0m     algo\u001B[38;5;241m=\u001B[39malgo,\n",
       "\u001B[1;32m    675\u001B[0m     max_evals\u001B[38;5;241m=\u001B[39mmax_evals,\n",
       "\u001B[1;32m    676\u001B[0m     timeout\u001B[38;5;241m=\u001B[39mtimeout,\n",
       "\u001B[1;32m    677\u001B[0m     loss_threshold\u001B[38;5;241m=\u001B[39mloss_threshold,\n",
       "\u001B[1;32m    678\u001B[0m     trials\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m,\n",
       "\u001B[1;32m    679\u001B[0m     rstate\u001B[38;5;241m=\u001B[39mrstate,\n",
       "\u001B[1;32m    680\u001B[0m     verbose\u001B[38;5;241m=\u001B[39mverbose,\n",
       "\u001B[1;32m    681\u001B[0m     max_queue_len\u001B[38;5;241m=\u001B[39mmax_queue_len,\n",
       "\u001B[1;32m    682\u001B[0m     allow_trials_fmin\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,  \u001B[38;5;66;03m# -- prevent recursion\u001B[39;00m\n",
       "\u001B[1;32m    683\u001B[0m     pass_expr_memo_ctrl\u001B[38;5;241m=\u001B[39mpass_expr_memo_ctrl,\n",
       "\u001B[1;32m    684\u001B[0m     catch_eval_exceptions\u001B[38;5;241m=\u001B[39mcatch_eval_exceptions,\n",
       "\u001B[1;32m    685\u001B[0m     return_argmin\u001B[38;5;241m=\u001B[39mreturn_argmin,\n",
       "\u001B[1;32m    686\u001B[0m     show_progressbar\u001B[38;5;241m=\u001B[39mshow_progressbar,\n",
       "\u001B[1;32m    687\u001B[0m     early_stop_fn\u001B[38;5;241m=\u001B[39mearly_stop_fn,\n",
       "\u001B[1;32m    688\u001B[0m     trials_save_file\u001B[38;5;241m=\u001B[39mtrials_save_file,\n",
       "\u001B[1;32m    689\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-3b6f5673-d2fc-417a-8640-653164380a7e/lib/python3.12/site-packages/hyperopt/fmin.py:586\u001B[0m, in \u001B[0;36mfmin\u001B[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001B[0m\n",
       "\u001B[1;32m    583\u001B[0m rval\u001B[38;5;241m.\u001B[39mcatch_eval_exceptions \u001B[38;5;241m=\u001B[39m catch_eval_exceptions\n",
       "\u001B[1;32m    585\u001B[0m \u001B[38;5;66;03m# next line is where the fmin is actually executed\u001B[39;00m\n",
       "\u001B[0;32m--> 586\u001B[0m rval\u001B[38;5;241m.\u001B[39mexhaust()\n",
       "\u001B[1;32m    588\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_argmin:\n",
       "\u001B[1;32m    589\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(trials\u001B[38;5;241m.\u001B[39mtrials) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-3b6f5673-d2fc-417a-8640-653164380a7e/lib/python3.12/site-packages/hyperopt/fmin.py:364\u001B[0m, in \u001B[0;36mFMinIter.exhaust\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m    362\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mexhaust\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n",
       "\u001B[1;32m    363\u001B[0m     n_done \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrials)\n",
       "\u001B[0;32m--> 364\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrun(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_evals \u001B[38;5;241m-\u001B[39m n_done, block_until_done\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39masynchronous)\n",
       "\u001B[1;32m    365\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrials\u001B[38;5;241m.\u001B[39mrefresh()\n",
       "\u001B[1;32m    366\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-3b6f5673-d2fc-417a-8640-653164380a7e/lib/python3.12/site-packages/hyperopt/fmin.py:300\u001B[0m, in \u001B[0;36mFMinIter.run\u001B[0;34m(self, N, block_until_done)\u001B[0m\n",
       "\u001B[1;32m    297\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpoll_interval_secs)\n",
       "\u001B[1;32m    298\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    299\u001B[0m     \u001B[38;5;66;03m# -- loop over trials and do the jobs directly\u001B[39;00m\n",
       "\u001B[0;32m--> 300\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mserial_evaluate()\n",
       "\u001B[1;32m    302\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrials\u001B[38;5;241m.\u001B[39mrefresh()\n",
       "\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrials_save_file \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-3b6f5673-d2fc-417a-8640-653164380a7e/lib/python3.12/site-packages/hyperopt/fmin.py:178\u001B[0m, in \u001B[0;36mFMinIter.serial_evaluate\u001B[0;34m(self, N)\u001B[0m\n",
       "\u001B[1;32m    176\u001B[0m ctrl \u001B[38;5;241m=\u001B[39m base\u001B[38;5;241m.\u001B[39mCtrl(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrials, current_trial\u001B[38;5;241m=\u001B[39mtrial)\n",
       "\u001B[1;32m    177\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 178\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdomain\u001B[38;5;241m.\u001B[39mevaluate(spec, ctrl)\n",
       "\u001B[1;32m    179\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    180\u001B[0m     logger\u001B[38;5;241m.\u001B[39merror(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjob exception: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mstr\u001B[39m(e))\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-3b6f5673-d2fc-417a-8640-653164380a7e/lib/python3.12/site-packages/hyperopt/base.py:892\u001B[0m, in \u001B[0;36mDomain.evaluate\u001B[0;34m(self, config, ctrl, attach_attachments)\u001B[0m\n",
       "\u001B[1;32m    883\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    884\u001B[0m     \u001B[38;5;66;03m# -- the \"work\" of evaluating `config` can be written\u001B[39;00m\n",
       "\u001B[1;32m    885\u001B[0m     \u001B[38;5;66;03m#    either into the pyll part (self.expr)\u001B[39;00m\n",
       "\u001B[1;32m    886\u001B[0m     \u001B[38;5;66;03m#    or the normal Python part (self.fn)\u001B[39;00m\n",
       "\u001B[1;32m    887\u001B[0m     pyll_rval \u001B[38;5;241m=\u001B[39m pyll\u001B[38;5;241m.\u001B[39mrec_eval(\n",
       "\u001B[1;32m    888\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexpr,\n",
       "\u001B[1;32m    889\u001B[0m         memo\u001B[38;5;241m=\u001B[39mmemo,\n",
       "\u001B[1;32m    890\u001B[0m         print_node_on_error\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrec_eval_print_node_on_error,\n",
       "\u001B[1;32m    891\u001B[0m     )\n",
       "\u001B[0;32m--> 892\u001B[0m     rval \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfn(pyll_rval)\n",
       "\u001B[1;32m    894\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(rval, (\u001B[38;5;28mfloat\u001B[39m, \u001B[38;5;28mint\u001B[39m, np\u001B[38;5;241m.\u001B[39mnumber)):\n",
       "\u001B[1;32m    895\u001B[0m     dict_rval \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mfloat\u001B[39m(rval), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstatus\u001B[39m\u001B[38;5;124m\"\u001B[39m: STATUS_OK}\n",
       "\n",
       "File \u001B[0;32m<command-8141152329916617>, line 272\u001B[0m, in \u001B[0;36mtrain_model_optimized.<locals>.objective\u001B[0;34m(params)\u001B[0m\n",
       "\u001B[1;32m    255\u001B[0m     model \u001B[38;5;241m=\u001B[39m ModelClass(\n",
       "\u001B[1;32m    256\u001B[0m         featuresCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeatures\u001B[39m\u001B[38;5;124m\"\u001B[39m, labelCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    257\u001B[0m         maxIter\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mint\u001B[39m(params[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmaxIter\u001B[39m\u001B[38;5;124m'\u001B[39m]),\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    260\u001B[0m         seed\u001B[38;5;241m=\u001B[39mConfig\u001B[38;5;241m.\u001B[39mRANDOM_SEED\n",
       "\u001B[1;32m    261\u001B[0m     )\n",
       "\u001B[1;32m    263\u001B[0m cv \u001B[38;5;241m=\u001B[39m CrossValidator(\n",
       "\u001B[1;32m    264\u001B[0m     estimator\u001B[38;5;241m=\u001B[39mmodel,\n",
       "\u001B[1;32m    265\u001B[0m     estimatorParamMaps\u001B[38;5;241m=\u001B[39m[{}],\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    269\u001B[0m     parallelism\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m\n",
       "\u001B[1;32m    270\u001B[0m )\n",
       "\u001B[0;32m--> 272\u001B[0m cv_model \u001B[38;5;241m=\u001B[39m cv\u001B[38;5;241m.\u001B[39mfit(train_data)\n",
       "\u001B[1;32m    273\u001B[0m avg_auc \u001B[38;5;241m=\u001B[39m cv_model\u001B[38;5;241m.\u001B[39mavgMetrics[\u001B[38;5;241m0\u001B[39m]\n",
       "\u001B[1;32m    274\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m-\u001B[39mavg_auc, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstatus\u001B[39m\u001B[38;5;124m'\u001B[39m: STATUS_OK}\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/MLWorkloadsInstrumentation/_pyspark.py:30\u001B[0m, in \u001B[0;36m_create_patch_function.<locals>.patched_method\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     28\u001B[0m call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
       "\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 30\u001B[0m     result \u001B[38;5;241m=\u001B[39m original_method(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     31\u001B[0m     call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/ml/base.py:203\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n",
       "\u001B[1;32m    201\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n",
       "\u001B[1;32m    202\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit(dataset)\n",
       "\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    205\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n",
       "\u001B[1;32m    206\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    207\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params)\n",
       "\u001B[1;32m    208\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/ml/tuning.py:890\u001B[0m, in \u001B[0;36mCrossValidator._fit\u001B[0;34m(self, dataset)\u001B[0m\n",
       "\u001B[1;32m    888\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    889\u001B[0m     bestIndex \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39margmin(metrics)\n",
       "\u001B[0;32m--> 890\u001B[0m bestModel \u001B[38;5;241m=\u001B[39m est\u001B[38;5;241m.\u001B[39mfit(dataset, epm[bestIndex])\n",
       "\u001B[1;32m    891\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_copyValues(\n",
       "\u001B[1;32m    892\u001B[0m     CrossValidatorModel(bestModel, metrics, cast(List[List[Model]], subModels), std_metrics)\n",
       "\u001B[1;32m    893\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/MLWorkloadsInstrumentation/_pyspark.py:30\u001B[0m, in \u001B[0;36m_create_patch_function.<locals>.patched_method\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     28\u001B[0m call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
       "\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 30\u001B[0m     result \u001B[38;5;241m=\u001B[39m original_method(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     31\u001B[0m     call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/ml/base.py:203\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n",
       "\u001B[1;32m    201\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n",
       "\u001B[1;32m    202\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit(dataset)\n",
       "\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    205\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n",
       "\u001B[1;32m    206\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    207\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params)\n",
       "\u001B[1;32m    208\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/ml/util.py:321\u001B[0m, in \u001B[0;36mtry_remote_fit.<locals>.wrapped\u001B[0;34m(self, dataset)\u001B[0m\n",
       "\u001B[1;32m    313\u001B[0m command \u001B[38;5;241m=\u001B[39m pb2\u001B[38;5;241m.\u001B[39mCommand()\n",
       "\u001B[1;32m    314\u001B[0m command\u001B[38;5;241m.\u001B[39mml_command\u001B[38;5;241m.\u001B[39mfit\u001B[38;5;241m.\u001B[39mCopyFrom(\n",
       "\u001B[1;32m    315\u001B[0m     pb2\u001B[38;5;241m.\u001B[39mMlCommand\u001B[38;5;241m.\u001B[39mFit(\n",
       "\u001B[1;32m    316\u001B[0m         estimator\u001B[38;5;241m=\u001B[39mestimator,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    319\u001B[0m     )\n",
       "\u001B[1;32m    320\u001B[0m )\n",
       "\u001B[0;32m--> 321\u001B[0m (_, properties, _) \u001B[38;5;241m=\u001B[39m client\u001B[38;5;241m.\u001B[39mexecute_command(command)\n",
       "\u001B[1;32m    322\u001B[0m model_info \u001B[38;5;241m=\u001B[39m deserialize(properties)\n",
       "\u001B[1;32m    323\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m warning_msg \u001B[38;5;241m:=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(model_info, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwarning_message\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1556\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n",
       "\u001B[1;32m   1554\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n",
       "\u001B[1;32m   1555\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n",
       "\u001B[0;32m-> 1556\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n",
       "\u001B[1;32m   1557\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n",
       "\u001B[1;32m   1558\u001B[0m )\n",
       "\u001B[1;32m   1559\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n",
       "\u001B[1;32m   1560\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2059\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n",
       "\u001B[1;32m   2056\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n",
       "\u001B[1;32m   2058\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n",
       "\u001B[0;32m-> 2059\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n",
       "\u001B[1;32m   2060\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n",
       "\u001B[1;32m   2061\u001B[0m     ):\n",
       "\u001B[1;32m   2062\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n",
       "\u001B[1;32m   2063\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2035\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n",
       "\u001B[1;32m   2033\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n",
       "\u001B[1;32m   2034\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 2035\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2355\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2354\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2355\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2356\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n",
       "\u001B[1;32m   2357\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2429\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   2431\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[0;32m-> 2433\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   2434\u001B[0m                 info,\n",
       "\u001B[1;32m   2435\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2436\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   2437\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   2438\u001B[0m                 status_code,\n",
       "\u001B[1;32m   2439\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n",
       "\u001B[1;32m   2442\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2443\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[1;32m   2444\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n",
       "\u001B[1;32m   2445\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2446\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mUnknownException\u001B[0m: (DENY_NEW_AND_EXISTING_RESOURCES) BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "UnknownException",
        "evalue": "(DENY_NEW_AND_EXISTING_RESOURCES) BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow."
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>UnknownException</span>: (DENY_NEW_AND_EXISTING_RESOURCES) BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow.\n[Trace ID: 00-abc889cc13f8be69b4d7974e5191e4d3-e33776309218a584-00]"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": null,
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": "",
        "sqlState": "XXKCM",
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mUnknownException\u001B[0m                          Traceback (most recent call last)",
        "File \u001B[0;32m<command-8141152329916617>, line 484\u001B[0m\n\u001B[1;32m    482\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    483\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m‚ùå An error occurred during the pipeline execution: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 484\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    486\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    487\u001B[0m     mlflow\u001B[38;5;241m.\u001B[39mend_run()\n",
        "File \u001B[0;32m<command-8141152329916617>, line 479\u001B[0m\n\u001B[1;32m    476\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    478\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 479\u001B[0m         run_complete_experiments()\n\u001B[1;32m    480\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m‚úÖ All experiments complete and MLflow runs closed successfully!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    482\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
        "File \u001B[0;32m<command-8141152329916617>, line 422\u001B[0m, in \u001B[0;36mrun_complete_experiments\u001B[0;34m()\u001B[0m\n\u001B[1;32m    420\u001B[0m \u001B[38;5;66;03m# 3. In-Flight Random Forest\u001B[39;00m\n\u001B[1;32m    421\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m mlflow\u001B[38;5;241m.\u001B[39mstart_run(run_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRF_In_Flight\u001B[39m\u001B[38;5;124m\"\u001B[39m, nested\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m--> 422\u001B[0m     m, metrics, p, cv \u001B[38;5;241m=\u001B[39m train_model_optimized(train_in, test_in, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRandomForest\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIn-Flight\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    423\u001B[0m     mlflow\u001B[38;5;241m.\u001B[39mlog_params(p)\n\u001B[1;32m    424\u001B[0m     mlflow\u001B[38;5;241m.\u001B[39mlog_metric(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcv_score\u001B[39m\u001B[38;5;124m\"\u001B[39m, cv)\n",
        "File \u001B[0;32m<command-8141152329916617>, line 279\u001B[0m, in \u001B[0;36mtrain_model_optimized\u001B[0;34m(train_data, test_data, model_name, model_type)\u001B[0m\n\u001B[1;32m    276\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m   Optimizing over \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mConfig\u001B[38;5;241m.\u001B[39mBAYES_MAX_EVALS\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m iterations...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    277\u001B[0m trials \u001B[38;5;241m=\u001B[39m Trials()\n\u001B[0;32m--> 279\u001B[0m best \u001B[38;5;241m=\u001B[39m fmin(\n\u001B[1;32m    280\u001B[0m     fn\u001B[38;5;241m=\u001B[39mobjective, space\u001B[38;5;241m=\u001B[39mspace, algo\u001B[38;5;241m=\u001B[39mtpe\u001B[38;5;241m.\u001B[39msuggest,\n\u001B[1;32m    281\u001B[0m     max_evals\u001B[38;5;241m=\u001B[39mConfig\u001B[38;5;241m.\u001B[39mBAYES_MAX_EVALS, trials\u001B[38;5;241m=\u001B[39mtrials,\n\u001B[1;32m    282\u001B[0m     rstate\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mdefault_rng(Config\u001B[38;5;241m.\u001B[39mRANDOM_SEED),\n\u001B[1;32m    283\u001B[0m     verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    284\u001B[0m )\n\u001B[1;32m    286\u001B[0m best_params_actual \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m    287\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m best\u001B[38;5;241m.\u001B[39mitems():\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-3b6f5673-d2fc-417a-8640-653164380a7e/lib/python3.12/site-packages/hyperopt/fmin.py:540\u001B[0m, in \u001B[0;36mfmin\u001B[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001B[0m\n\u001B[1;32m    537\u001B[0m     fn \u001B[38;5;241m=\u001B[39m __objective_fmin_wrapper(fn)\n\u001B[1;32m    539\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m allow_trials_fmin \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(trials, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfmin\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 540\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m trials\u001B[38;5;241m.\u001B[39mfmin(\n\u001B[1;32m    541\u001B[0m         fn,\n\u001B[1;32m    542\u001B[0m         space,\n\u001B[1;32m    543\u001B[0m         algo\u001B[38;5;241m=\u001B[39malgo,\n\u001B[1;32m    544\u001B[0m         max_evals\u001B[38;5;241m=\u001B[39mmax_evals,\n\u001B[1;32m    545\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mtimeout,\n\u001B[1;32m    546\u001B[0m         loss_threshold\u001B[38;5;241m=\u001B[39mloss_threshold,\n\u001B[1;32m    547\u001B[0m         max_queue_len\u001B[38;5;241m=\u001B[39mmax_queue_len,\n\u001B[1;32m    548\u001B[0m         rstate\u001B[38;5;241m=\u001B[39mrstate,\n\u001B[1;32m    549\u001B[0m         pass_expr_memo_ctrl\u001B[38;5;241m=\u001B[39mpass_expr_memo_ctrl,\n\u001B[1;32m    550\u001B[0m         verbose\u001B[38;5;241m=\u001B[39mverbose,\n\u001B[1;32m    551\u001B[0m         catch_eval_exceptions\u001B[38;5;241m=\u001B[39mcatch_eval_exceptions,\n\u001B[1;32m    552\u001B[0m         return_argmin\u001B[38;5;241m=\u001B[39mreturn_argmin,\n\u001B[1;32m    553\u001B[0m         show_progressbar\u001B[38;5;241m=\u001B[39mshow_progressbar,\n\u001B[1;32m    554\u001B[0m         early_stop_fn\u001B[38;5;241m=\u001B[39mearly_stop_fn,\n\u001B[1;32m    555\u001B[0m         trials_save_file\u001B[38;5;241m=\u001B[39mtrials_save_file,\n\u001B[1;32m    556\u001B[0m     )\n\u001B[1;32m    558\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m trials \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    559\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(trials_save_file):\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-3b6f5673-d2fc-417a-8640-653164380a7e/lib/python3.12/site-packages/hyperopt/base.py:671\u001B[0m, in \u001B[0;36mTrials.fmin\u001B[0;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001B[0m\n\u001B[1;32m    666\u001B[0m \u001B[38;5;66;03m# -- Stop-gap implementation!\u001B[39;00m\n\u001B[1;32m    667\u001B[0m \u001B[38;5;66;03m#    fmin should have been a Trials method in the first place\u001B[39;00m\n\u001B[1;32m    668\u001B[0m \u001B[38;5;66;03m#    but for now it's still sitting in another file.\u001B[39;00m\n\u001B[1;32m    669\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfmin\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m fmin\n\u001B[0;32m--> 671\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m fmin(\n\u001B[1;32m    672\u001B[0m     fn,\n\u001B[1;32m    673\u001B[0m     space,\n\u001B[1;32m    674\u001B[0m     algo\u001B[38;5;241m=\u001B[39malgo,\n\u001B[1;32m    675\u001B[0m     max_evals\u001B[38;5;241m=\u001B[39mmax_evals,\n\u001B[1;32m    676\u001B[0m     timeout\u001B[38;5;241m=\u001B[39mtimeout,\n\u001B[1;32m    677\u001B[0m     loss_threshold\u001B[38;5;241m=\u001B[39mloss_threshold,\n\u001B[1;32m    678\u001B[0m     trials\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    679\u001B[0m     rstate\u001B[38;5;241m=\u001B[39mrstate,\n\u001B[1;32m    680\u001B[0m     verbose\u001B[38;5;241m=\u001B[39mverbose,\n\u001B[1;32m    681\u001B[0m     max_queue_len\u001B[38;5;241m=\u001B[39mmax_queue_len,\n\u001B[1;32m    682\u001B[0m     allow_trials_fmin\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,  \u001B[38;5;66;03m# -- prevent recursion\u001B[39;00m\n\u001B[1;32m    683\u001B[0m     pass_expr_memo_ctrl\u001B[38;5;241m=\u001B[39mpass_expr_memo_ctrl,\n\u001B[1;32m    684\u001B[0m     catch_eval_exceptions\u001B[38;5;241m=\u001B[39mcatch_eval_exceptions,\n\u001B[1;32m    685\u001B[0m     return_argmin\u001B[38;5;241m=\u001B[39mreturn_argmin,\n\u001B[1;32m    686\u001B[0m     show_progressbar\u001B[38;5;241m=\u001B[39mshow_progressbar,\n\u001B[1;32m    687\u001B[0m     early_stop_fn\u001B[38;5;241m=\u001B[39mearly_stop_fn,\n\u001B[1;32m    688\u001B[0m     trials_save_file\u001B[38;5;241m=\u001B[39mtrials_save_file,\n\u001B[1;32m    689\u001B[0m )\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-3b6f5673-d2fc-417a-8640-653164380a7e/lib/python3.12/site-packages/hyperopt/fmin.py:586\u001B[0m, in \u001B[0;36mfmin\u001B[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001B[0m\n\u001B[1;32m    583\u001B[0m rval\u001B[38;5;241m.\u001B[39mcatch_eval_exceptions \u001B[38;5;241m=\u001B[39m catch_eval_exceptions\n\u001B[1;32m    585\u001B[0m \u001B[38;5;66;03m# next line is where the fmin is actually executed\u001B[39;00m\n\u001B[0;32m--> 586\u001B[0m rval\u001B[38;5;241m.\u001B[39mexhaust()\n\u001B[1;32m    588\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_argmin:\n\u001B[1;32m    589\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(trials\u001B[38;5;241m.\u001B[39mtrials) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-3b6f5673-d2fc-417a-8640-653164380a7e/lib/python3.12/site-packages/hyperopt/fmin.py:364\u001B[0m, in \u001B[0;36mFMinIter.exhaust\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    362\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mexhaust\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    363\u001B[0m     n_done \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrials)\n\u001B[0;32m--> 364\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrun(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_evals \u001B[38;5;241m-\u001B[39m n_done, block_until_done\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39masynchronous)\n\u001B[1;32m    365\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrials\u001B[38;5;241m.\u001B[39mrefresh()\n\u001B[1;32m    366\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-3b6f5673-d2fc-417a-8640-653164380a7e/lib/python3.12/site-packages/hyperopt/fmin.py:300\u001B[0m, in \u001B[0;36mFMinIter.run\u001B[0;34m(self, N, block_until_done)\u001B[0m\n\u001B[1;32m    297\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpoll_interval_secs)\n\u001B[1;32m    298\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    299\u001B[0m     \u001B[38;5;66;03m# -- loop over trials and do the jobs directly\u001B[39;00m\n\u001B[0;32m--> 300\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mserial_evaluate()\n\u001B[1;32m    302\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrials\u001B[38;5;241m.\u001B[39mrefresh()\n\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrials_save_file \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-3b6f5673-d2fc-417a-8640-653164380a7e/lib/python3.12/site-packages/hyperopt/fmin.py:178\u001B[0m, in \u001B[0;36mFMinIter.serial_evaluate\u001B[0;34m(self, N)\u001B[0m\n\u001B[1;32m    176\u001B[0m ctrl \u001B[38;5;241m=\u001B[39m base\u001B[38;5;241m.\u001B[39mCtrl(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrials, current_trial\u001B[38;5;241m=\u001B[39mtrial)\n\u001B[1;32m    177\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 178\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdomain\u001B[38;5;241m.\u001B[39mevaluate(spec, ctrl)\n\u001B[1;32m    179\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    180\u001B[0m     logger\u001B[38;5;241m.\u001B[39merror(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjob exception: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mstr\u001B[39m(e))\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-3b6f5673-d2fc-417a-8640-653164380a7e/lib/python3.12/site-packages/hyperopt/base.py:892\u001B[0m, in \u001B[0;36mDomain.evaluate\u001B[0;34m(self, config, ctrl, attach_attachments)\u001B[0m\n\u001B[1;32m    883\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    884\u001B[0m     \u001B[38;5;66;03m# -- the \"work\" of evaluating `config` can be written\u001B[39;00m\n\u001B[1;32m    885\u001B[0m     \u001B[38;5;66;03m#    either into the pyll part (self.expr)\u001B[39;00m\n\u001B[1;32m    886\u001B[0m     \u001B[38;5;66;03m#    or the normal Python part (self.fn)\u001B[39;00m\n\u001B[1;32m    887\u001B[0m     pyll_rval \u001B[38;5;241m=\u001B[39m pyll\u001B[38;5;241m.\u001B[39mrec_eval(\n\u001B[1;32m    888\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexpr,\n\u001B[1;32m    889\u001B[0m         memo\u001B[38;5;241m=\u001B[39mmemo,\n\u001B[1;32m    890\u001B[0m         print_node_on_error\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrec_eval_print_node_on_error,\n\u001B[1;32m    891\u001B[0m     )\n\u001B[0;32m--> 892\u001B[0m     rval \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfn(pyll_rval)\n\u001B[1;32m    894\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(rval, (\u001B[38;5;28mfloat\u001B[39m, \u001B[38;5;28mint\u001B[39m, np\u001B[38;5;241m.\u001B[39mnumber)):\n\u001B[1;32m    895\u001B[0m     dict_rval \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mfloat\u001B[39m(rval), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstatus\u001B[39m\u001B[38;5;124m\"\u001B[39m: STATUS_OK}\n",
        "File \u001B[0;32m<command-8141152329916617>, line 272\u001B[0m, in \u001B[0;36mtrain_model_optimized.<locals>.objective\u001B[0;34m(params)\u001B[0m\n\u001B[1;32m    255\u001B[0m     model \u001B[38;5;241m=\u001B[39m ModelClass(\n\u001B[1;32m    256\u001B[0m         featuresCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeatures\u001B[39m\u001B[38;5;124m\"\u001B[39m, labelCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    257\u001B[0m         maxIter\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mint\u001B[39m(params[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmaxIter\u001B[39m\u001B[38;5;124m'\u001B[39m]),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    260\u001B[0m         seed\u001B[38;5;241m=\u001B[39mConfig\u001B[38;5;241m.\u001B[39mRANDOM_SEED\n\u001B[1;32m    261\u001B[0m     )\n\u001B[1;32m    263\u001B[0m cv \u001B[38;5;241m=\u001B[39m CrossValidator(\n\u001B[1;32m    264\u001B[0m     estimator\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m    265\u001B[0m     estimatorParamMaps\u001B[38;5;241m=\u001B[39m[{}],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    269\u001B[0m     parallelism\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    270\u001B[0m )\n\u001B[0;32m--> 272\u001B[0m cv_model \u001B[38;5;241m=\u001B[39m cv\u001B[38;5;241m.\u001B[39mfit(train_data)\n\u001B[1;32m    273\u001B[0m avg_auc \u001B[38;5;241m=\u001B[39m cv_model\u001B[38;5;241m.\u001B[39mavgMetrics[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    274\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m-\u001B[39mavg_auc, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstatus\u001B[39m\u001B[38;5;124m'\u001B[39m: STATUS_OK}\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/MLWorkloadsInstrumentation/_pyspark.py:30\u001B[0m, in \u001B[0;36m_create_patch_function.<locals>.patched_method\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     28\u001B[0m call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 30\u001B[0m     result \u001B[38;5;241m=\u001B[39m original_method(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     31\u001B[0m     call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/ml/base.py:203\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    201\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n\u001B[1;32m    202\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit(dataset)\n\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    205\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    206\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    207\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params)\n\u001B[1;32m    208\u001B[0m     )\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/ml/tuning.py:890\u001B[0m, in \u001B[0;36mCrossValidator._fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    888\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    889\u001B[0m     bestIndex \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39margmin(metrics)\n\u001B[0;32m--> 890\u001B[0m bestModel \u001B[38;5;241m=\u001B[39m est\u001B[38;5;241m.\u001B[39mfit(dataset, epm[bestIndex])\n\u001B[1;32m    891\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_copyValues(\n\u001B[1;32m    892\u001B[0m     CrossValidatorModel(bestModel, metrics, cast(List[List[Model]], subModels), std_metrics)\n\u001B[1;32m    893\u001B[0m )\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/MLWorkloadsInstrumentation/_pyspark.py:30\u001B[0m, in \u001B[0;36m_create_patch_function.<locals>.patched_method\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     28\u001B[0m call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 30\u001B[0m     result \u001B[38;5;241m=\u001B[39m original_method(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     31\u001B[0m     call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/ml/base.py:203\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    201\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n\u001B[1;32m    202\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit(dataset)\n\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    205\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    206\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    207\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params)\n\u001B[1;32m    208\u001B[0m     )\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/ml/util.py:321\u001B[0m, in \u001B[0;36mtry_remote_fit.<locals>.wrapped\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    313\u001B[0m command \u001B[38;5;241m=\u001B[39m pb2\u001B[38;5;241m.\u001B[39mCommand()\n\u001B[1;32m    314\u001B[0m command\u001B[38;5;241m.\u001B[39mml_command\u001B[38;5;241m.\u001B[39mfit\u001B[38;5;241m.\u001B[39mCopyFrom(\n\u001B[1;32m    315\u001B[0m     pb2\u001B[38;5;241m.\u001B[39mMlCommand\u001B[38;5;241m.\u001B[39mFit(\n\u001B[1;32m    316\u001B[0m         estimator\u001B[38;5;241m=\u001B[39mestimator,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    319\u001B[0m     )\n\u001B[1;32m    320\u001B[0m )\n\u001B[0;32m--> 321\u001B[0m (_, properties, _) \u001B[38;5;241m=\u001B[39m client\u001B[38;5;241m.\u001B[39mexecute_command(command)\n\u001B[1;32m    322\u001B[0m model_info \u001B[38;5;241m=\u001B[39m deserialize(properties)\n\u001B[1;32m    323\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m warning_msg \u001B[38;5;241m:=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(model_info, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwarning_message\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1556\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n\u001B[1;32m   1554\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n\u001B[1;32m   1555\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n\u001B[0;32m-> 1556\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n\u001B[1;32m   1557\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n\u001B[1;32m   1558\u001B[0m )\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n\u001B[1;32m   1560\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2059\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n\u001B[1;32m   2056\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   2058\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m-> 2059\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n\u001B[1;32m   2060\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n\u001B[1;32m   2061\u001B[0m     ):\n\u001B[1;32m   2062\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   2063\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2035\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n\u001B[1;32m   2033\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[1;32m   2034\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 2035\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2355\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2354\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2355\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2356\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[1;32m   2357\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2429\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2431\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[0;32m-> 2433\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2434\u001B[0m                 info,\n\u001B[1;32m   2435\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2436\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2437\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2438\u001B[0m                 status_code,\n\u001B[1;32m   2439\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2442\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2443\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[1;32m   2444\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n\u001B[1;32m   2445\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2446\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mUnknownException\u001B[0m: (DENY_NEW_AND_EXISTING_RESOURCES) BAD_REQUEST: Sorry, cannot run the resource because you have hit your free daily limit. Please come back again tomorrow."
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# #!/usr/bin/env python3\n",
    "# \"\"\"\n",
    "# Optimized ML Experiments for Databricks Community Edition\n",
    "# - FIXED: Logs models to workspace registry with signatures\n",
    "# - Models can be registered to UC manually via UI/API after logging\n",
    "# - Avoids MLeap dependency issue on Community Edition\n",
    "# \"\"\"\n",
    "\n",
    "# import warnings\n",
    "# import os\n",
    "# import mlflow\n",
    "# import mlflow.spark\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "# from pyspark.ml.classification import GBTClassifier, RandomForestClassifier\n",
    "# from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "# from pyspark.ml.tuning import CrossValidator\n",
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.functions import col, udf\n",
    "# from pyspark.sql.types import DoubleType\n",
    "# from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "# # Signature imports\n",
    "# from mlflow.models.signature import ModelSignature\n",
    "# from mlflow.types.schema import Schema, TensorSpec\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # CONFIGURATION\n",
    "# # =============================================================================\n",
    "\n",
    "# class Config:\n",
    "#     \"\"\"Optimized for Databricks Community Edition\"\"\"\n",
    "    \n",
    "#     TOP_K_FEATURES = 40 \n",
    "#     CV_FOLDS = 2\n",
    "#     BAYES_MAX_EVALS = 4\n",
    "#     TEST_RATIO = 0.2\n",
    "#     RANDOM_SEED = 42\n",
    "    \n",
    "#     GOLD_TABLE = \"default.gold_ml_features_experimental\"\n",
    "#     EXPERIMENT_NAME = \"/Shared/Flightmasters_Optimized_Experiments\"\n",
    "    \n",
    "#     # CRITICAL CHANGE: Use workspace registry for initial logging\n",
    "#     MLFLOW_TRACKING_URI = \"databricks\"\n",
    "#     MLFLOW_REGISTRY_URI = \"databricks\"  \n",
    "    \n",
    "#     # Model names (without UC prefix for workspace registry)\n",
    "#     MODEL_RF_PRE = \"model_rf_pre\"\n",
    "#     MODEL_GBT_PRE = \"model_gbt_pre\"\n",
    "#     MODEL_RF_IN = \"model_rf_in\"\n",
    "#     MODEL_GBT_IN = \"model_gbt_in\"\n",
    "    \n",
    "#     # UC settings (for manual registration later)\n",
    "#     UC_CATALOG = \"workspace\"\n",
    "#     UC_SCHEMA = \"default\"\n",
    "#     UC_VOLUME_NAME = \"mlflow_shared_tmp\"\n",
    "    \n",
    "#     USE_CHECKPOINTING = True\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # HELPERS\n",
    "# # =============================================================================\n",
    "\n",
    "# def setup_uc_volume(spark):\n",
    "#     \"\"\"Setup Unity Catalog Volume for artifacts\"\"\"\n",
    "#     catalog = Config.UC_CATALOG\n",
    "#     schema = Config.UC_SCHEMA\n",
    "#     volume_name = Config.UC_VOLUME_NAME\n",
    "    \n",
    "#     volume_path = f\"{catalog}.{schema}.{volume_name}\"\n",
    "#     env_path = f\"/Volumes/{catalog}/{schema}/{volume_name}\"\n",
    "    \n",
    "#     print(\"\\n\" + \"=\" * 80)\n",
    "#     print(f\"UNITY CATALOG VOLUME SETUP\")\n",
    "#     print(\"=\" * 80)\n",
    "#     print(f\"Volume Target: {volume_path}\")\n",
    "    \n",
    "#     try:\n",
    "#         volume_exists = spark.sql(f\"SHOW VOLUMES IN {catalog}.{schema}\").filter(\n",
    "#             col(\"volume_name\") == volume_name\n",
    "#         ).count() > 0\n",
    "        \n",
    "#         if not volume_exists:\n",
    "#             spark.sql(f\"CREATE VOLUME {volume_path}\")\n",
    "#             print(\"‚úÖ Volume created successfully.\")\n",
    "#         else:\n",
    "#             print(\"‚úÖ Volume already exists.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ö†Ô∏è  WARNING: Could not check/create volume: {e}\")\n",
    "#         pass\n",
    "        \n",
    "#     os.environ['MLFLOW_DFS_TMP'] = env_path\n",
    "#     os.environ['SPARKML_TEMP_DFS_PATH'] = env_path\n",
    "    \n",
    "#     print(f\"‚úÖ Environment paths set to: {env_path}\")\n",
    "#     return volume_path\n",
    "\n",
    "\n",
    "# def create_safe_vector_slicer(indices_to_keep):\n",
    "#     \"\"\"Create UDF to slice vectors\"\"\"\n",
    "#     indices_list = list(indices_to_keep)\n",
    "    \n",
    "#     @udf(returnType=VectorUDT())\n",
    "#     def safe_slicer(features):\n",
    "#         if features is None: \n",
    "#             return None\n",
    "#         max_idx = features.size - 1\n",
    "#         selected_values = [float(features[i]) for i in indices_list if i <= max_idx]\n",
    "#         return Vectors.dense(selected_values)\n",
    "    \n",
    "#     return safe_slicer\n",
    "\n",
    "\n",
    "# def checkpoint_if_enabled(df, eager=True):\n",
    "#     \"\"\"Checkpoint data to prevent OOM\"\"\"\n",
    "#     if Config.USE_CHECKPOINTING:\n",
    "#         return df.localCheckpoint(eager=eager)\n",
    "#     return df\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # 1. FEATURE IMPORTANCE\n",
    "# # =============================================================================\n",
    "\n",
    "# def analyze_feature_importance(train_data, top_k):\n",
    "#     print(\"\\n\" + \"=\" * 80)\n",
    "#     print(\"PHASE 1: FEATURE IMPORTANCE ANALYSIS\")\n",
    "#     print(\"=\" * 80)\n",
    "    \n",
    "#     sample = train_data.select(\"features\").first()\n",
    "#     if not sample:\n",
    "#         raise ValueError(\"Training data is empty or features column is missing.\")\n",
    "        \n",
    "#     total_features = sample.features.size\n",
    "#     print(f\"\\nüìä Original features: {total_features}\")\n",
    "    \n",
    "#     print(\"\\nüå≤ Training Random Forest for feature ranking...\")\n",
    "#     rf = RandomForestClassifier(\n",
    "#         featuresCol=\"features\", labelCol=\"label\",\n",
    "#         numTrees=30, maxDepth=8, seed=Config.RANDOM_SEED\n",
    "#     )\n",
    "    \n",
    "#     rf_model = rf.fit(train_data)\n",
    "#     importances = rf_model.featureImportances.toArray()\n",
    "    \n",
    "#     top_k_indices = np.argsort(importances)[-top_k:][::-1]\n",
    "#     top_k_scores = importances[top_k_indices]\n",
    "    \n",
    "#     selected_importance = np.sum(top_k_scores)\n",
    "#     total_importance = np.sum(importances)\n",
    "#     retention = (selected_importance / total_importance) * 100\n",
    "    \n",
    "#     print(f\"\\nüìà Feature Selection Summary:\")\n",
    "#     print(f\"   Original: {total_features} -> Selected: {top_k}\")\n",
    "#     print(f\"   Information retained: {retention:.1f}%\")\n",
    "    \n",
    "#     return total_features, top_k_indices.tolist(), retention\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # 2. CREATE DUAL DATASETS\n",
    "# # =============================================================================\n",
    "\n",
    "# def create_dual_datasets(df_gold, selected_indices, dep_delay_orig_index=11):\n",
    "#     print(\"\\n\" + \"=\" * 80)\n",
    "#     print(\"PHASE 2: CREATING DUAL DATASETS\")\n",
    "#     print(\"=\" * 80)\n",
    "    \n",
    "#     dep_delay_in_selected = dep_delay_orig_index in selected_indices\n",
    "#     if dep_delay_in_selected: \n",
    "#         print(f\"‚úÖ dep_delay found at original index {dep_delay_orig_index}\")\n",
    "#     else: \n",
    "#         print(f\"‚ö†Ô∏è  dep_delay (index {dep_delay_orig_index}) not in top-K\")\n",
    "    \n",
    "#     print(f\"\\nüîß Applying feature selection...\")\n",
    "#     slicer_udf = create_safe_vector_slicer(selected_indices)\n",
    "#     df_selected = df_gold.withColumn(\"features\", slicer_udf(col(\"features\"))).select(\"features\", \"label\")\n",
    "#     df_selected = df_selected.withColumn(\"label\", col(\"label\").cast(DoubleType()))\n",
    "#     df_selected = checkpoint_if_enabled(df_selected)\n",
    "    \n",
    "#     if dep_delay_in_selected:\n",
    "#         print(f\"\\nüìã Creating Pre-Departure (removing dep_delay)...\")\n",
    "#         dep_delay_new_index = selected_indices.index(dep_delay_orig_index)\n",
    "#         pre_dep_indices = [i for i in range(len(selected_indices)) if i != dep_delay_new_index]\n",
    "#         pre_dep_slicer = create_safe_vector_slicer(pre_dep_indices)\n",
    "#         df_pre_dep = df_selected.withColumn(\"features\", pre_dep_slicer(col(\"features\"))).select(\"features\", \"label\")\n",
    "#     else:\n",
    "#         df_pre_dep = df_selected\n",
    "    \n",
    "#     df_in_flight = df_selected\n",
    "    \n",
    "#     df_pre_dep = checkpoint_if_enabled(df_pre_dep, eager=False)\n",
    "#     df_in_flight = checkpoint_if_enabled(df_in_flight, eager=False)\n",
    "    \n",
    "#     return df_pre_dep, df_in_flight, dep_delay_in_selected\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # 3. TRAIN/TEST SPLIT\n",
    "# # =============================================================================\n",
    "\n",
    "# def split_and_checkpoint(df, name):\n",
    "#     print(f\"\\nüîÄ Splitting {name} dataset...\")\n",
    "#     train, test = df.randomSplit([1.0 - Config.TEST_RATIO, Config.TEST_RATIO], seed=Config.RANDOM_SEED)\n",
    "#     print(f\"   Train count: {train.count():,}\")\n",
    "#     print(f\"   Test count: {test.count():,}\")\n",
    "    \n",
    "#     train = checkpoint_if_enabled(train, eager=True)\n",
    "#     test = checkpoint_if_enabled(test, eager=True)\n",
    "#     return train, test\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # 4. MODEL TRAINING\n",
    "# # =============================================================================\n",
    "\n",
    "# def train_model_optimized(train_data, test_data, model_name, model_type):\n",
    "#     \"\"\"Bayesian optimization with reduced CV folds\"\"\"\n",
    "    \n",
    "#     print(f\"\\n\\tüéØ Training {model_name} ({model_type})\")\n",
    "#     print(f\"\\t   {Config.CV_FOLDS}-fold CV + {Config.BAYES_MAX_EVALS} Bayesian evals\")\n",
    "    \n",
    "#     if model_name == \"RandomForest\":\n",
    "#         space = {\n",
    "#             'numTrees': hp.choice('numTrees', [50, 100]),\n",
    "#             'maxDepth': hp.choice('maxDepth', [10, 15]),\n",
    "#             'minInstancesPerNode': hp.choice('minInstancesPerNode', [25, 50])\n",
    "#         }\n",
    "#         ModelClass = RandomForestClassifier\n",
    "#         param_map = {'numTrees': [50, 100], 'maxDepth': [10, 15], 'minInstancesPerNode': [25, 50]}\n",
    "#     else:\n",
    "#         space = {\n",
    "#             'maxIter': hp.choice('maxIter', [50, 100]),\n",
    "#             'maxDepth': hp.choice('maxDepth', [4, 6]),\n",
    "#             'stepSize': hp.uniform('stepSize', 0.05, 0.15)\n",
    "#         }\n",
    "#         ModelClass = GBTClassifier\n",
    "#         param_map = {'maxIter': [50, 100], 'maxDepth': [4, 6]}\n",
    "    \n",
    "#     def objective(params):\n",
    "#         if model_name == \"RandomForest\":\n",
    "#             model = ModelClass(\n",
    "#                 featuresCol=\"features\", labelCol=\"label\",\n",
    "#                 numTrees=int(params['numTrees']),\n",
    "#                 maxDepth=int(params['maxDepth']),\n",
    "#                 minInstancesPerNode=int(params['minInstancesPerNode']),\n",
    "#                 seed=Config.RANDOM_SEED\n",
    "#             )\n",
    "#         else:\n",
    "#             model = ModelClass(\n",
    "#                 featuresCol=\"features\", labelCol=\"label\",\n",
    "#                 maxIter=int(params['maxIter']),\n",
    "#                 maxDepth=int(params['maxDepth']),\n",
    "#                 stepSize=float(params['stepSize']),\n",
    "#                 seed=Config.RANDOM_SEED\n",
    "#             )\n",
    "        \n",
    "#         cv = CrossValidator(\n",
    "#             estimator=model,\n",
    "#             estimatorParamMaps=[{}],\n",
    "#             evaluator=BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\"),\n",
    "#             numFolds=Config.CV_FOLDS,\n",
    "#             seed=Config.RANDOM_SEED,\n",
    "#             parallelism=1\n",
    "#         )\n",
    "        \n",
    "#         cv_model = cv.fit(train_data)\n",
    "#         avg_auc = cv_model.avgMetrics[0]\n",
    "#         return {'loss': -avg_auc, 'status': STATUS_OK}\n",
    "    \n",
    "#     print(f\"\\t   Optimizing over {Config.BAYES_MAX_EVALS} iterations...\")\n",
    "#     trials = Trials()\n",
    "    \n",
    "#     best = fmin(\n",
    "#         fn=objective, space=space, algo=tpe.suggest,\n",
    "#         max_evals=Config.BAYES_MAX_EVALS, trials=trials,\n",
    "#         rstate=np.random.default_rng(Config.RANDOM_SEED),\n",
    "#         verbose=False\n",
    "#     )\n",
    "    \n",
    "#     best_params_actual = {}\n",
    "#     for k, v in best.items():\n",
    "#         if k in param_map:\n",
    "#             best_params_actual[k] = param_map[k][v]\n",
    "#         else:\n",
    "#             best_params_actual[k] = v\n",
    "    \n",
    "#     print(f\"\\t   Best params: {best_params_actual}\")\n",
    "    \n",
    "#     final_model = ModelClass(\n",
    "#         featuresCol=\"features\", labelCol=\"label\",\n",
    "#         **best_params_actual, seed=Config.RANDOM_SEED\n",
    "#     ).fit(train_data)\n",
    "    \n",
    "#     predictions = final_model.transform(test_data)\n",
    "    \n",
    "#     metrics = {\n",
    "#         \"auc_roc\": BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\").evaluate(predictions),\n",
    "#         \"accuracy\": MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\").evaluate(predictions),\n",
    "#         \"f1_score\": MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"f1\").evaluate(predictions)\n",
    "#     }\n",
    "    \n",
    "#     cv_score = -min([t['result']['loss'] for t in trials.trials])\n",
    "#     print(f\"\\t   Best CV Score: {cv_score:.4f}\")\n",
    "#     print(f\"\\t   Test AUC-ROC: {metrics['auc_roc']:.4f}\")\n",
    "    \n",
    "#     return final_model, metrics, best_params_actual, cv_score\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # 5. MAIN PIPELINE (WORKSPACE REGISTRY LOGGING)\n",
    "# # =============================================================================\n",
    "\n",
    "# def run_complete_experiments():\n",
    "#     \"\"\"Main execution pipeline - logs to workspace registry\"\"\"\n",
    "    \n",
    "#     print(\"\\n\" + \"=\" * 80)\n",
    "#     print(\"FLIGHTMASTERS OPTIMIZED EXPERIMENTS\")\n",
    "#     print(\"Strategy: Log to Workspace Registry (bypasses MLeap issue)\")\n",
    "#     print(\"=\" * 80)\n",
    "    \n",
    "#     # Setup MLflow for workspace registry\n",
    "#     mlflow.set_tracking_uri(Config.MLFLOW_TRACKING_URI)\n",
    "#     mlflow.set_registry_uri(Config.MLFLOW_REGISTRY_URI)\n",
    "#     mlflow.set_experiment(Config.EXPERIMENT_NAME)\n",
    "    \n",
    "#     spark = SparkSession.builder.getOrCreate()\n",
    "#     setup_uc_volume(spark)\n",
    "    \n",
    "#     mlflow.end_run()\n",
    "    \n",
    "#     # === MASTER PARENT RUN ===\n",
    "#     with mlflow.start_run(run_name=\"Flight_Experiment_Master_Workspace\"):\n",
    "        \n",
    "#         # Load data\n",
    "#         print(f\"\\nüì• Loading Gold table...\")\n",
    "#         df_gold = spark.table(Config.GOLD_TABLE)\n",
    "#         df_gold = df_gold.withColumn(\"label\", col(\"label\").cast(DoubleType())).filter(\n",
    "#             (col(\"label\") == 0.0) | (col(\"label\") == 1.0)\n",
    "#         )\n",
    "#         df_gold = checkpoint_if_enabled(df_gold, eager=True)\n",
    "        \n",
    "#         train_full, test_full = df_gold.randomSplit([0.8, 0.2], seed=Config.RANDOM_SEED)\n",
    "#         train_full = checkpoint_if_enabled(train_full, eager=False)\n",
    "#         test_full = checkpoint_if_enabled(test_full, eager=False)\n",
    "\n",
    "#         # Feature selection\n",
    "#         with mlflow.start_run(run_name=\"Feature_Selection\", nested=True):\n",
    "#             orig_features, selected_indices, retention = analyze_feature_importance(\n",
    "#                 train_full, Config.TOP_K_FEATURES\n",
    "#             )\n",
    "#             mlflow.log_params({\n",
    "#                 \"step\": \"Feature_Selection\",\n",
    "#                 \"original_features\": orig_features,\n",
    "#                 \"selected_features_count\": Config.TOP_K_FEATURES\n",
    "#             })\n",
    "#             mlflow.log_metric(\"information_retained_pct\", retention)\n",
    "\n",
    "#         # Create datasets\n",
    "#         df_pre, df_in, _ = create_dual_datasets(df_gold, selected_indices)\n",
    "#         train_pre, test_pre = split_and_checkpoint(df_pre, \"Pre-Departure\")\n",
    "#         train_in, test_in = split_and_checkpoint(df_in, \"In-Flight\")\n",
    "#         results = {}\n",
    "        \n",
    "#         # Define signatures using TensorSpec (UC-compatible format)\n",
    "#         pre_dep_feature_count = train_pre.select(\"features\").first().features.size\n",
    "#         in_flight_feature_count = train_in.select(\"features\").first().features.size\n",
    "        \n",
    "#         pre_dep_input_schema = Schema([TensorSpec(type=np.dtype('float64'), shape=(-1,), name=\"features\")])\n",
    "#         in_flight_input_schema = Schema([TensorSpec(type=np.dtype('float64'), shape=(-1,), name=\"features\")])\n",
    "#         output_schema = Schema([TensorSpec(type=np.dtype('float64'), shape=(-1, 2), name=\"probability\")])\n",
    "        \n",
    "#         pre_dep_signature = ModelSignature(inputs=pre_dep_input_schema, outputs=output_schema)\n",
    "#         in_flight_signature = ModelSignature(inputs=in_flight_input_schema, outputs=output_schema)\n",
    "        \n",
    "#         print(f\"\\n‚úÖ Signatures defined. Features: Pre-Dep={pre_dep_feature_count}, In-Flight={in_flight_feature_count}\")\n",
    "\n",
    "#         # Get sample inputs\n",
    "#         sample_pre = train_pre.limit(1).select(\"features\").toPandas()\n",
    "#         sample_in = train_in.limit(1).select(\"features\").toPandas()\n",
    "        \n",
    "#         # === TRAIN & LOG MODELS ===\n",
    "        \n",
    "#         # 1. Pre-Departure Random Forest\n",
    "#         with mlflow.start_run(run_name=\"RF_Pre_Departure\", nested=True):\n",
    "#             m, metrics, p, cv = train_model_optimized(train_pre, test_pre, \"RandomForest\", \"Pre-Departure\")\n",
    "#             mlflow.log_params(p)\n",
    "#             mlflow.log_metric(\"cv_score\", cv)\n",
    "#             mlflow.log_metric(\"auc_roc\", metrics[\"auc_roc\"])\n",
    "            \n",
    "            \n",
    "#             mlflow.spark.log_model(\n",
    "#                 spark_model=m,\n",
    "#                 artifact_path=\"model\",\n",
    "#                 signature=pre_dep_signature\n",
    "#             )\n",
    "#             results[\"RF_Pre\"] = metrics\n",
    "#             print(f\"‚úÖ RF Pre-Departure logged to workspace registry.\")\n",
    "            \n",
    "#         # 2. Pre-Departure GBT\n",
    "#         with mlflow.start_run(run_name=\"GBT_Pre_Departure\", nested=True):\n",
    "#             m, metrics, p, cv = train_model_optimized(train_pre, test_pre, \"GBT\", \"Pre-Departure\")\n",
    "#             mlflow.log_params(p)\n",
    "#             mlflow.log_metric(\"cv_score\", cv)\n",
    "#             mlflow.log_metric(\"auc_roc\", metrics[\"auc_roc\"])\n",
    "            \n",
    "#             mlflow.spark.log_model(\n",
    "#                 spark_model=m,\n",
    "#                 artifact_path=\"model\",\n",
    "#                 signature=pre_dep_signature\n",
    "#             )\n",
    "#             results[\"GBT_Pre\"] = metrics\n",
    "#             print(f\"‚úÖ GBT Pre-Departure logged to workspace registry.\")\n",
    "            \n",
    "#         # 3. In-Flight Random Forest\n",
    "#         with mlflow.start_run(run_name=\"RF_In_Flight\", nested=True):\n",
    "#             m, metrics, p, cv = train_model_optimized(train_in, test_in, \"RandomForest\", \"In-Flight\")\n",
    "#             mlflow.log_params(p)\n",
    "#             mlflow.log_metric(\"cv_score\", cv)\n",
    "#             mlflow.log_metric(\"auc_roc\", metrics[\"auc_roc\"])\n",
    "            \n",
    "#             mlflow.spark.log_model(\n",
    "#                 spark_model=m,\n",
    "#                 artifact_path=\"model\",\n",
    "#                 signature=in_flight_signature\n",
    "#             )\n",
    "#             results[\"RF_In\"] = metrics\n",
    "#             print(f\"‚úÖ RF In-Flight logged to workspace registry.\")\n",
    "            \n",
    "#         # 4. In-Flight GBT\n",
    "#         with mlflow.start_run(run_name=\"GBT_In_Flight\", nested=True):\n",
    "#             m, metrics, p, cv = train_model_optimized(train_in, test_in, \"GBT\", \"In-Flight\")\n",
    "#             mlflow.log_params(p)\n",
    "#             mlflow.log_metric(\"cv_score\", cv)\n",
    "#             mlflow.log_metric(\"auc_roc\", metrics[\"auc_roc\"])\n",
    "            \n",
    "#             mlflow.spark.log_model(\n",
    "#                 spark_model=m,\n",
    "#                 artifact_path=\"model\",\n",
    "#                 signature=in_flight_signature\n",
    "#             )\n",
    "#             results[\"GBT_In\"] = metrics\n",
    "#             print(f\"‚úÖ GBT In-Flight logged to workspace registry.\")\n",
    "            \n",
    "#         # === FINAL SUMMARY ===\n",
    "#         print(\"\\n\" + \"=\" * 80)\n",
    "#         print(\"FINAL RESULTS SUMMARY\")\n",
    "#         print(\"=\" * 80)\n",
    "#         for k, v in results.items():\n",
    "#             print(f\"Model: {k:<15} | Test AUC: {v['auc_roc']:.4f}\")\n",
    "        \n",
    "#         print(\"\\n\" + \"=\" * 80)\n",
    "#         print(\"NEXT STEPS: Manual Unity Catalog Registration\")\n",
    "#         print(\"=\" * 80)\n",
    "#         print(\"Your models are now logged in the workspace registry with signatures.\")\n",
    "#         print(\"\\nTo register to Unity Catalog:\")\n",
    "#         print(\"1. Go to MLflow UI (Experiments page)\")\n",
    "#         print(\"2. Find your runs and click on each model\")\n",
    "#         print(\"3. Click 'Register Model' button\")\n",
    "#         print(f\"4. Select Unity Catalog and use format: {Config.UC_CATALOG}.{Config.UC_SCHEMA}.model_name\")\n",
    "#         print(\"\\nAlternatively, use the MLflow Client API to register programmatically\")\n",
    "#         print(\"after models are logged (example code in next message).\")\n",
    "            \n",
    "#         return results, selected_indices\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # MAIN EXECUTION\n",
    "# # =============================================================================\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "    \n",
    "#     try:\n",
    "#         run_complete_experiments()\n",
    "#         print(\"\\n‚úÖ All experiments complete and MLflow runs closed successfully!\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå An error occurred during the pipeline execution: {e}\")\n",
    "#         raise e\n",
    "        \n",
    "#     finally:\n",
    "#         mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f417d51-c5e5-489c-a6c0-4ec52fe6b5f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The Notebook below is supposed to be a modified version of the one above that will register the models post creation which would allow the integration with the FlightMasters_Delay_prediction notebook that Uses the Aviation Stack API (I couldn't run it due to freetier limits being exceeded)\n",
    "\n",
    "Hopefully it works, but if it doesn't you have the code above as a refrence to build off of it and fix it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f76f3ca-98d6-4496-8cb1-87e22b956217",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "import os\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "from pyspark.ml.classification import GBTClassifier, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "# Signature imports\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types.schema import Schema, TensorSpec\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Optimized for Databricks Community Edition\"\"\"\n",
    "    \n",
    "    TOP_K_FEATURES = 40 \n",
    "    CV_FOLDS = 2\n",
    "    BAYES_MAX_EVALS = 4\n",
    "    TEST_RATIO = 0.2\n",
    "    RANDOM_SEED = 42\n",
    "    \n",
    "    GOLD_TABLE = \"default.gold_ml_features_experimental\"\n",
    "    EXPERIMENT_NAME = \"/Shared/Flightmasters_Optimized_Experiments\"\n",
    "    \n",
    "    # CRITICAL CHANGE: Use workspace registry for initial logging\n",
    "    MLFLOW_TRACKING_URI = \"databricks\"\n",
    "    MLFLOW_REGISTRY_URI = \"databricks\"  # Changed from databricks-uc\n",
    "    \n",
    "    # Model names (without UC prefix for workspace registry)\n",
    "    MODEL_RF_PRE = \"model_rf_pre\"\n",
    "    MODEL_GBT_PRE = \"model_gbt_pre\"\n",
    "    MODEL_RF_IN = \"model_rf_in\"\n",
    "    MODEL_GBT_IN = \"model_gbt_in\"\n",
    "    \n",
    "    # UC settings (for manual registration later)\n",
    "    UC_CATALOG = \"workspace\"\n",
    "    UC_SCHEMA = \"default\"\n",
    "    UC_VOLUME_NAME = \"mlflow_shared_tmp\"\n",
    "    \n",
    "    USE_CHECKPOINTING = True\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# HELPERS\n",
    "# =============================================================================\n",
    "\n",
    "def setup_uc_volume(spark):\n",
    "    \"\"\"Setup Unity Catalog Volume for artifacts\"\"\"\n",
    "    catalog = Config.UC_CATALOG\n",
    "    schema = Config.UC_SCHEMA\n",
    "    volume_name = Config.UC_VOLUME_NAME\n",
    "    \n",
    "    volume_path = f\"{catalog}.{schema}.{volume_name}\"\n",
    "    env_path = f\"/Volumes/{catalog}/{schema}/{volume_name}\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"UNITY CATALOG VOLUME SETUP\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Volume Target: {volume_path}\")\n",
    "    \n",
    "    try:\n",
    "        volume_exists = spark.sql(f\"SHOW VOLUMES IN {catalog}.{schema}\").filter(\n",
    "            col(\"volume_name\") == volume_name\n",
    "        ).count() > 0\n",
    "        \n",
    "        if not volume_exists:\n",
    "            spark.sql(f\"CREATE VOLUME {volume_path}\")\n",
    "            print(\"‚úÖ Volume created successfully.\")\n",
    "        else:\n",
    "            print(\"‚úÖ Volume already exists.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  WARNING: Could not check/create volume: {e}\")\n",
    "        pass\n",
    "        \n",
    "    os.environ['MLFLOW_DFS_TMP'] = env_path\n",
    "    os.environ['SPARKML_TEMP_DFS_PATH'] = env_path\n",
    "    \n",
    "    print(f\"‚úÖ Environment paths set to: {env_path}\")\n",
    "    return volume_path\n",
    "\n",
    "\n",
    "def create_safe_vector_slicer(indices_to_keep):\n",
    "    \"\"\"Create UDF to slice vectors\"\"\"\n",
    "    indices_list = list(indices_to_keep)\n",
    "    \n",
    "    @udf(returnType=VectorUDT())\n",
    "    def safe_slicer(features):\n",
    "        if features is None: \n",
    "            return None\n",
    "        max_idx = features.size - 1\n",
    "        selected_values = [float(features[i]) for i in indices_list if i <= max_idx]\n",
    "        return Vectors.dense(selected_values)\n",
    "    \n",
    "    return safe_slicer\n",
    "\n",
    "\n",
    "def checkpoint_if_enabled(df, eager=True):\n",
    "    \"\"\"Checkpoint data to prevent OOM\"\"\"\n",
    "    if Config.USE_CHECKPOINTING:\n",
    "        return df.localCheckpoint(eager=eager)\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1. FEATURE IMPORTANCE\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_feature_importance(train_data, top_k):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PHASE 1: FEATURE IMPORTANCE ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    sample = train_data.select(\"features\").first()\n",
    "    if not sample:\n",
    "        raise ValueError(\"Training data is empty or features column is missing.\")\n",
    "        \n",
    "    total_features = sample.features.size\n",
    "    print(f\"\\nüìä Original features: {total_features}\")\n",
    "    \n",
    "    print(\"\\nüå≤ Training Random Forest for feature ranking...\")\n",
    "    rf = RandomForestClassifier(\n",
    "        featuresCol=\"features\", labelCol=\"label\",\n",
    "        numTrees=30, maxDepth=8, seed=Config.RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    rf_model = rf.fit(train_data)\n",
    "    importances = rf_model.featureImportances.toArray()\n",
    "    \n",
    "    top_k_indices = np.argsort(importances)[-top_k:][::-1]\n",
    "    top_k_scores = importances[top_k_indices]\n",
    "    \n",
    "    selected_importance = np.sum(top_k_scores)\n",
    "    total_importance = np.sum(importances)\n",
    "    retention = (selected_importance / total_importance) * 100\n",
    "    \n",
    "    print(f\"\\nüìà Feature Selection Summary:\")\n",
    "    print(f\"   Original: {total_features} -> Selected: {top_k}\")\n",
    "    print(f\"   Information retained: {retention:.1f}%\")\n",
    "    \n",
    "    return total_features, top_k_indices.tolist(), retention\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. CREATE DUAL DATASETS\n",
    "# =============================================================================\n",
    "\n",
    "def create_dual_datasets(df_gold, selected_indices, dep_delay_orig_index=11):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PHASE 2: CREATING DUAL DATASETS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    dep_delay_in_selected = dep_delay_orig_index in selected_indices\n",
    "    if dep_delay_in_selected: \n",
    "        print(f\"‚úÖ dep_delay found at original index {dep_delay_orig_index}\")\n",
    "    else: \n",
    "        print(f\"‚ö†Ô∏è  dep_delay (index {dep_delay_orig_index}) not in top-K\")\n",
    "    \n",
    "    print(f\"\\nüîß Applying feature selection...\")\n",
    "    slicer_udf = create_safe_vector_slicer(selected_indices)\n",
    "    df_selected = df_gold.withColumn(\"features\", slicer_udf(col(\"features\"))).select(\"features\", \"label\")\n",
    "    df_selected = df_selected.withColumn(\"label\", col(\"label\").cast(DoubleType()))\n",
    "    df_selected = checkpoint_if_enabled(df_selected)\n",
    "    \n",
    "    if dep_delay_in_selected:\n",
    "        print(f\"\\nüìã Creating Pre-Departure (removing dep_delay)...\")\n",
    "        dep_delay_new_index = selected_indices.index(dep_delay_orig_index)\n",
    "        pre_dep_indices = [i for i in range(len(selected_indices)) if i != dep_delay_new_index]\n",
    "        pre_dep_slicer = create_safe_vector_slicer(pre_dep_indices)\n",
    "        df_pre_dep = df_selected.withColumn(\"features\", pre_dep_slicer(col(\"features\"))).select(\"features\", \"label\")\n",
    "    else:\n",
    "        df_pre_dep = df_selected\n",
    "    \n",
    "    df_in_flight = df_selected\n",
    "    \n",
    "    df_pre_dep = checkpoint_if_enabled(df_pre_dep, eager=False)\n",
    "    df_in_flight = checkpoint_if_enabled(df_in_flight, eager=False)\n",
    "    \n",
    "    return df_pre_dep, df_in_flight, dep_delay_in_selected\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. TRAIN/TEST SPLIT\n",
    "# =============================================================================\n",
    "\n",
    "def split_and_checkpoint(df, name):\n",
    "    print(f\"\\nüîÄ Splitting {name} dataset...\")\n",
    "    train, test = df.randomSplit([1.0 - Config.TEST_RATIO, Config.TEST_RATIO], seed=Config.RANDOM_SEED)\n",
    "    print(f\"   Train count: {train.count():,}\")\n",
    "    print(f\"   Test count: {test.count():,}\")\n",
    "    \n",
    "    train = checkpoint_if_enabled(train, eager=True)\n",
    "    test = checkpoint_if_enabled(test, eager=True)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4. MODEL TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "def train_model_optimized(train_data, test_data, model_name, model_type):\n",
    "    \"\"\"Bayesian optimization with reduced CV folds\"\"\"\n",
    "    \n",
    "    print(f\"\\n\\tüéØ Training {model_name} ({model_type})\")\n",
    "    print(f\"\\t   {Config.CV_FOLDS}-fold CV + {Config.BAYES_MAX_EVALS} Bayesian evals\")\n",
    "    \n",
    "    if model_name == \"RandomForest\":\n",
    "        space = {\n",
    "            'numTrees': hp.choice('numTrees', [50, 100]),\n",
    "            'maxDepth': hp.choice('maxDepth', [10, 15]),\n",
    "            'minInstancesPerNode': hp.choice('minInstancesPerNode', [25, 50])\n",
    "        }\n",
    "        ModelClass = RandomForestClassifier\n",
    "        param_map = {'numTrees': [50, 100], 'maxDepth': [10, 15], 'minInstancesPerNode': [25, 50]}\n",
    "    else:\n",
    "        space = {\n",
    "            'maxIter': hp.choice('maxIter', [50, 100]),\n",
    "            'maxDepth': hp.choice('maxDepth', [4, 6]),\n",
    "            'stepSize': hp.uniform('stepSize', 0.05, 0.15)\n",
    "        }\n",
    "        ModelClass = GBTClassifier\n",
    "        param_map = {'maxIter': [50, 100], 'maxDepth': [4, 6]}\n",
    "    \n",
    "    def objective(params):\n",
    "        if model_name == \"RandomForest\":\n",
    "            model = ModelClass(\n",
    "                featuresCol=\"features\", labelCol=\"label\",\n",
    "                numTrees=int(params['numTrees']),\n",
    "                maxDepth=int(params['maxDepth']),\n",
    "                minInstancesPerNode=int(params['minInstancesPerNode']),\n",
    "                seed=Config.RANDOM_SEED\n",
    "            )\n",
    "        else:\n",
    "            model = ModelClass(\n",
    "                featuresCol=\"features\", labelCol=\"label\",\n",
    "                maxIter=int(params['maxIter']),\n",
    "                maxDepth=int(params['maxDepth']),\n",
    "                stepSize=float(params['stepSize']),\n",
    "                seed=Config.RANDOM_SEED\n",
    "            )\n",
    "        \n",
    "        cv = CrossValidator(\n",
    "            estimator=model,\n",
    "            estimatorParamMaps=[{}],\n",
    "            evaluator=BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\"),\n",
    "            numFolds=Config.CV_FOLDS,\n",
    "            seed=Config.RANDOM_SEED,\n",
    "            parallelism=1\n",
    "        )\n",
    "        \n",
    "        cv_model = cv.fit(train_data)\n",
    "        avg_auc = cv_model.avgMetrics[0]\n",
    "        return {'loss': -avg_auc, 'status': STATUS_OK}\n",
    "    \n",
    "    print(f\"\\t   Optimizing over {Config.BAYES_MAX_EVALS} iterations...\")\n",
    "    trials = Trials()\n",
    "    \n",
    "    best = fmin(\n",
    "        fn=objective, space=space, algo=tpe.suggest,\n",
    "        max_evals=Config.BAYES_MAX_EVALS, trials=trials,\n",
    "        rstate=np.random.default_rng(Config.RANDOM_SEED),\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    best_params_actual = {}\n",
    "    for k, v in best.items():\n",
    "        if k in param_map:\n",
    "            best_params_actual[k] = param_map[k][v]\n",
    "        else:\n",
    "            best_params_actual[k] = v\n",
    "    \n",
    "    print(f\"\\t   Best params: {best_params_actual}\")\n",
    "    \n",
    "    final_model = ModelClass(\n",
    "        featuresCol=\"features\", labelCol=\"label\",\n",
    "        **best_params_actual, seed=Config.RANDOM_SEED\n",
    "    ).fit(train_data)\n",
    "    \n",
    "    predictions = final_model.transform(test_data)\n",
    "    \n",
    "    metrics = {\n",
    "        \"auc_roc\": BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\").evaluate(predictions),\n",
    "        \"accuracy\": MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\").evaluate(predictions),\n",
    "        \"f1_score\": MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"f1\").evaluate(predictions)\n",
    "    }\n",
    "    \n",
    "    cv_score = -min([t['result']['loss'] for t in trials.trials])\n",
    "    print(f\"\\t   Best CV Score: {cv_score:.4f}\")\n",
    "    print(f\"\\t   Test AUC-ROC: {metrics['auc_roc']:.4f}\")\n",
    "    \n",
    "    return final_model, metrics, best_params_actual, cv_score\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 5. MAIN PIPELINE (WORKSPACE REGISTRY LOGGING)\n",
    "# =============================================================================\n",
    "\n",
    "def run_complete_experiments():\n",
    "    \"\"\"Main execution pipeline - logs to workspace registry\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"FLIGHTMASTERS OPTIMIZED EXPERIMENTS\")\n",
    "    print(\"Strategy: Log to Workspace Registry (bypasses MLeap issue)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Setup MLflow for workspace registry\n",
    "    mlflow.set_tracking_uri(Config.MLFLOW_TRACKING_URI)\n",
    "    mlflow.set_registry_uri(Config.MLFLOW_REGISTRY_URI)\n",
    "    mlflow.set_experiment(Config.EXPERIMENT_NAME)\n",
    "    \n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    setup_uc_volume(spark)\n",
    "    \n",
    "    mlflow.end_run()\n",
    "    \n",
    "    # === MASTER PARENT RUN ===\n",
    "    with mlflow.start_run(run_name=\"Flight_Experiment_Master_Workspace\"):\n",
    "        \n",
    "        # Load data\n",
    "        print(f\"\\nüì• Loading Gold table...\")\n",
    "        df_gold = spark.table(Config.GOLD_TABLE)\n",
    "        df_gold = df_gold.withColumn(\"label\", col(\"label\").cast(DoubleType())).filter(\n",
    "            (col(\"label\") == 0.0) | (col(\"label\") == 1.0)\n",
    "        )\n",
    "        df_gold = checkpoint_if_enabled(df_gold, eager=True)\n",
    "        \n",
    "        train_full, test_full = df_gold.randomSplit([0.8, 0.2], seed=Config.RANDOM_SEED)\n",
    "        train_full = checkpoint_if_enabled(train_full, eager=False)\n",
    "        test_full = checkpoint_if_enabled(test_full, eager=False)\n",
    "\n",
    "        # Feature selection\n",
    "        with mlflow.start_run(run_name=\"Feature_Selection\", nested=True):\n",
    "            orig_features, selected_indices, retention = analyze_feature_importance(\n",
    "                train_full, Config.TOP_K_FEATURES\n",
    "            )\n",
    "            mlflow.log_params({\n",
    "                \"step\": \"Feature_Selection\",\n",
    "                \"original_features\": orig_features,\n",
    "                \"selected_features_count\": Config.TOP_K_FEATURES\n",
    "            })\n",
    "            mlflow.log_metric(\"information_retained_pct\", retention)\n",
    "\n",
    "        # Create datasets\n",
    "        df_pre, df_in, _ = create_dual_datasets(df_gold, selected_indices)\n",
    "        train_pre, test_pre = split_and_checkpoint(df_pre, \"Pre-Departure\")\n",
    "        train_in, test_in = split_and_checkpoint(df_in, \"In-Flight\")\n",
    "        results = {}\n",
    "        \n",
    "        # Define signatures using TensorSpec (UC-compatible format)\n",
    "        pre_dep_feature_count = train_pre.select(\"features\").first().features.size\n",
    "        in_flight_feature_count = train_in.select(\"features\").first().features.size\n",
    "        \n",
    "        pre_dep_input_schema = Schema([TensorSpec(type=np.dtype('float64'), shape=(-1,), name=\"features\")])\n",
    "        in_flight_input_schema = Schema([TensorSpec(type=np.dtype('float64'), shape=(-1,), name=\"features\")])\n",
    "        output_schema = Schema([TensorSpec(type=np.dtype('float64'), shape=(-1, 2), name=\"probability\")])\n",
    "        \n",
    "        pre_dep_signature = ModelSignature(inputs=pre_dep_input_schema, outputs=output_schema)\n",
    "        in_flight_signature = ModelSignature(inputs=in_flight_input_schema, outputs=output_schema)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Signatures defined. Features: Pre-Dep={pre_dep_feature_count}, In-Flight={in_flight_feature_count}\")\n",
    "\n",
    "        # Get sample inputs - convert DenseVector to list/array for JSON serialization\n",
    "        sample_pre_row = train_pre.limit(1).select(\"features\").collect()[0]\n",
    "        sample_pre = pd.DataFrame([sample_pre_row.features.toArray()])\n",
    "        \n",
    "        sample_in_row = train_in.limit(1).select(\"features\").collect()[0]\n",
    "        sample_in = pd.DataFrame([sample_in_row.features.toArray()])\n",
    "        \n",
    "        # === TRAIN & LOG MODELS ===\n",
    "        \n",
    "        # 1. Pre-Departure Random Forest\n",
    "        with mlflow.start_run(run_name=\"RF_Pre_Departure\", nested=True):\n",
    "            m, metrics, p, cv = train_model_optimized(train_pre, test_pre, \"RandomForest\", \"Pre-Departure\")\n",
    "            mlflow.log_params(p)\n",
    "            mlflow.log_metric(\"cv_score\", cv)\n",
    "            mlflow.log_metric(\"auc_roc\", metrics[\"auc_roc\"])\n",
    "            \n",
    "            # CRITICAL FIX: Log WITHOUT registered_model_name parameter\n",
    "            mlflow.spark.log_model(\n",
    "                spark_model=m,\n",
    "                artifact_path=\"model\",\n",
    "                signature=pre_dep_signature,\n",
    "                input_example=sample_pre\n",
    "            )\n",
    "            results[\"RF_Pre\"] = metrics\n",
    "            print(f\"‚úÖ RF Pre-Departure logged to workspace registry.\")\n",
    "            \n",
    "        # 2. Pre-Departure GBT\n",
    "        with mlflow.start_run(run_name=\"GBT_Pre_Departure\", nested=True):\n",
    "            m, metrics, p, cv = train_model_optimized(train_pre, test_pre, \"GBT\", \"Pre-Departure\")\n",
    "            mlflow.log_params(p)\n",
    "            mlflow.log_metric(\"cv_score\", cv)\n",
    "            mlflow.log_metric(\"auc_roc\", metrics[\"auc_roc\"])\n",
    "            \n",
    "            mlflow.spark.log_model(\n",
    "                spark_model=m,\n",
    "                artifact_path=\"model\",\n",
    "                signature=pre_dep_signature,\n",
    "                input_example=sample_pre\n",
    "            )\n",
    "            results[\"GBT_Pre\"] = metrics\n",
    "            print(f\"‚úÖ GBT Pre-Departure logged to workspace registry.\")\n",
    "            \n",
    "        # 3. In-Flight Random Forest\n",
    "        with mlflow.start_run(run_name=\"RF_In_Flight\", nested=True):\n",
    "            m, metrics, p, cv = train_model_optimized(train_in, test_in, \"RandomForest\", \"In-Flight\")\n",
    "            mlflow.log_params(p)\n",
    "            mlflow.log_metric(\"cv_score\", cv)\n",
    "            mlflow.log_metric(\"auc_roc\", metrics[\"auc_roc\"])\n",
    "            \n",
    "            mlflow.spark.log_model(\n",
    "                spark_model=m,\n",
    "                artifact_path=\"model\",\n",
    "                signature=in_flight_signature,\n",
    "                input_example=sample_in\n",
    "            )\n",
    "            results[\"RF_In\"] = metrics\n",
    "            print(f\"‚úÖ RF In-Flight logged to workspace registry.\")\n",
    "            \n",
    "        # 4. In-Flight GBT\n",
    "        with mlflow.start_run(run_name=\"GBT_In_Flight\", nested=True):\n",
    "            m, metrics, p, cv = train_model_optimized(train_in, test_in, \"GBT\", \"In-Flight\")\n",
    "            mlflow.log_params(p)\n",
    "            mlflow.log_metric(\"cv_score\", cv)\n",
    "            mlflow.log_metric(\"auc_roc\", metrics[\"auc_roc\"])\n",
    "            \n",
    "            mlflow.spark.log_model(\n",
    "                spark_model=m,\n",
    "                artifact_path=\"model\",\n",
    "                signature=in_flight_signature,\n",
    "                input_example=sample_in\n",
    "            )\n",
    "            results[\"GBT_In\"] = metrics\n",
    "            print(f\"‚úÖ GBT In-Flight logged to workspace registry.\")\n",
    "            \n",
    "        # === FINAL SUMMARY ===\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"FINAL RESULTS SUMMARY\")\n",
    "        print(\"=\" * 80)\n",
    "        for k, v in results.items():\n",
    "            print(f\"Model: {k:<15} | Test AUC: {v['auc_roc']:.4f}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"NEXT STEPS: Manual Unity Catalog Registration\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your models are now logged in the workspace registry with signatures.\")\n",
    "        print(\"\\nTo register to Unity Catalog:\")\n",
    "        print(\"1. Go to MLflow UI (Experiments page)\")\n",
    "        print(\"2. Find your runs and click on each model\")\n",
    "        print(\"3. Click 'Register Model' button\")\n",
    "        print(f\"4. Select Unity Catalog and use format: {Config.UC_CATALOG}.{Config.UC_SCHEMA}.model_name\")\n",
    "        print(\"\\nAlternatively, use the MLflow Client API to register programmatically\")\n",
    "        print(\"after models are logged (example code in next message).\")\n",
    "            \n",
    "        return results, selected_indices\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    try:\n",
    "        run_complete_experiments()\n",
    "        print(\"\\n‚úÖ All experiments complete and MLflow runs closed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå An error occurred during the pipeline execution: {e}\")\n",
    "        raise e\n",
    "        \n",
    "    finally:\n",
    "        mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a94f5cc-f6d8-4153-ab68-d0faf5cb8363",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Useful Information:\n",
    "\n",
    "You can find the ML models and information regarding feature selection in the Experiments tab on the left hand side.\n",
    "\n",
    "You can see the indexs of what features were used for training in the Artifacts tab of Feature Engineering within experiments (indexes refrence the Gold table).\n",
    "\n",
    "Once done head on to Flightmasters_Delay_Prediction notebook where you will need to fix and run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cdb58c3-89c1-4f87-9d05-0bc960c94e16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Advanced_Experiment_Tracking_and_Models",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
