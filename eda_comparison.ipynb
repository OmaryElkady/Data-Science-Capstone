{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA Comparison: Original vs Processed Flight Data\n",
    "\n",
    "This notebook provides a comprehensive comparison between the original `flights_sample_3m.csv` dataset and the processed `flights_processed.csv` dataset.\n",
    "\n",
    "## Objectives:\n",
    "1. Load and analyze both datasets\n",
    "2. Compare data quality improvements\n",
    "3. Analyze feature engineering changes\n",
    "4. Visualize distributions and relationships\n",
    "\n",
    "5. Generate summary insights\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Configure display settings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.display.max_columns = 50\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.float_format = \"{:,.3f}\".format\n",
    "\n",
    "# Set style\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(\"eda_comparison_outputs\", exist_ok=True)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Numpy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "original_path = Path(\"./dataset/flights_sample_3m.csv\")\n",
    "processed_path = Path(\"./dataset/flights_processed.csv\")\n",
    "\n",
    "print(\"Dataset paths:\")\n",
    "print(f\"Original: {original_path.resolve()}\")\n",
    "print(f\"Processed: {processed_path.resolve()}\")\n",
    "\n",
    "# Check if files exist\n",
    "print(f\"\\nOriginal exists: {original_path.exists()}\")\n",
    "print(f\"Processed exists: {processed_path.exists()}\")\n",
    "\n",
    "if original_path.exists():\n",
    "    original_size = original_path.stat().st_size / (1024 * 1024)\n",
    "    print(f\"Original file size: {original_size:.2f} MB\")\n",
    "\n",
    "if processed_path.exists():\n",
    "    processed_size = processed_path.stat().st_size / (1024 * 1024)\n",
    "    print(f\"Processed file size: {processed_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets with memory optimization\n",
    "print(\"Loading original dataset...\")\n",
    "try:\n",
    "    # Load original dataset\n",
    "    df_original = pd.read_csv(original_path)\n",
    "    print(f\"Original dataset loaded: {df_original.shape}\")\n",
    "except MemoryError:\n",
    "    print(\"Memory error loading original dataset. Using chunked loading...\")\n",
    "    chunks = []\n",
    "    for chunk in pd.read_csv(original_path, chunksize=100_000):\n",
    "        chunks.append(chunk)\n",
    "    df_original = pd.concat(chunks, ignore_index=True)\n",
    "    print(f\"Original dataset loaded via chunks: {df_original.shape}\")\n",
    "\n",
    "print(\"\\nLoading processed dataset...\")\n",
    "df_processed = pd.read_csv(processed_path)\n",
    "print(f\"Processed dataset loaded: {df_processed.shape}\")\n",
    "\n",
    "# Basic info about both datasets\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Original:  {df_original.shape[0]:,} rows × {df_original.shape[1]} columns\")\n",
    "print(f\"Processed: {df_processed.shape[0]:,} rows × {df_processed.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Structure Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare column structures\n",
    "original_cols = set(df_original.columns)\n",
    "processed_cols = set(df_processed.columns)\n",
    "\n",
    "print(\"COLUMN COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Original columns: {len(original_cols)}\")\n",
    "print(f\"Processed columns: {len(processed_cols)}\")\n",
    "\n",
    "# Find differences\n",
    "removed_cols = original_cols - processed_cols\n",
    "added_cols = processed_cols - original_cols\n",
    "common_cols = original_cols & processed_cols\n",
    "\n",
    "print(f\"\\nCommon columns: {len(common_cols)}\")\n",
    "print(f\"Removed columns: {len(removed_cols)}\")\n",
    "print(f\"Added columns: {len(added_cols)}\")\n",
    "\n",
    "if removed_cols:\n",
    "    print(f\"\\nRemoved columns: {sorted(list(removed_cols))}\")\n",
    "\n",
    "if added_cols:\n",
    "    print(f\"\\nAdded columns: {sorted(list(added_cols))}\")\n",
    "\n",
    "# Show data types comparison\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"DATA TYPES COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nOriginal dataset dtypes:\")\n",
    "print(df_original.dtypes.value_counts())\n",
    "\n",
    "print(\"\\nProcessed dataset dtypes:\")\n",
    "print(df_processed.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Missing Values Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values comparison\n",
    "print(\"MISSING VALUES COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Calculate missing values for both datasets\n",
    "missing_original = df_original.isnull().sum().sort_values(ascending=False)\n",
    "missing_processed = df_processed.isnull().sum().sort_values(ascending=False)\n",
    "\n",
    "# Convert to percentages\n",
    "missing_orig_pct = (missing_original / len(df_original) * 100).round(2)\n",
    "missing_proc_pct = (missing_processed / len(df_processed) * 100).round(2)\n",
    "\n",
    "# Create comparison dataframe for common columns\n",
    "missing_comparison = []\n",
    "for col in common_cols:\n",
    "    if col in df_original.columns and col in df_processed.columns:\n",
    "        missing_comparison.append(\n",
    "            {\n",
    "                \"Column\": col,\n",
    "                \"Original_Missing\": missing_original[col],\n",
    "                \"Original_Pct\": missing_orig_pct[col],\n",
    "                \"Processed_Missing\": missing_processed[col],\n",
    "                \"Processed_Pct\": missing_proc_pct[col],\n",
    "                \"Improvement\": missing_orig_pct[col] - missing_proc_pct[col],\n",
    "            }\n",
    "        )\n",
    "\n",
    "missing_df = pd.DataFrame(missing_comparison)\n",
    "missing_df = missing_df.sort_values(\"Improvement\", ascending=False)\n",
    "\n",
    "print(\"Missing values comparison (top 20 columns):\")\n",
    "print(missing_df.head(20).to_string(index=False))\n",
    "\n",
    "# Overall missing values summary\n",
    "total_missing_orig = df_original.isnull().sum().sum()\n",
    "total_missing_proc = df_processed.isnull().sum().sum()\n",
    "\n",
    "print(f\"\\nOVERALL MISSING VALUES SUMMARY:\")\n",
    "print(f\"Original dataset:  {total_missing_orig:,} missing values\")\n",
    "print(f\"Processed dataset: {total_missing_proc:,} missing values\")\n",
    "print(f\"Reduction: {total_missing_orig - total_missing_proc:,} missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistical Summary Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary for numeric columns\n",
    "print(\"STATISTICAL SUMMARY COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get numeric columns from both datasets\n",
    "numeric_orig = df_original.select_dtypes(include=[np.number]).columns\n",
    "numeric_proc = df_processed.select_dtypes(include=[np.number]).columns\n",
    "common_numeric = list(set(numeric_orig) & set(numeric_proc))\n",
    "\n",
    "print(f\"Common numeric columns: {len(common_numeric)}\")\n",
    "print(f\"Original numeric columns: {len(numeric_orig)}\")\n",
    "print(f\"Processed numeric columns: {len(numeric_proc)}\")\n",
    "\n",
    "if common_numeric:\n",
    "    print(f\"\\nCommon numeric columns: {sorted(common_numeric)}\")\n",
    "\n",
    "# Statistical summaries\n",
    "print(\"\\nOriginal dataset - Numeric summary:\")\n",
    "orig_stats = df_original[common_numeric].describe()\n",
    "print(orig_stats)\n",
    "\n",
    "print(\"\\nProcessed dataset - Numeric summary:\")\n",
    "proc_stats = df_processed[common_numeric].describe()\n",
    "print(proc_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Distribution Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare key statistics for important columns\n",
    "key_columns = [\"DEP_DELAY\", \"ARR_DELAY\", \"DISTANCE\", \"AIR_TIME\"]\n",
    "available_key_cols = [col for col in key_columns if col in common_numeric]\n",
    "\n",
    "if available_key_cols:\n",
    "    print(\"KEY STATISTICS COMPARISON\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    stats_comparison = []\n",
    "\n",
    "    for col in available_key_cols:\n",
    "        orig_mean = df_original[col].mean()\n",
    "        proc_mean = df_processed[col].mean()\n",
    "        orig_std = df_original[col].std()\n",
    "        proc_std = df_processed[col].std()\n",
    "        orig_median = df_original[col].median()\n",
    "        proc_median = df_processed[col].median()\n",
    "\n",
    "        stats_comparison.append(\n",
    "            {\n",
    "                \"Column\": col,\n",
    "                \"Orig_Mean\": orig_mean,\n",
    "                \"Proc_Mean\": proc_mean,\n",
    "                \"Mean_Change\": proc_mean - orig_mean,\n",
    "                \"Orig_Std\": orig_std,\n",
    "                \"Proc_Std\": proc_std,\n",
    "                \"Std_Change\": proc_std - orig_std,\n",
    "                \"Orig_Median\": orig_median,\n",
    "                \"Proc_Median\": proc_median,\n",
    "                \"Median_Change\": proc_median - orig_median,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    stats_df = pd.DataFrame(stats_comparison)\n",
    "    print(stats_df.round(3).to_string(index=False))\n",
    "else:\n",
    "    print(\"No common key columns found for detailed comparison.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution comparison for key numeric columns\n",
    "if available_key_cols:\n",
    "    n_cols = len(available_key_cols)\n",
    "    fig, axes = plt.subplots(n_cols, 2, figsize=(15, 4 * n_cols))\n",
    "\n",
    "    if n_cols == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    for i, col in enumerate(available_key_cols):\n",
    "        # Original distribution\n",
    "        axes[i, 0].hist(df_original[col].dropna(), bins=50, alpha=0.7, color=\"red\", label=\"Original\")\n",
    "        axes[i, 0].set_title(f\"{col} - Original Distribution\")\n",
    "        axes[i, 0].set_xlabel(col)\n",
    "        axes[i, 0].set_ylabel(\"Frequency\")\n",
    "        axes[i, 0].legend()\n",
    "\n",
    "        # Processed distribution\n",
    "        axes[i, 1].hist(df_processed[col].dropna(), bins=50, alpha=0.7, color=\"green\", label=\"Processed\")\n",
    "        axes[i, 1].set_title(f\"{col} - Processed Distribution\")\n",
    "        axes[i, 1].set_xlabel(col)\n",
    "        axes[i, 1].set_ylabel(\"Frequency\")\n",
    "        axes[i, 1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"eda_comparison_outputs/distribution_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No key columns available for distribution comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Target Variable Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for target variables in processed dataset\n",
    "target_candidates = [\"any_delay\", \"delay_15min\", \"delay_30min\", \"ARR_DELAY\", \"DEP_DELAY\"]\n",
    "available_targets = [col for col in target_candidates if col in df_processed.columns]\n",
    "\n",
    "if available_targets:\n",
    "    print(\"TARGET VARIABLE ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    for target in available_targets:\n",
    "        print(f\"\\n{target}:\")\n",
    "\n",
    "        if df_processed[target].dtype in [\"int64\", \"float64\"]:\n",
    "            # Numeric target\n",
    "            print(f\"  - Type: Numeric\")\n",
    "            print(f\"  - Min: {df_processed[target].min():.3f}\")\n",
    "            print(f\"  - Max: {df_processed[target].max():.3f}\")\n",
    "            print(f\"  - Mean: {df_processed[target].mean():.3f}\")\n",
    "            print(f\"  - Std: {df_processed[target].std():.3f}\")\n",
    "            print(f\"  - Median: {df_processed[target].median():.3f}\")\n",
    "\n",
    "            # Distribution\n",
    "            plt.figure(figsize=(10, 4))\n",
    "\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.hist(df_processed[target].dropna(), bins=50, alpha=0.7)\n",
    "            plt.title(f\"{target} - Distribution\")\n",
    "            plt.xlabel(target)\n",
    "            plt.ylabel(\"Frequency\")\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.boxplot(df_processed[target].dropna())\n",
    "            plt.title(f\"{target} - Box Plot\")\n",
    "            plt.ylabel(target)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"eda_comparison_outputs/target_{target}_analysis.png\", dpi=300, bbox_inches=\"tight\")\n",
    "            plt.show()\n",
    "\n",
    "        else:\n",
    "            # Categorical target\n",
    "            print(f\"  - Type: Categorical\")\n",
    "            value_counts = df_processed[target].value_counts()\n",
    "            print(f\"  - Value distribution:\")\n",
    "            for val, count in value_counts.items():\n",
    "                pct = count / len(df_processed) * 100\n",
    "                print(f\"    {val}: {count:,} ({pct:.2f}%)\")\n",
    "\n",
    "            # Bar plot\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            value_counts.plot(kind=\"bar\")\n",
    "            plt.title(f\"{target} - Value Distribution\")\n",
    "            plt.xlabel(target)\n",
    "            plt.ylabel(\"Count\")\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"eda_comparison_outputs/target_{target}_analysis.png\", dpi=300, bbox_inches=\"tight\")\n",
    "            plt.show()\n",
    "else:\n",
    "    print(\"No target variables found in the processed dataset.\")\n",
    "    print(\"Available columns:\", list(df_processed.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Engineering Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze new features in processed dataset\n",
    "print(\"FEATURE ENGINEERING ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if added_cols:\n",
    "    print(f\"New features added ({len(added_cols)}):\")\n",
    "\n",
    "    for col in sorted(added_cols):\n",
    "        dtype = df_processed[col].dtype\n",
    "        n_unique = df_processed[col].nunique()\n",
    "        missing_count = df_processed[col].isnull().sum()\n",
    "\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  - Data type: {dtype}\")\n",
    "        print(f\"  - Unique values: {n_unique}\")\n",
    "        print(f\"  - Missing values: {missing_count}\")\n",
    "\n",
    "        if dtype in [\"object\", \"category\"]:\n",
    "            print(f\"  - Top values:\")\n",
    "            top_values = df_processed[col].value_counts().head(5)\n",
    "            for val, count in top_values.items():\n",
    "                print(f\"    {val}: {count}\")\n",
    "        else:\n",
    "            print(f\"  - Min: {df_processed[col].min():.3f}\")\n",
    "            print(f\"  - Max: {df_processed[col].max():.3f}\")\n",
    "            print(f\"  - Mean: {df_processed[col].mean():.3f}\")\n",
    "            print(f\"  - Std: {df_processed[col].std():.3f}\")\n",
    "else:\n",
    "    print(\"No new features were added to the processed dataset.\")\n",
    "\n",
    "# Analyze removed features\n",
    "if removed_cols:\n",
    "    print(f\"\\n\\nRemoved features ({len(removed_cols)}):\")\n",
    "\n",
    "    for col in sorted(removed_cols):\n",
    "        dtype = df_original[col].dtype\n",
    "        n_unique = df_original[col].nunique()\n",
    "        missing_count = df_original[col].isnull().sum()\n",
    "\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  - Data type: {dtype}\")\n",
    "        print(f\"  - Unique values: {n_unique}\")\n",
    "        print(f\"  - Missing values: {missing_count}\")\n",
    "        print(f\"  - Missing percentage: {missing_count/len(df_original)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary report\n",
    "print(\"COMPREHENSIVE EDA COMPARISON SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Dataset sizes\n",
    "print(f\"\\n1. DATASET SIZES:\")\n",
    "print(f\"   Original:  {df_original.shape[0]:,} rows × {df_original.shape[1]} columns\")\n",
    "print(f\"   Processed: {df_processed.shape[0]:,} rows × {df_processed.shape[1]} columns\")\n",
    "print(f\"   Row reduction: {df_original.shape[0] - df_processed.shape[0]:,} rows\")\n",
    "print(f\"   Column change: {df_processed.shape[1] - df_original.shape[1]} columns\")\n",
    "\n",
    "# Data quality improvements\n",
    "print(f\"\\n2. DATA QUALITY IMPROVEMENTS:\")\n",
    "print(f\"   Missing values reduction: {total_missing_orig - total_missing_proc:,}\")\n",
    "print(f\"   Missing percentage - Original: {total_missing_orig/(df_original.shape[0]*df_original.shape[1])*100:.2f}%\")\n",
    "print(f\"   Missing percentage - Processed: {total_missing_proc/(df_processed.shape[0]*df_processed.shape[1])*100:.2f}%\")\n",
    "\n",
    "# Feature changes\n",
    "print(f\"\\n3. FEATURE CHANGES:\")\n",
    "print(f\"   Removed columns: {len(removed_cols)}\")\n",
    "print(f\"   Added columns: {len(added_cols)}\")\n",
    "print(f\"   Common columns: {len(common_cols)}\")\n",
    "\n",
    "# Data types\n",
    "print(f\"\\n4. DATA TYPE CHANGES:\")\n",
    "type_changes = comparison_df[comparison_df[\"Type_Changed\"] == \"Yes\"] if \"comparison_df\" in locals() else pd.DataFrame()\n",
    "print(f\"   Columns with type changes: {len(type_changes)}\")\n",
    "\n",
    "# Target variables\n",
    "if available_targets:\n",
    "    print(f\"\\n5. TARGET VARIABLES:\")\n",
    "    for target in available_targets:\n",
    "        if target in [\"any_delay\", \"delay_15min\", \"delay_30min\"]:\n",
    "            positive_rate = df_processed[target].mean()\n",
    "            print(f\"   {target}: {positive_rate:.3f} positive rate\")\n",
    "\n",
    "# Memory usage\n",
    "orig_memory = df_original.memory_usage(deep=True).sum() / (1024**2)\n",
    "proc_memory = df_processed.memory_usage(deep=True).sum() / (1024**2)\n",
    "print(f\"\\n6. MEMORY USAGE:\")\n",
    "print(f\"   Original: {orig_memory:.2f} MB\")\n",
    "print(f\"   Processed: {proc_memory:.2f} MB\")\n",
    "print(f\"   Memory reduction: {orig_memory - proc_memory:.2f} MB ({(orig_memory - proc_memory)/orig_memory*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n\\nSummary report and visualizations saved to: eda_comparison_outputs/\")\n",
    "print(\"=\" * 60)\n",
    "print(\"EDA COMPARISON COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Original Dataset Deep Dive - Exploratory Charts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Dataset - Comprehensive Visual Analysis\n",
    "print(\"ORIGINAL DATASET EXPLORATORY ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Sample the original dataset for visualization (to avoid memory issues)\n",
    "sample_size = min(100000, len(df_original))\n",
    "df_orig_sample = df_original.sample(n=sample_size, random_state=42)\n",
    "print(f\"Using sample of {sample_size:,} rows for visualization\")\n",
    "\n",
    "# 1. Missing Values Pattern Analysis\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Missing values heatmap\n",
    "missing_data = df_orig_sample.isnull()\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.heatmap(missing_data.T, cbar=True, yticklabels=True, cmap=\"viridis\")\n",
    "plt.title(\"Missing Values Pattern (Sample)\")\n",
    "plt.xlabel(\"Row Index (Sample)\")\n",
    "plt.ylabel(\"Columns\")\n",
    "\n",
    "# Missing values by column\n",
    "missing_counts = df_orig_sample.isnull().sum().sort_values(ascending=True)\n",
    "plt.subplot(2, 2, 2)\n",
    "missing_counts.plot(kind=\"barh\", color=\"coral\")\n",
    "plt.title(\"Missing Values Count by Column\")\n",
    "plt.xlabel(\"Missing Count\")\n",
    "\n",
    "# Missing percentage by column\n",
    "missing_pct = (missing_counts / len(df_orig_sample) * 100).round(2)\n",
    "plt.subplot(2, 2, 3)\n",
    "missing_pct.plot(kind=\"barh\", color=\"lightcoral\")\n",
    "plt.title(\"Missing Values Percentage by Column\")\n",
    "plt.xlabel(\"Missing Percentage (%)\")\n",
    "\n",
    "# Data types distribution\n",
    "plt.subplot(2, 2, 4)\n",
    "dtype_counts = df_orig_sample.dtypes.value_counts()\n",
    "plt.pie(dtype_counts.values, labels=dtype_counts.index, autopct=\"%1.1f%%\")\n",
    "plt.title(\"Data Types Distribution\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"eda_comparison_outputs/original_dataset_overview.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Missing values analysis completed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Key Numeric Variables Analysis\n",
    "print(\"KEY NUMERIC VARIABLES ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Get key numeric columns\n",
    "key_numeric_cols = [\"DEP_DELAY\", \"ARR_DELAY\", \"DISTANCE\", \"AIR_TIME\", \"CRS_ELAPSED_TIME\", \"ELAPSED_TIME\"]\n",
    "available_numeric = [col for col in key_numeric_cols if col in df_orig_sample.columns]\n",
    "\n",
    "if available_numeric:\n",
    "    # Create comprehensive numeric analysis\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, col in enumerate(available_numeric[:6]):\n",
    "        # Remove outliers for better visualization\n",
    "        data = df_orig_sample[col].dropna()\n",
    "        Q1 = data.quantile(0.25)\n",
    "        Q3 = data.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        data_clean = data[(data >= lower_bound) & (data <= upper_bound)]\n",
    "\n",
    "        # Histogram\n",
    "        axes[i].hist(data_clean, bins=50, alpha=0.7, color=\"skyblue\", edgecolor=\"black\")\n",
    "        axes[i].set_title(f\"{col} Distribution (Outliers Removed)\")\n",
    "        axes[i].set_xlabel(col)\n",
    "        axes[i].set_ylabel(\"Frequency\")\n",
    "\n",
    "        # Add statistics text\n",
    "        stats_text = f\"Mean: {data.mean():.1f}\\nMedian: {data.median():.1f}\\nStd: {data.std():.1f}\"\n",
    "        axes[i].text(\n",
    "            0.7,\n",
    "            0.8,\n",
    "            stats_text,\n",
    "            transform=axes[i].transAxes,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8),\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"eda_comparison_outputs/original_numeric_analysis.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    # Box plots for outlier analysis\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, col in enumerate(available_numeric[:6]):\n",
    "        df_orig_sample.boxplot(column=col, ax=axes[i])\n",
    "        axes[i].set_title(f\"{col} - Box Plot (Outliers Shown)\")\n",
    "        axes[i].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"eda_comparison_outputs/original_boxplots.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"Numeric variables analysis completed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Categorical Variables Analysis\n",
    "print(\"CATEGORICAL VARIABLES ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Get key categorical columns\n",
    "key_cat_cols = [\"AIRLINE\", \"AIRLINE_CODE\", \"ORIGIN\", \"DEST\", \"ORIGIN_CITY\", \"DEST_CITY\"]\n",
    "available_cat = [col for col in key_cat_cols if col in df_orig_sample.columns]\n",
    "\n",
    "if available_cat:\n",
    "    # Create categorical analysis\n",
    "    n_cols = min(3, len(available_cat))\n",
    "    n_rows = (len(available_cat) + n_cols - 1) // n_cols\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 6 * n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = [axes] if n_cols == 1 else axes\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "\n",
    "    for i, col in enumerate(available_cat):\n",
    "        # Get top 15 categories\n",
    "        top_categories = df_orig_sample[col].value_counts().head(15)\n",
    "\n",
    "        # Create horizontal bar plot\n",
    "        axes[i].barh(range(len(top_categories)), top_categories.values, color=\"lightgreen\")\n",
    "        axes[i].set_yticks(range(len(top_categories)))\n",
    "        axes[i].set_yticklabels(top_categories.index)\n",
    "        axes[i].set_title(f\"Top 15 {col} Values\")\n",
    "        axes[i].set_xlabel(\"Count\")\n",
    "        axes[i].invert_yaxis()\n",
    "\n",
    "        # Add percentage annotations\n",
    "        total = df_orig_sample[col].value_counts().sum()\n",
    "        for j, v in enumerate(top_categories.values):\n",
    "            pct = (v / total) * 100\n",
    "            axes[i].text(v + max(top_categories.values) * 0.01, j, f\"{pct:.1f}%\", va=\"center\", fontsize=8)\n",
    "\n",
    "    # Hide empty subplots\n",
    "    for i in range(len(available_cat), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"eda_comparison_outputs/original_categorical_analysis.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"Categorical variables analysis completed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Route Analysis and Flight Patterns\n",
    "print(\"ROUTE ANALYSIS AND FLIGHT PATTERNS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Route analysis\n",
    "if \"ORIGIN\" in df_orig_sample.columns and \"DEST\" in df_orig_sample.columns:\n",
    "    # Top routes\n",
    "    routes = df_orig_sample.groupby([\"ORIGIN\", \"DEST\"]).size().reset_index(name=\"count\")\n",
    "    routes = routes.sort_values(\"count\", ascending=False).head(20)\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # Top routes\n",
    "    plt.subplot(2, 2, 1)\n",
    "    route_labels = [f\"{row['ORIGIN']}-{row['DEST']}\" for _, row in routes.iterrows()]\n",
    "    plt.barh(range(len(routes)), routes[\"count\"].values, color=\"orange\")\n",
    "    plt.yticks(range(len(routes)), route_labels)\n",
    "    plt.title(\"Top 20 Routes by Flight Count\")\n",
    "    plt.xlabel(\"Number of Flights\")\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "    # Origin airports\n",
    "    plt.subplot(2, 2, 2)\n",
    "    origins = df_orig_sample[\"ORIGIN\"].value_counts().head(15)\n",
    "    origins.plot(kind=\"bar\", color=\"lightblue\")\n",
    "    plt.title(\"Top 15 Origin Airports\")\n",
    "    plt.xlabel(\"Airport Code\")\n",
    "    plt.ylabel(\"Number of Flights\")\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Destination airports\n",
    "    plt.subplot(2, 2, 3)\n",
    "    dests = df_orig_sample[\"DEST\"].value_counts().head(15)\n",
    "    dests.plot(kind=\"bar\", color=\"lightcoral\")\n",
    "    plt.title(\"Top 15 Destination Airports\")\n",
    "    plt.xlabel(\"Airport Code\")\n",
    "    plt.ylabel(\"Number of Flights\")\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Route distance analysis (if distance available)\n",
    "    if \"DISTANCE\" in df_orig_sample.columns:\n",
    "        plt.subplot(2, 2, 4)\n",
    "        df_orig_sample[\"DISTANCE\"].hist(bins=50, color=\"lightgreen\", alpha=0.7)\n",
    "        plt.title(\"Flight Distance Distribution\")\n",
    "        plt.xlabel(\"Distance (miles)\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "\n",
    "        # Add statistics\n",
    "        mean_dist = df_orig_sample[\"DISTANCE\"].mean()\n",
    "        median_dist = df_orig_sample[\"DISTANCE\"].median()\n",
    "        plt.axvline(mean_dist, color=\"red\", linestyle=\"--\", label=f\"Mean: {mean_dist:.0f}\")\n",
    "        plt.axvline(median_dist, color=\"blue\", linestyle=\"--\", label=f\"Median: {median_dist:.0f}\")\n",
    "        plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"eda_comparison_outputs/original_route_analysis.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"Route analysis completed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Delay Analysis and Patterns\n",
    "print(\"DELAY ANALYSIS AND PATTERNS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Delay analysis\n",
    "delay_cols = [\"DEP_DELAY\", \"ARR_DELAY\"]\n",
    "available_delays = [col for col in delay_cols if col in df_orig_sample.columns]\n",
    "\n",
    "if available_delays:\n",
    "    plt.figure(figsize=(20, 12))\n",
    "\n",
    "    # 1. Delay distributions\n",
    "    for i, col in enumerate(available_delays):\n",
    "        plt.subplot(2, 3, i + 1)\n",
    "        data = df_orig_sample[col].dropna()\n",
    "\n",
    "        # Create histogram with different bins for negative, zero, and positive delays\n",
    "        neg_data = data[data < 0]\n",
    "        zero_data = data[data == 0]\n",
    "        pos_data = data[data > 0]\n",
    "\n",
    "        plt.hist(neg_data, bins=30, alpha=0.7, color=\"red\", label=f\"Early ({len(neg_data):,})\")\n",
    "        plt.hist(zero_data, bins=1, alpha=0.7, color=\"green\", label=f\"On Time ({len(zero_data):,})\")\n",
    "        plt.hist(pos_data, bins=50, alpha=0.7, color=\"orange\", label=f\"Delayed ({len(pos_data):,})\")\n",
    "\n",
    "        plt.title(f\"{col} Distribution\")\n",
    "        plt.xlabel(\"Delay (minutes)\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.legend()\n",
    "        plt.yscale(\"log\")  # Log scale due to large differences\n",
    "\n",
    "    # 2. Delay correlation\n",
    "    if len(available_delays) == 2:\n",
    "        plt.subplot(2, 3, 3)\n",
    "        delay_data = df_orig_sample[available_delays].dropna()\n",
    "        plt.scatter(delay_data[available_delays[0]], delay_data[available_delays[1]], alpha=0.3, s=1)\n",
    "        plt.xlabel(available_delays[0])\n",
    "        plt.ylabel(available_delays[1])\n",
    "        plt.title(f\"{available_delays[0]} vs {available_delays[1]}\")\n",
    "\n",
    "        # Add correlation coefficient\n",
    "        corr = delay_data[available_delays[0]].corr(delay_data[available_delays[1]])\n",
    "        plt.text(\n",
    "            0.05,\n",
    "            0.95,\n",
    "            f\"Correlation: {corr:.3f}\",\n",
    "            transform=plt.gca().transAxes,\n",
    "            bbox=dict(boxstyle=\"round\", facecolor=\"white\"),\n",
    "        )\n",
    "\n",
    "    # 3. Delay by airline (if available)\n",
    "    if \"AIRLINE\" in df_orig_sample.columns and available_delays:\n",
    "        plt.subplot(2, 3, 4)\n",
    "        delay_by_airline = df_orig_sample.groupby(\"AIRLINE\")[available_delays[0]].agg([\"mean\", \"count\"]).reset_index()\n",
    "        delay_by_airline = delay_by_airline[delay_by_airline[\"count\"] >= 100]  # Only airlines with 100+ flights\n",
    "        delay_by_airline = delay_by_airline.sort_values(\"mean\", ascending=False).head(15)\n",
    "\n",
    "        plt.barh(range(len(delay_by_airline)), delay_by_airline[\"mean\"].values, color=\"purple\")\n",
    "        plt.yticks(range(len(delay_by_airline)), delay_by_airline[\"AIRLINE\"])\n",
    "        plt.title(f\"Average {available_delays[0]} by Airline\")\n",
    "        plt.xlabel(\"Average Delay (minutes)\")\n",
    "        plt.gca().invert_yaxis()\n",
    "\n",
    "    # 4. Delay by route distance\n",
    "    if \"DISTANCE\" in df_orig_sample.columns and available_delays:\n",
    "        plt.subplot(2, 3, 5)\n",
    "        df_orig_sample[\"distance_bin\"] = pd.cut(\n",
    "            df_orig_sample[\"DISTANCE\"],\n",
    "            bins=[0, 500, 1000, 2000, 5000],\n",
    "            labels=[\"Short (<500)\", \"Medium (500-1000)\", \"Long (1000-2000)\", \"Very Long (>2000)\"],\n",
    "        )\n",
    "\n",
    "        delay_by_distance = df_orig_sample.groupby(\"distance_bin\")[available_delays[0]].mean()\n",
    "        delay_by_distance.plot(kind=\"bar\", color=\"teal\")\n",
    "        plt.title(f\"Average {available_delays[0]} by Distance\")\n",
    "        plt.xlabel(\"Distance Category\")\n",
    "        plt.ylabel(\"Average Delay (minutes)\")\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "    # 5. Delay trends over time (if date available)\n",
    "    if \"FL_DATE\" in df_orig_sample.columns and available_delays:\n",
    "        plt.subplot(2, 3, 6)\n",
    "        df_orig_sample[\"FL_DATE\"] = pd.to_datetime(df_orig_sample[\"FL_DATE\"], errors=\"coerce\")\n",
    "        df_orig_sample[\"month\"] = df_orig_sample[\"FL_DATE\"].dt.month\n",
    "\n",
    "        delay_by_month = df_orig_sample.groupby(\"month\")[available_delays[0]].mean()\n",
    "        delay_by_month.plot(kind=\"line\", marker=\"o\", color=\"darkred\")\n",
    "        plt.title(f\"Average {available_delays[0]} by Month\")\n",
    "        plt.xlabel(\"Month\")\n",
    "        plt.ylabel(\"Average Delay (minutes)\")\n",
    "        plt.xticks(range(1, 13))\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"eda_comparison_outputs/original_delay_analysis.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"Delay analysis completed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Correlation Analysis\n",
    "print(\"CORRELATION ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Get numeric columns for correlation\n",
    "numeric_cols = df_orig_sample.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if len(numeric_cols) > 2:\n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = df_orig_sample[numeric_cols].corr()\n",
    "\n",
    "    plt.figure(figsize=(15, 12))\n",
    "\n",
    "    # Full correlation heatmap\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.heatmap(corr_matrix, annot=False, cmap=\"RdBu_r\", center=0, square=True, cbar_kws={\"shrink\": 0.8})\n",
    "    plt.title(\"Full Correlation Matrix\")\n",
    "\n",
    "    # Focus on high correlations\n",
    "    plt.subplot(2, 2, 2)\n",
    "    # Get upper triangle of correlation matrix\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    corr_masked = corr_matrix.mask(mask)\n",
    "\n",
    "    # Find high correlations (|r| > 0.7)\n",
    "    high_corr = corr_masked[abs(corr_masked) > 0.7]\n",
    "    if not high_corr.empty:\n",
    "        sns.heatmap(high_corr, annot=True, cmap=\"RdBu_r\", center=0, square=True, cbar_kws={\"shrink\": 0.8}, fmt=\".2f\")\n",
    "        plt.title(\"High Correlations (|r| > 0.7)\")\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, \"No high correlations found\", ha=\"center\", va=\"center\", transform=plt.gca().transAxes)\n",
    "        plt.title(\"High Correlations (|r| > 0.7)\")\n",
    "\n",
    "    # Top correlations\n",
    "    plt.subplot(2, 2, 3)\n",
    "    # Flatten correlation matrix and get top correlations\n",
    "    corr_pairs = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i + 1, len(corr_matrix.columns)):\n",
    "            corr_pairs.append(\n",
    "                {\"var1\": corr_matrix.columns[i], \"var2\": corr_matrix.columns[j], \"correlation\": abs(corr_matrix.iloc[i, j])}\n",
    "            )\n",
    "\n",
    "    corr_df = pd.DataFrame(corr_pairs)\n",
    "    top_correlations = corr_df.nlargest(10, \"correlation\")\n",
    "\n",
    "    y_pos = np.arange(len(top_correlations))\n",
    "    plt.barh(y_pos, top_correlations[\"correlation\"].values, color=\"lightblue\")\n",
    "    plt.yticks(y_pos, [f\"{row['var1']} - {row['var2']}\" for _, row in top_correlations.iterrows()])\n",
    "    plt.xlabel(\"Absolute Correlation\")\n",
    "    plt.title(\"Top 10 Variable Correlations\")\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "    # Correlation with target variables (if available)\n",
    "    plt.subplot(2, 2, 4)\n",
    "    target_cols = [col for col in [\"DEP_DELAY\", \"ARR_DELAY\"] if col in numeric_cols]\n",
    "    if target_cols:\n",
    "        target_corr = corr_matrix[target_cols].drop(target_cols, axis=0)\n",
    "        target_corr = target_corr.abs().max(axis=1).sort_values(ascending=True).tail(15)\n",
    "\n",
    "        plt.barh(range(len(target_corr)), target_corr.values, color=\"lightcoral\")\n",
    "        plt.yticks(range(len(target_corr)), target_corr.index)\n",
    "        plt.xlabel(\"Max Absolute Correlation with Delay Variables\")\n",
    "        plt.title(\"Variables Most Correlated with Delays\")\n",
    "        plt.gca().invert_yaxis()\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, \"No delay variables found\", ha=\"center\", va=\"center\", transform=plt.gca().transAxes)\n",
    "        plt.title(\"Correlation with Target Variables\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"eda_comparison_outputs/original_correlation_analysis.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"Correlation analysis completed and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Original Dataset Summary Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive statistics summary for original dataset\n",
    "print(\"ORIGINAL DATASET SUMMARY STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Basic dataset info\n",
    "print(f\"Dataset Shape: {df_original.shape[0]:,} rows × {df_original.shape[1]} columns\")\n",
    "print(f\"Memory Usage: {df_original.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
    "print(\n",
    "    f\"Date Range: {df_original['FL_DATE'].min()} to {df_original['FL_DATE'].max()}\"\n",
    "    if \"FL_DATE\" in df_original.columns\n",
    "    else \"No date column found\"\n",
    ")\n",
    "\n",
    "# Data quality summary\n",
    "print(f\"\\nDATA QUALITY SUMMARY:\")\n",
    "print(f\"Total Missing Values: {df_original.isnull().sum().sum():,}\")\n",
    "print(f\"Columns with Missing Values: {(df_original.isnull().sum() > 0).sum()}\")\n",
    "print(f\"Duplicate Rows: {df_original.duplicated().sum():,}\")\n",
    "\n",
    "# Numeric columns summary\n",
    "numeric_cols = df_original.select_dtypes(include=[np.number]).columns\n",
    "print(f\"\\nNUMERIC COLUMNS ({len(numeric_cols)}):\")\n",
    "for col in numeric_cols:\n",
    "    stats = df_original[col].describe()\n",
    "    missing_pct = (df_original[col].isnull().sum() / len(df_original)) * 100\n",
    "    print(f\"  {col}:\")\n",
    "    print(f\"    Range: {stats['min']:.1f} to {stats['max']:.1f}\")\n",
    "    print(f\"    Mean: {stats['mean']:.1f}, Median: {stats['50%']:.1f}\")\n",
    "    print(f\"    Missing: {missing_pct:.1f}%\")\n",
    "\n",
    "# Categorical columns summary\n",
    "categorical_cols = df_original.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "print(f\"\\nCATEGORICAL COLUMNS ({len(categorical_cols)}):\")\n",
    "for col in categorical_cols:\n",
    "    unique_count = df_original[col].nunique()\n",
    "    missing_pct = (df_original[col].isnull().sum() / len(df_original)) * 100\n",
    "    print(f\"  {col}: {unique_count:,} unique values, {missing_pct:.1f}% missing\")\n",
    "\n",
    "# Flight patterns summary\n",
    "if \"AIRLINE\" in df_original.columns:\n",
    "    print(f\"\\nFLIGHT PATTERNS:\")\n",
    "    print(f\"  Airlines: {df_original['AIRLINE'].nunique():,}\")\n",
    "    print(f\"  Origin Airports: {df_original['ORIGIN'].nunique():,}\")\n",
    "    print(f\"  Destination Airports: {df_original['DEST'].nunique():,}\")\n",
    "    print(f\"  Unique Routes: {df_original.groupby(['ORIGIN', 'DEST']).size().shape[0]:,}\")\n",
    "\n",
    "# Delay summary\n",
    "if \"DEP_DELAY\" in df_original.columns:\n",
    "    dep_delay_stats = df_original[\"DEP_DELAY\"].describe()\n",
    "    print(f\"\\nDEPARTURE DELAY SUMMARY:\")\n",
    "    print(f\"  On-time flights: {(df_original['DEP_DELAY'] <= 0).sum():,} ({(df_original['DEP_DELAY'] <= 0).mean()*100:.1f}%)\")\n",
    "    print(f\"  Delayed flights: {(df_original['DEP_DELAY'] > 0).sum():,} ({(df_original['DEP_DELAY'] > 0).mean()*100:.1f}%)\")\n",
    "    print(f\"  Average delay: {dep_delay_stats['mean']:.1f} minutes\")\n",
    "    print(f\"  Median delay: {dep_delay_stats['50%']:.1f} minutes\")\n",
    "\n",
    "if \"ARR_DELAY\" in df_original.columns:\n",
    "    arr_delay_stats = df_original[\"ARR_DELAY\"].describe()\n",
    "    print(f\"\\nARRIVAL DELAY SUMMARY:\")\n",
    "    print(f\"  On-time arrivals: {(df_original['ARR_DELAY'] <= 0).sum():,} ({(df_original['ARR_DELAY'] <= 0).mean()*100:.1f}%)\")\n",
    "    print(f\"  Delayed arrivals: {(df_original['ARR_DELAY'] > 0).sum():,} ({(df_original['ARR_DELAY'] > 0).mean()*100:.1f}%)\")\n",
    "    print(f\"  Average delay: {arr_delay_stats['mean']:.1f} minutes\")\n",
    "    print(f\"  Median delay: {arr_delay_stats['50%']:.1f} minutes\")\n",
    "\n",
    "# Save summary statistics to file\n",
    "summary_stats = {\n",
    "    \"dataset_info\": {\n",
    "        \"shape\": list(df_original.shape),\n",
    "        \"memory_mb\": float(df_original.memory_usage(deep=True).sum() / (1024**2)),\n",
    "        \"date_range\": {\n",
    "            \"start\": str(df_original[\"FL_DATE\"].min()) if \"FL_DATE\" in df_original.columns else None,\n",
    "            \"end\": str(df_original[\"FL_DATE\"].max()) if \"FL_DATE\" in df_original.columns else None,\n",
    "        },\n",
    "    },\n",
    "    \"data_quality\": {\n",
    "        \"total_missing\": int(df_original.isnull().sum().sum()),\n",
    "        \"columns_with_missing\": int((df_original.isnull().sum() > 0).sum()),\n",
    "        \"duplicate_rows\": int(df_original.duplicated().sum()),\n",
    "    },\n",
    "    \"numeric_columns\": {\n",
    "        col: {\n",
    "            \"count\": int(df_original[col].count()),\n",
    "            \"mean\": float(df_original[col].mean()),\n",
    "            \"median\": float(df_original[col].median()),\n",
    "            \"std\": float(df_original[col].std()),\n",
    "            \"min\": float(df_original[col].min()),\n",
    "            \"max\": float(df_original[col].max()),\n",
    "            \"missing_pct\": float((df_original[col].isnull().sum() / len(df_original)) * 100),\n",
    "        }\n",
    "        for col in numeric_cols\n",
    "    },\n",
    "    \"categorical_columns\": {\n",
    "        col: {\n",
    "            \"unique_count\": int(df_original[col].nunique()),\n",
    "            \"missing_pct\": float((df_original[col].isnull().sum() / len(df_original)) * 100),\n",
    "        }\n",
    "        for col in categorical_cols\n",
    "    },\n",
    "}\n",
    "\n",
    "# Add flight patterns if available\n",
    "if \"AIRLINE\" in df_original.columns:\n",
    "    summary_stats[\"flight_patterns\"] = {\n",
    "        \"airlines\": int(df_original[\"AIRLINE\"].nunique()),\n",
    "        \"origin_airports\": int(df_original[\"ORIGIN\"].nunique()),\n",
    "        \"destination_airports\": int(df_original[\"DEST\"].nunique()),\n",
    "        \"unique_routes\": int(df_original.groupby([\"ORIGIN\", \"DEST\"]).size().shape[0]),\n",
    "    }\n",
    "\n",
    "# Add delay statistics if available\n",
    "if \"DEP_DELAY\" in df_original.columns:\n",
    "    summary_stats[\"departure_delay\"] = {\n",
    "        \"on_time_count\": int((df_original[\"DEP_DELAY\"] <= 0).sum()),\n",
    "        \"on_time_pct\": float((df_original[\"DEP_DELAY\"] <= 0).mean() * 100),\n",
    "        \"delayed_count\": int((df_original[\"DEP_DELAY\"] > 0).sum()),\n",
    "        \"delayed_pct\": float((df_original[\"DEP_DELAY\"] > 0).mean() * 100),\n",
    "        \"mean_delay\": float(df_original[\"DEP_DELAY\"].mean()),\n",
    "        \"median_delay\": float(df_original[\"DEP_DELAY\"].median()),\n",
    "    }\n",
    "\n",
    "if \"ARR_DELAY\" in df_original.columns:\n",
    "    summary_stats[\"arrival_delay\"] = {\n",
    "        \"on_time_count\": int((df_original[\"ARR_DELAY\"] <= 0).sum()),\n",
    "        \"on_time_pct\": float((df_original[\"ARR_DELAY\"] <= 0).mean() * 100),\n",
    "        \"delayed_count\": int((df_original[\"ARR_DELAY\"] > 0).sum()),\n",
    "        \"delayed_pct\": float((df_original[\"ARR_DELAY\"] > 0).mean() * 100),\n",
    "        \"mean_delay\": float(df_original[\"ARR_DELAY\"].mean()),\n",
    "        \"median_delay\": float(df_original[\"ARR_DELAY\"].median()),\n",
    "    }\n",
    "\n",
    "# Save to JSON file\n",
    "with open(\"eda_comparison_outputs/original_dataset_summary.json\", \"w\") as f:\n",
    "    json.dump(summary_stats, f, indent=2)\n",
    "\n",
    "print(f\"\\n\\nOriginal dataset summary statistics saved to: eda_comparison_outputs/original_dataset_summary.json\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
