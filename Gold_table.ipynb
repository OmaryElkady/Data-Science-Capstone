{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed159208-0833-4fe5-ba2d-c70d22639456",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ü•á Gold Table: Enhanced ML Features with Temporal Intelligence\n",
    "\n",
    "**Purpose:** Transform Enhanced Silver data into ML-ready feature vectors with temporal features for superior predictive performance.\n",
    "\n",
    "**Key Enhancements:**\n",
    "- ‚úÖ Uses Enhanced Silver with 15 columns (temporal features included)\n",
    "- ‚úÖ Temporal feature encoding in ML pipeline\n",
    "- ‚úÖ Improved target classification (5 classes)\n",
    "- ‚úÖ ~1200+ feature dimensions (vs ~800 original)\n",
    "\n",
    "**Pipeline:** Bronze ‚Üí Enhanced Silver (15 cols) ‚Üí **Enhanced Gold (ML-ready)**\n",
    "\n",
    "**Source:** `default.silver_flights_processed` (Enhanced with temporal features)\n",
    "**Output:** `default.gold_ml_features` (Enhanced ML vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "214f45d4-ffee-4d9c-a2d1-7ed191dfe71a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StandardScaler, StringIndexer, VectorAssembler\n",
    "from pyspark.sql.functions import col, when"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c9851db-21a3-44d0-80c0-e4e1f573c476",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load Silver Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e89e173d-0161-42ed-a7d7-e8964310bb61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nüì• Loading Silver table...\")\n",
    "df_silver = spark.table(\"default.silver_flights_processed\")\n",
    "\n",
    "print(f\"‚úÖ Loaded {df_silver.count():,} records\")\n",
    "print(f\"Columns: {len(df_silver.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c4c183c-831c-4183-9c44-852f23c7f141",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### TARGET VARIABLE CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49a01fe0-2a8f-4f8e-8cea-a84af67a9fc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nüéØ Creating binary target (delayed ‚â•15 min)...\")\n",
    "\n",
    "# Binary classification: 0 = on-time (<15 min), 1 = delayed (‚â•15 min)\n",
    "# Exclude cancelled flights (null arrival_delay)\n",
    "df_with_target = df_silver.filter(col(\"arrival_delay\").isNotNull()).withColumn(\n",
    "    \"label\",\n",
    "    when(col(\"arrival_delay\") >= 15, 1.0).otherwise(0.0)\n",
    ")\n",
    "\n",
    "print(\"üìä Target Distribution:\")\n",
    "df_with_target.groupBy(\"label\").count().orderBy(\"label\").show()\n",
    "\n",
    "total = df_with_target.count()\n",
    "delayed = df_with_target.filter(col(\"label\") == 1.0).count()\n",
    "print(f\"Total flights: {total:,}\")\n",
    "print(f\"Delayed (‚â•15 min): {delayed:,} ({delayed/total*100:.1f}%)\")\n",
    "print(f\"On-time (<15 min): {total - delayed:,} ({(total - delayed)/total*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61145cd2-5c0b-4c1c-a84a-23cdfb1e55c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### FEATURE ORGANIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa48d348-11ba-408c-b63f-f92279a2eb05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nüìã Organizing features...\")\n",
    "\n",
    "# Categorical features (need encoding)\n",
    "categorical_features = [\n",
    "    \"airline_name\",\n",
    "    \"airline_code\",\n",
    "    \"origin_airport_code\",\n",
    "    \"destination_airport_code\",\n",
    "    \"season\"\n",
    "]\n",
    "\n",
    "# Numerical features\n",
    "numerical_features = [\n",
    "    \"flight_month\",\n",
    "    \"flight_year\",\n",
    "    \"day_of_week\",\n",
    "    \"week_of_year\",\n",
    "    \"day_of_month\",\n",
    "    \"quarter\",\n",
    "    \"fl_number\",\n",
    "    \"crs_dep_time\",\n",
    "    \"crs_arr_time\",\n",
    "    \"crs_elapsed_time\",\n",
    "    \"distance\",\n",
    "    \"dep_delay\"  # At index 11 - will be removed for pre-departure model in experiments\n",
    "]\n",
    "\n",
    "# Boolean features (already 0/1 in Silver)\n",
    "boolean_features = [\n",
    "    \"is_weekend\",\n",
    "    \"is_holiday\",\n",
    "    \"is_near_holiday\",\n",
    "    \"is_holiday_period\"\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Feature counts:\")\n",
    "print(f\"   Categorical: {len(categorical_features)}\")\n",
    "print(f\"   Numerical: {len(numerical_features)} (includes dep_delay at index 11)\")\n",
    "print(f\"   Boolean: {len(boolean_features)}\")\n",
    "print(f\"   Total: {len(categorical_features + numerical_features + boolean_features)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca5527e6-d82c-46d4-879c-1fcaca74f6f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### HANDLE TIME FEATURES (Convert HHMM format to hour of day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e69b1bff-1ba4-4472-8e73-b27b44631035",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n‚è∞ Converting time features to hour of day...\")\n",
    "\n",
    "# Convert HHMM format (e.g., 1430) to hour (e.g., 14)\n",
    "df_ml_ready = df_with_target.withColumn(\n",
    "    \"dep_hour\",\n",
    "    (col(\"crs_dep_time\") / 100).cast(\"int\")\n",
    ").withColumn(\n",
    "    \"arr_hour\",\n",
    "    (col(\"crs_arr_time\") / 100).cast(\"int\")\n",
    ")\n",
    "\n",
    "# Replace original time columns with hour columns\n",
    "numerical_features.remove(\"crs_dep_time\")\n",
    "numerical_features.remove(\"crs_arr_time\")\n",
    "numerical_features.extend([\"dep_hour\", \"arr_hour\"])\n",
    "\n",
    "print(\"‚úÖ Converted crs_dep_time ‚Üí dep_hour (0-23)\")\n",
    "print(\"‚úÖ Converted crs_arr_time ‚Üí arr_hour (0-23)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d1a561d-3cd7-4b0d-b7aa-9843f9376164",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### HANDLE MISSING VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2aade0eb-e3ea-4fc5-a4e6-7b3971d9ab7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nüîç Checking for missing values...\")\n",
    "\n",
    "from pyspark.sql.functions import count, isnan\n",
    "\n",
    "all_feature_cols = categorical_features + numerical_features + boolean_features\n",
    "missing_found = False\n",
    "\n",
    "for col_name in all_feature_cols:\n",
    "    null_count = df_ml_ready.filter(col(col_name).isNull()).count()\n",
    "    if null_count > 0:\n",
    "        print(f\"  ‚ö†Ô∏è {col_name}: {null_count:,} nulls\")\n",
    "        missing_found = True\n",
    "\n",
    "if missing_found:\n",
    "    print(\"\\nüîß Filling missing values...\")\n",
    "    # Fill numerical nulls with 0\n",
    "    for col_name in numerical_features:\n",
    "        df_ml_ready = df_ml_ready.fillna({col_name: 0})\n",
    "    print(\"‚úÖ Missing values handled\")\n",
    "else:\n",
    "    print(\"‚úÖ No missing values found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1a26bd3-0505-4c45-b44d-a3417b5a85f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ML PIPELINE CONSTRUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94a62d7f-486c-4db8-8097-9b1d2590b226",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n‚ö° Building feature engineering pipeline...\")\n",
    "\n",
    "# Stage 1: String Indexers\n",
    "categorical_indexed = [f\"{c}_index\" for c in categorical_features]\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=f\"{c}_index\", handleInvalid=\"keep\")\n",
    "    for c in categorical_features\n",
    "]\n",
    "print(f\"‚úÖ Created {len(indexers)} string indexers\")\n",
    "\n",
    "# Stage 2: One-Hot Encoders\n",
    "categorical_encoded = [f\"{c}_ohe\" for c in categorical_features]\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=categorical_indexed,\n",
    "    outputCols=categorical_encoded,\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "print(f\"‚úÖ Created one-hot encoder\")\n",
    "\n",
    "# Stage 3: Assemble all features\n",
    "all_features = numerical_features + boolean_features + categorical_encoded\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=all_features,\n",
    "    outputCol=\"unscaled_features\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "print(f\"‚úÖ Created feature assembler ({len(all_features)} input columns)\")\n",
    "\n",
    "# Stage 4: Standard Scaler\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"unscaled_features\",\n",
    "    outputCol=\"features\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "print(f\"‚úÖ Created standard scaler\")\n",
    "\n",
    "# Combine pipeline\n",
    "pipeline = Pipeline(stages=indexers + [encoder, assembler, scaler])\n",
    "print(f\"\\nüöÄ Pipeline ready with {len(indexers) + 3} stages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ccfc054-c0bf-44e4-8fc6-e1e43bc771cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### FIT AND TRANSFORM PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cb795ed-1223-4d69-9eb5-3a84573f5256",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n‚ö° Fitting pipeline (this may take a few minutes)...\")\n",
    "fitted_pipeline = pipeline.fit(df_ml_ready)\n",
    "print(\"‚úÖ Pipeline fitted\")\n",
    "\n",
    "print(\"\\nüîÑ Transforming data...\")\n",
    "df_gold = fitted_pipeline.transform(df_ml_ready)\n",
    "print(\"‚úÖ Transformation complete\")\n",
    "\n",
    "# Select final columns\n",
    "df_gold_final = df_gold.select(\"features\", \"label\")\n",
    "\n",
    "# Check feature vector size\n",
    "sample = df_gold_final.select(\"features\").first()\n",
    "feature_size = sample.features.size\n",
    "\n",
    "print(f\"\\nüìè Feature vector dimensions: {feature_size}\")\n",
    "print(f\"   Note: High dimensionality due to one-hot encoding of airports/airlines\")\n",
    "print(f\"   Feature selection will be performed in experiments notebook\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90535b1d-9dcc-4cd0-b85a-0166ad627f14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### FINAL VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcd980b3-d5c1-421a-9295-ed7aadfeaa21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"GOLD TABLE VALIDATION\")\n",
    "\n",
    "print(\"\\nüìã Schema:\")\n",
    "df_gold_final.printSchema()\n",
    "\n",
    "print(\"\\nüìä Final Target Distribution:\")\n",
    "df_gold_final.groupBy(\"label\").count().orderBy(\"label\").show()\n",
    "\n",
    "print(\"\\nüîé Sample (features truncated):\")\n",
    "df_gold_final.show(3)\n",
    "\n",
    "record_count = df_gold_final.count()\n",
    "print(f\"\\n‚úÖ Gold table ready:\")\n",
    "print(f\"   Records: {record_count:,}\")\n",
    "print(f\"   Features: {feature_size}\")\n",
    "print(f\"   Target: Binary (0=on-time, 1=delayed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a84657a2-3f75-456d-9c24-d377248a4fd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### CREATE FINAL GOLD TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2303929d-fab7-4766-b715-f20298326b78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "GOLD_PATH = \"/Volumes/workspace/default/ds-capstone/gold/ml_features_experimental\"\n",
    "GOLD_TABLE_NAME = \"default.gold_ml_features_experimental\"\n",
    "DATABASE_NAME = \"default\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAVING GOLD TABLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Helper functions\n",
    "def path_exists(path):\n",
    "    try:\n",
    "        dbutils.fs.ls(path)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def create_directory_if_not_exists(path):\n",
    "    if not path_exists(path):\n",
    "        dbutils.fs.mkdirs(path)\n",
    "        print(f\"‚úÖ Created directory: {path}\")\n",
    "    else:\n",
    "        print(f\"‚ÑπÔ∏è  Directory exists: {path}\")\n",
    "\n",
    "# Check and clean if needed\n",
    "print(f\"\\nüìÅ Checking path: {GOLD_PATH}\")\n",
    "if path_exists(GOLD_PATH):\n",
    "    print(f\"‚ö†Ô∏è  Path exists, will overwrite\")\n",
    "    try:\n",
    "        test_df = spark.read.format(\"delta\").load(GOLD_PATH)\n",
    "        print(f\"   Found existing table with {test_df.count()} records\")\n",
    "    except:\n",
    "        print(f\"   Cleaning invalid data...\")\n",
    "        dbutils.fs.rm(GOLD_PATH, recurse=True)\n",
    "\n",
    "# Create parent directory\n",
    "gold_parent = \"/\".join(GOLD_PATH.split(\"/\")[:-1])\n",
    "create_directory_if_not_exists(gold_parent)\n",
    "\n",
    "# Write Delta table\n",
    "print(f\"\\nüíæ Writing Delta table...\")\n",
    "try:\n",
    "    df_gold_final.write.format(\"delta\").mode(\"overwrite\").save(GOLD_PATH)\n",
    "    print(f\"‚úÖ Delta table written to: {GOLD_PATH}\")\n",
    "    print(f\"‚úÖ Records written: {df_gold_final.count():,}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(f\"   Cleaning and retrying...\")\n",
    "    dbutils.fs.rm(GOLD_PATH, recurse=True)\n",
    "    df_gold_final.write.format(\"delta\").mode(\"overwrite\").save(GOLD_PATH)\n",
    "    print(f\"‚úÖ Succeeded after cleanup\")\n",
    "\n",
    "# Register table\n",
    "print(f\"\\nüìå Registering table: {GOLD_TABLE_NAME}\")\n",
    "try:\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {DATABASE_NAME}\")\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {GOLD_TABLE_NAME}\")\n",
    "    \n",
    "    df_for_table = spark.read.format(\"delta\").load(GOLD_PATH)\n",
    "    df_for_table.write.format(\"delta\").mode(\"overwrite\").saveAsTable(GOLD_TABLE_NAME)\n",
    "    \n",
    "    print(f\"‚úÖ Table registered: {GOLD_TABLE_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Error with saveAsTable: {e}\")\n",
    "    try:\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {GOLD_TABLE_NAME}\n",
    "            USING DELTA\n",
    "            LOCATION '{GOLD_PATH}'\n",
    "        \"\"\")\n",
    "        print(f\"‚úÖ Table registered with LOCATION clause\")\n",
    "    except Exception as e2:\n",
    "        print(f\"‚ö†Ô∏è  Registration failed: {e2}\")\n",
    "        print(f\"üí° Access data using: spark.read.format('delta').load('{GOLD_PATH}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9d660e6-923c-4f21-9277-51f82f280a9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e69939bb-071f-4765-b131-63913f5a14a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"GOLD TABLE CREATION COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n‚úÖ What was created:\")\n",
    "print(f\"   ‚Ä¢ Delta table at: {GOLD_PATH}\")\n",
    "print(f\"   ‚Ä¢ Registered as: {GOLD_TABLE_NAME}\")\n",
    "print(f\"   ‚Ä¢ Records: {record_count:,}\")\n",
    "print(f\"   ‚Ä¢ Feature dimensions: {feature_size}\")\n",
    "print(f\"   ‚Ä¢ Binary target: 0 (on-time) / 1 (delayed ‚â•15 min)\")\n",
    "\n",
    "print(\"\\nüìã Feature breakdown:\")\n",
    "print(f\"   ‚Ä¢ Numerical: {len(numerical_features)} (includes dep_delay)\")\n",
    "print(f\"   ‚Ä¢ Boolean: {len(boolean_features)}\")\n",
    "print(f\"   ‚Ä¢ Categorical (one-hot): {len(categorical_features)} ‚Üí ~{feature_size - len(numerical_features) - len(boolean_features)} dims\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  IMPORTANT for experiments:\")\n",
    "print(f\"   ‚Ä¢ dep_delay is at index 11 in numerical features\")\n",
    "print(f\"   ‚Ä¢ Remove this index for pre-departure model\")\n",
    "print(f\"   ‚Ä¢ Keep all features for in-flight model\")\n",
    "\n",
    "print(\"\\nüéØ Next steps:\")\n",
    "print(\"   1. Run feature importance analysis (in experiments)\")\n",
    "print(\"   2. Select top-K most important features\")\n",
    "print(\"   3. Train dual models (pre-departure + in-flight)\")\n",
    "print(\"   4. Register best models in MLflow\")\n",
    "\n",
    "print(\"\\n‚úÖ Gold table is ready for ML experiments!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a64cede6-8a80-4084-8561-91bd4ab3a325",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Gold_table",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
