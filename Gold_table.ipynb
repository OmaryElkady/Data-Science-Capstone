{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed159208-0833-4fe5-ba2d-c70d22639456",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸ¥‡ Gold Table: Enhanced ML Features with Temporal Intelligence\n",
    "\n",
    "**Purpose:** Transform Enhanced Silver data into ML-ready feature vectors with temporal features for superior predictive performance.\n",
    "\n",
    "**Key Enhancements:**\n",
    "- âœ… Uses Enhanced Silver with 15 columns (temporal features included)\n",
    "- âœ… Temporal feature encoding in ML pipeline\n",
    "- âœ… Improved target classification (5 classes)\n",
    "- âœ… ~1200+ feature dimensions (vs ~800 original)\n",
    "\n",
    "**Pipeline:** Bronze â†’ Enhanced Silver (15 cols) â†’ **Enhanced Gold (ML-ready)**\n",
    "\n",
    "**Source:** `default.silver_flights_processed` (Enhanced with temporal features)\n",
    "**Output:** `default.gold_ml_features` (Enhanced ML vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1014f477-b484-4c06-91b6-c7c280e269da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Loading Enhanced Silver table with temporal features...\nâœ… Loaded Enhanced Silver: 2,520,650 flights\nColumns: 17 (enhanced with temporal features)\n\nðŸ“‹ Enhanced Silver Schema:\nroot\n |-- airline_name: string (nullable = true)\n |-- airline_code: string (nullable = true)\n |-- origin_airport_code: string (nullable = true)\n |-- destination_airport_code: string (nullable = true)\n |-- arrival_delay: double (nullable = true)\n |-- flight_date: date (nullable = true)\n |-- flight_month: integer (nullable = true)\n |-- flight_year: integer (nullable = true)\n |-- day_of_week: integer (nullable = true)\n |-- week_of_year: integer (nullable = true)\n |-- day_of_month: integer (nullable = true)\n |-- is_weekend: boolean (nullable = true)\n |-- is_holiday: boolean (nullable = true)\n |-- is_near_holiday: boolean (nullable = true)\n |-- is_holiday_period: boolean (nullable = true)\n |-- season: string (nullable = true)\n |-- quarter: integer (nullable = true)\n\n\nðŸ”Ž Sample Enhanced Silver (showing temporal features):\n+---------------------+-----------+-------------+-----------+----------+----------+------+\n|airline_name         |flight_date|arrival_delay|day_of_week|is_weekend|is_holiday|season|\n+---------------------+-----------+-------------+-----------+----------+----------+------+\n|United Air Lines Inc.|2019-01-09 |-14.0        |4          |false     |false     |Winter|\n|Delta Air Lines Inc. |2022-11-19 |-5.0         |7          |true      |false     |Fall  |\n|United Air Lines Inc.|2022-07-22 |0.0          |6          |false     |false     |Summer|\n+---------------------+-----------+-------------+-----------+----------+----------+------+\nonly showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“¥ Loading Enhanced Silver table with temporal features...\")\n",
    "\n",
    "# Load the enhanced Silver table (now has 15 columns with temporal features)\n",
    "df_silver = spark.table(\"default.silver_flights_processed\")\n",
    "\n",
    "print(f\"âœ… Loaded Enhanced Silver: {df_silver.count():,} flights\")\n",
    "print(f\"Columns: {len(df_silver.columns)} (enhanced with temporal features)\")\n",
    "\n",
    "# Show what we're working with\n",
    "print(\"\\nðŸ“‹ Enhanced Silver Schema:\")\n",
    "df_silver.printSchema()\n",
    "\n",
    "# Show sample to confirm temporal features are present\n",
    "print(\"\\nðŸ”Ž Sample Enhanced Silver (showing temporal features):\")\n",
    "df_silver.select(\"airline_name\", \"flight_date\", \"arrival_delay\", \"day_of_week\", \"is_weekend\", \"is_holiday\", \"season\").show(\n",
    "    3, truncate=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5adcbcaa-5cc9-4fde-ad79-a352f279404c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /databricks/python3/lib/python3.12/site-packages (4.0.0+databricks.connect.17.2.2)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a16f0303-46ca-4976-adc4-01705bca72e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Enhanced Gold ML pipeline imports loaded\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StandardScaler, StringIndexer, VectorAssembler\n",
    "from pyspark.sql.functions import col, count, isnan, when\n",
    "\n",
    "print(\"âœ… Enhanced Gold ML pipeline imports loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c6da287-83db-433b-b781-94c268b6a0c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver = spark.table(\"default.silver_flights_processed\") # Added because it was missing and wasn't allowing to run further notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "351a9f23-d4b7-4211-88cc-ab430f42069e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Creating enhanced arrival status with 5 categories...\n\nðŸ“Š Enhanced Target Distribution (5 categories):\n+------------------+-------+\n|    arrival_status|  count|\n+------------------+-------+\n|             Early|1262663|\n|           On-Time| 711524|\n|           Delayed| 324074|\n|  Severely Delayed| 165718|\n|Cancelled/Diverted|  56671|\n+------------------+-------+\n\n\nTotal flights for ML: 2,520,650\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸŽ¯ Creating enhanced arrival status with 5 categories...\")\n",
    "\n",
    "# Enhanced target classification (more granular than original 4 classes)\n",
    "df_with_status = df_silver.withColumn(\n",
    "    \"arrival_status\",\n",
    "    when(col(\"arrival_delay\").isNull(), \"Cancelled/Diverted\")\n",
    "    .when(col(\"arrival_delay\") >= 60, \"Severely Delayed\")  # New: 60+ min\n",
    "    .when(col(\"arrival_delay\") >= 15, \"Delayed\")  # 15-59 min\n",
    "    .when(col(\"arrival_delay\") >= -5, \"On-Time\")  # -5 to 14 min\n",
    "    .when(col(\"arrival_delay\") < -5, \"Early\")  # <-5 min\n",
    "    .otherwise(\"Unknown\"),\n",
    ")\n",
    "\n",
    "# Show enhanced target distribution\n",
    "print(\"\\nðŸ“Š Enhanced Target Distribution (5 categories):\")\n",
    "target_counts = df_with_status.groupBy(\"arrival_status\").count().orderBy(\"count\", ascending=False)\n",
    "target_counts.show()\n",
    "\n",
    "total_flights = df_with_status.count()\n",
    "print(f\"\\nTotal flights for ML: {total_flights:,}\")\n",
    "\n",
    "df_silver = df_with_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35efbaf3-5de2-4e7d-a202-60d70f29e58c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Enhanced Feature Engineering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e590b8a-145b-4570-8882-84a6da6092da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Organizing enhanced features for ML pipeline...\nðŸ“Š Enhanced Feature Inventory:\n  Categorical features: 5 (includes season)\n  Numerical features: 6 (includes 4 temporal)\n  Boolean features: 4 (all temporal)\n  Total input features: 15\n  Original Gold had: 4 input features\n  Enhancement added: 11 temporal features\n\nðŸ”„ Converting boolean features to numerical...\nâœ… Boolean features converted to 0/1 for ML compatibility\n"
     ]
    }
   ],
   "source": [
    "print(\"Organizing enhanced features for ML pipeline...\")\n",
    "\n",
    "# Categorical features (need string indexing + one-hot encoding)\n",
    "categorical_features = [\n",
    "    \"airline_name\",\n",
    "    \"airline_code\",\n",
    "    \"origin_airport_code\",\n",
    "    \"destination_airport_code\",\n",
    "    \"season\",  # New temporal categorical feature\n",
    "]\n",
    "\n",
    "# Numerical features (can use directly, including temporal)\n",
    "numerical_features = [\n",
    "    \"flight_month\",\n",
    "    \"flight_year\",\n",
    "    \"day_of_week\",  # New temporal\n",
    "    \"week_of_year\",  # New temporal\n",
    "    \"day_of_month\",  # New temporal\n",
    "    \"quarter\",  # New temporal\n",
    "]\n",
    "\n",
    "# Boolean features (convert to 0/1, all new temporal)\n",
    "boolean_features = [\n",
    "    \"is_weekend\",  # New temporal\n",
    "    \"is_holiday\",  # New temporal\n",
    "    \"is_near_holiday\",  # New temporal\n",
    "    \"is_holiday_period\",  # New temporal\n",
    "]\n",
    "\n",
    "print(f\"ðŸ“Š Enhanced Feature Inventory:\")\n",
    "print(f\"  Categorical features: {len(categorical_features)} (includes season)\")\n",
    "print(f\"  Numerical features: {len(numerical_features)} (includes 4 temporal)\")\n",
    "print(f\"  Boolean features: {len(boolean_features)} (all temporal)\")\n",
    "print(f\"  Total input features: {len(categorical_features + numerical_features + boolean_features)}\")\n",
    "print(f\"  Original Gold had: 4 input features\")\n",
    "print(f\"  Enhancement added: {len(numerical_features + boolean_features + ['season'])} temporal features\")\n",
    "\n",
    "# Convert boolean features to numerical (0/1)\n",
    "print(f\"\\nðŸ”„ Converting boolean features to numerical...\")\n",
    "df_ml_ready = df_silver\n",
    "for bool_col in boolean_features:\n",
    "    df_ml_ready = df_ml_ready.withColumn(bool_col, when(col(bool_col) == True, 1.0).otherwise(0.0))\n",
    "\n",
    "print(\"âœ… Boolean features converted to 0/1 for ML compatibility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "deafebaf-4fea-4bdc-b141-9d3c2e412dad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Validating enhanced target distribution...\nSample Enhanced Target (arrival_delay â†’ arrival_status):\n+-------------+----------------+----------+----------+------+\n|arrival_delay|  arrival_status|is_weekend|is_holiday|season|\n+-------------+----------------+----------+----------+------+\n|        -14.0|           Early|       0.0|       0.0|Winter|\n|         -5.0|         On-Time|       1.0|       0.0|  Fall|\n|          0.0|         On-Time|       0.0|       0.0|Summer|\n|         24.0|         Delayed|       0.0|       0.0|Spring|\n|        141.0|Severely Delayed|       0.0|       0.0|Summer|\n|        -29.0|           Early|       1.0|       0.0|Summer|\n|         23.0|         Delayed|       0.0|       0.0|Summer|\n|        -11.0|           Early|       1.0|       0.0|Winter|\n|         60.0|Severely Delayed|       0.0|       0.0|Summer|\n|          1.0|         On-Time|       0.0|       0.0|Summer|\n+-------------+----------------+----------+----------+------+\nonly showing top 10 rows\n\nðŸ“Š Final Enhanced Target Distribution:\n+------------------+-------+\n|    arrival_status|  count|\n+------------------+-------+\n|             Early|1262663|\n|           On-Time| 711524|\n|           Delayed| 324074|\n|  Severely Delayed| 165718|\n|Cancelled/Diverted|  56671|\n+------------------+-------+\n\nEnhanced dataset ready for ML pipeline\nTotal features available: 15\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ” Validating enhanced target distribution...\")\n",
    "\n",
    "# Show enhanced target sample\n",
    "print(\"Sample Enhanced Target (arrival_delay â†’ arrival_status):\")\n",
    "df_ml_ready.select(\"arrival_delay\", \"arrival_status\", \"is_weekend\", \"is_holiday\", \"season\").show(10)\n",
    "\n",
    "# Show final target distribution for ML\n",
    "print(\"\\nðŸ“Š Final Enhanced Target Distribution:\")\n",
    "df_ml_ready.groupBy(\"arrival_status\").count().orderBy(\"count\", ascending=False).show()\n",
    "\n",
    "print(f\"Enhanced dataset ready for ML pipeline\")\n",
    "print(f\"Total features available: {len(categorical_features + numerical_features + boolean_features)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e29f82a4-1030-41c9-88ac-0cd5e1a31e98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing value counts per column (before ML pipeline):\n+-------------+------------+------------+-------------------+------------------------+-----------+------------+-----------+-----------+------------+------------+----------+----------+---------------+-----------------+------+-------+--------------+\n|arrival_delay|airline_name|airline_code|origin_airport_code|destination_airport_code|flight_date|flight_month|flight_year|day_of_week|week_of_year|day_of_month|is_weekend|is_holiday|is_near_holiday|is_holiday_period|season|quarter|arrival_status|\n+-------------+------------+------------+-------------------+------------------------+-----------+------------+-----------+-----------+------------+------------+----------+----------+---------------+-----------------+------+-------+--------------+\n|        56671|           0|           0|                  0|                       0|          0|           0|          0|          0|           0|           0|         0|         0|              0|                0|     0|      0|             0|\n+-------------+------------+------------+-------------------+------------------------+-----------+------------+-----------+-----------+------------+------------+----------+----------+---------------+-----------------+------+-------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Get all column names\n",
    "all_columns = df_silver.columns\n",
    "\n",
    "# Find just the float/double columns\n",
    "numeric_cols = [c_name for (c_name, c_type) in df_silver.dtypes if c_type in (\"float\", \"double\")]\n",
    "\n",
    "# Get all *other* columns\n",
    "other_cols = [c_name for c_name in all_columns if c_name not in numeric_cols]\n",
    "\n",
    "# Create expressions for numeric columns (check for null OR nan)\n",
    "numeric_expressions = [count(when(col(c).isNull() | isnan(c), c)).alias(c) for c in numeric_cols]\n",
    "\n",
    "# Create expressions for all other columns (check for null only)\n",
    "other_expressions = [count(when(col(c).isNull(), c)).alias(c) for c in other_cols]\n",
    "\n",
    "# Combine the lists of expressions\n",
    "all_expressions = numeric_expressions + other_expressions\n",
    "\n",
    "# Run the counts and show the result\n",
    "print(\"Missing value counts per column (before ML pipeline):\")\n",
    "df_silver.select(*all_expressions).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30c3beee-e11c-4d47-9155-0a4af8d61a40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš¡ Building Enhanced ML Pipeline with Temporal Features...\nâœ… All enhanced features found. Building ML pipeline...\nðŸ“ String indexers created for 5 categorical features\nðŸŽ¯ One-hot encoder created for categorical features\nðŸ·ï¸ Target label indexer created (5-class classification)\nðŸ”§ Feature assembler created with 15 feature columns:\n   â€¢ Numerical (temporal): 6\n   â€¢ Boolean (temporal): 4\n   â€¢ Categorical (one-hot): 5\nâš–ï¸ Standard scaler configured (mean=0, std=1)\n\nðŸš€ Enhanced ML Pipeline Ready!\nTotal pipeline stages: 9\nExpected output: ~810+ feature dimensions\n"
     ]
    }
   ],
   "source": [
    "print(\"âš¡ Building Enhanced ML Pipeline with Temporal Features...\")\n",
    "\n",
    "# Updated feature requirements (now includes temporal features)\n",
    "required_cols = [\n",
    "    \"airline_name\",\n",
    "    \"airline_code\",\n",
    "    \"origin_airport_code\",\n",
    "    \"destination_airport_code\",\n",
    "    \"flight_month\",\n",
    "    \"flight_year\",\n",
    "    \"arrival_status\",\n",
    "    # New temporal features\n",
    "    \"day_of_week\",\n",
    "    \"week_of_year\",\n",
    "    \"day_of_month\",\n",
    "    \"quarter\",\n",
    "    \"season\",\n",
    "    \"is_weekend\",\n",
    "    \"is_holiday\",\n",
    "    \"is_near_holiday\",\n",
    "    \"is_holiday_period\",\n",
    "]\n",
    "\n",
    "missing_cols = [c for c in required_cols if c not in df_ml_ready.columns]\n",
    "\n",
    "if missing_cols:\n",
    "    print(f\"âŒ ERROR: Missing columns for enhanced pipeline: {missing_cols}\")\n",
    "else:\n",
    "    print(\"âœ… All enhanced features found. Building ML pipeline...\")\n",
    "\n",
    "    # --- Enhanced ML Pipeline Stages ---\n",
    "\n",
    "    # Stage 1: String Indexers for categorical features (including temporal)\n",
    "    categorical_cols_indexed = [f\"{c}_index\" for c in categorical_features]\n",
    "\n",
    "    indexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_index\", handleInvalid=\"keep\") for c in categorical_features]\n",
    "\n",
    "    print(f\"ðŸ“ String indexers created for {len(categorical_features)} categorical features\")\n",
    "\n",
    "    # Stage 2: One-Hot Encoders\n",
    "    categorical_cols_ohe = [f\"{c}_ohe\" for c in categorical_features]\n",
    "\n",
    "    ohe_encoder = OneHotEncoder(inputCols=categorical_cols_indexed, outputCols=categorical_cols_ohe, handleInvalid=\"keep\")\n",
    "\n",
    "    print(f\"ðŸŽ¯ One-hot encoder created for categorical features\")\n",
    "\n",
    "    # Stage 3: Target Label Indexer\n",
    "    label_indexer = StringIndexer(inputCol=\"arrival_status\", outputCol=\"label\", handleInvalid=\"keep\")\n",
    "\n",
    "    print(\"ðŸ·ï¸ Target label indexer created (5-class classification)\")\n",
    "\n",
    "    # Stage 4: Feature Vector Assembler (Enhanced with temporal)\n",
    "    all_feature_cols = numerical_features + boolean_features + categorical_cols_ohe\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=all_feature_cols, outputCol=\"unscaled_features\", handleInvalid=\"skip\")\n",
    "\n",
    "    print(f\"ðŸ”§ Feature assembler created with {len(all_feature_cols)} feature columns:\")\n",
    "    print(f\"   â€¢ Numerical (temporal): {len(numerical_features)}\")\n",
    "    print(f\"   â€¢ Boolean (temporal): {len(boolean_features)}\")\n",
    "    print(f\"   â€¢ Categorical (one-hot): {len(categorical_cols_ohe)}\")\n",
    "\n",
    "    # Stage 5: Standard Scaler\n",
    "    scaler = StandardScaler(inputCol=\"unscaled_features\", outputCol=\"features\", withStd=True, withMean=True)\n",
    "\n",
    "    print(\"âš–ï¸ Standard scaler configured (mean=0, std=1)\")\n",
    "\n",
    "    # Combine all stages into enhanced pipeline\n",
    "    enhanced_pipeline_stages = indexers + [ohe_encoder, label_indexer, assembler, scaler]\n",
    "    enhanced_pipeline = Pipeline(stages=enhanced_pipeline_stages)\n",
    "\n",
    "    print(f\"\\nðŸš€ Enhanced ML Pipeline Ready!\")\n",
    "    print(f\"Total pipeline stages: {len(enhanced_pipeline_stages)}\")\n",
    "    print(f\"Expected output: ~{800 + len(numerical_features + boolean_features)}+ feature dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12c5a456-c91a-4008-85f7-fd98573b4c08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## âš¡ Pipeline Execution & Gold Table Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "271242f0-b92a-4b9d-bae0-462377404eba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš¡ Fitting Enhanced ML Pipeline...\nThis may take a few minutes for large datasets with temporal features...\nâœ… Enhanced pipeline fitted successfully\n\nðŸ”„ Transforming data through enhanced pipeline...\nâœ… Enhanced transformation completed\nOutput columns: 31\nEnhanced feature vector size: 813 dimensions\nOriginal Gold had: ~800 dimensions\nEnhancement added: ~13 temporal dimensions\n"
     ]
    }
   ],
   "source": [
    "print(\"âš¡ Fitting Enhanced ML Pipeline...\")\n",
    "print(\"This may take a few minutes for large datasets with temporal features...\")\n",
    "\n",
    "# Fit the enhanced pipeline on our temporal-enriched data\n",
    "fitted_pipeline = enhanced_pipeline.fit(df_ml_ready)\n",
    "print(\"âœ… Enhanced pipeline fitted successfully\")\n",
    "\n",
    "# Transform the data through the complete ML pipeline\n",
    "print(\"\\nðŸ”„ Transforming data through enhanced pipeline...\")\n",
    "df_gold_ml = fitted_pipeline.transform(df_ml_ready)\n",
    "\n",
    "print(\"âœ… Enhanced transformation completed\")\n",
    "print(f\"Output columns: {len(df_gold_ml.columns)}\")\n",
    "\n",
    "# Check the feature vector size\n",
    "sample_features = df_gold_ml.select(\"features\").first().features\n",
    "print(f\"Enhanced feature vector size: {sample_features.size} dimensions\")\n",
    "print(f\"Original Gold had: ~800 dimensions\")\n",
    "print(f\"Enhancement added: ~{sample_features.size - 800} temporal dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82136dd2-4209-4ce4-a8af-9211dc1db418",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¥‡ Creating Final Enhanced Gold Table...\nâœ… Enhanced Gold table created\nFinal columns: 2 (features, label)\nTotal rows: 2,520,650\n\nðŸ“‹ Enhanced Gold Schema:\nroot\n |-- features: vectorudt (nullable = true)\n |-- label: double (nullable = false)\n\n\nðŸ“Š Enhanced Target Label Distribution:\n+-----+-------+\n|label|  count|\n+-----+-------+\n|  0.0|1262663|\n|  1.0| 711524|\n|  2.0| 324074|\n|  3.0| 165718|\n|  4.0|  56671|\n+-----+-------+\n\n\nðŸ”Ž Enhanced Gold Sample (truncated):\n+--------------------+-----+\n|            features|label|\n+--------------------+-----+\n|[-1.6049222153511...|  0.0|\n|[1.41913690323475...|  1.0|\n|[0.20951325580039...|  1.0|\n+--------------------+-----+\nonly showing top 3 rows\n\nðŸŽ¯ Enhanced Gold ML Table Summary:\n  â€¢ Feature dimensions: 813\n  â€¢ Target classes: 5 (vs 4 original)\n  â€¢ Temporal features: 11\n  â€¢ Total enhancement: 13+ new dimensions\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ¥‡ Creating Final Enhanced Gold Table...\")\n",
    "\n",
    "# Select only the ML-ready features and labels\n",
    "df_gold_ml_final = df_gold_ml.select(\"features\", \"label\")\n",
    "\n",
    "print(f\"âœ… Enhanced Gold table created\")\n",
    "print(f\"Final columns: {len(df_gold_ml_final.columns)} (features, label)\")\n",
    "print(f\"Total rows: {df_gold_ml_final.count():,}\")\n",
    "\n",
    "# Validate the enhanced output\n",
    "print(\"\\nðŸ“‹ Enhanced Gold Schema:\")\n",
    "df_gold_ml_final.printSchema()\n",
    "\n",
    "# Show label distribution for enhanced target\n",
    "print(\"\\nðŸ“Š Enhanced Target Label Distribution:\")\n",
    "df_gold_ml_final.groupBy(\"label\").count().orderBy(\"label\").show()\n",
    "\n",
    "# Show sample (careful with large vectors)\n",
    "print(\"\\nðŸ”Ž Enhanced Gold Sample (truncated):\")\n",
    "df_gold_ml_final.show(3, truncate=True)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Enhanced Gold ML Table Summary:\")\n",
    "print(f\"  â€¢ Feature dimensions: {sample_features.size}\")\n",
    "print(f\"  â€¢ Target classes: 5 (vs 4 original)\")\n",
    "print(f\"  â€¢ Temporal features: {len(numerical_features + boolean_features + ['season'])}\")\n",
    "print(f\"  â€¢ Total enhancement: {sample_features.size - 800}+ new dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f135ed5-3d62-4ac2-b233-9ee359e8dd69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This is the final DataFrame from your ML pipeline\n",
    "assert df_gold_ml_final, \"The DataFrame 'df_gold_ml_final' does not exist.\"\n",
    "\n",
    "# Define the paths for your new Gold ML table\n",
    "GOLD_ML_PATH = \"/Volumes/workspace/default/ds-capstone/gold/ml_features_experimental\" # Once again, this path was updated to follow the \"ds-capstone\" convention established in the Bronze_table notebook. The old path was as follows \"/Volumes/workspace/default/ds_capstone/gold/ml_features_experimental\"\n",
    "GOLD_ML_TABLE_NAME = \"default.gold_ml_features_experimental\"\n",
    "DATABASE_NAME = \"default\"\n",
    "\n",
    "assert DATABASE_NAME, \"DATABASE_NAME is not defined.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c46aaca5-32d3-43f0-82e5-07917cacd1c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def path_exists(path):\n",
    "    \"\"\"Check if a path exists\"\"\"\n",
    "    try:\n",
    "        dbutils.fs.ls(path)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def create_directory_if_not_exists(path):\n",
    "    \"\"\"Create directory if it doesn't exist\"\"\"\n",
    "    if not path_exists(path):\n",
    "        dbutils.fs.mkdirs(path)\n",
    "        print(f\"âœ… Created directory: {path}\")\n",
    "    else:\n",
    "        print(f\"â„¹ï¸  Directory already exists: {path}\")\n",
    "\n",
    "\n",
    "def table_exists(table_name):\n",
    "    \"\"\"Check if a table exists\"\"\"\n",
    "    try:\n",
    "        spark.table(table_name)\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29d5efc8-e4c6-423a-bf77-ce3839b9113a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nðŸ“ Checking Gold ML path: /Volumes/workspace/default/ds-capstone/gold/ml_features_experimental\nâœ… Path is clear, ready to create new table\nâœ… Created directory: /Volumes/workspace/default/ds-capstone/gold\n\nðŸ’¾ Writing Gold ML Delta table...\nâœ… Delta table written to: /Volumes/workspace/default/ds-capstone/gold/ml_features_experimental\nâœ… Records written: 2,520,650\n\nðŸ“Œ Registering Delta table as: default.gold_ml_features_experimental\nâœ… Database 'default' ready\n   Dropped existing table (if any)\nâœ… Table registered successfully as 'default.gold_ml_features_experimental'!\n"
     ]
    }
   ],
   "source": [
    "# Check if Gold ML path exists and clean if needed\n",
    "print(f\"\\nðŸ“ Checking Gold ML path: {GOLD_ML_PATH}\")\n",
    "if path_exists(GOLD_ML_PATH):\n",
    "    print(f\"âš ï¸  Path already exists. Checking if it's a valid Delta table...\")\n",
    "    try:\n",
    "        # Try to read as Delta\n",
    "        test_df = spark.read.format(\"delta\").load(GOLD_ML_PATH)\n",
    "        print(f\"âœ… Valid Delta table found with {test_df.count()} records\")\n",
    "        print(f\"ðŸ’¡ Will overwrite existing table\")\n",
    "    except:\n",
    "        print(f\"âš ï¸  Path exists but is not a valid Delta table\")\n",
    "        print(f\"ðŸ§¹ Cleaning up old data...\")\n",
    "        dbutils.fs.rm(GOLD_ML_PATH, recurse=True)\n",
    "        print(f\"âœ… Old data removed\")\n",
    "else:\n",
    "    print(f\"âœ… Path is clear, ready to create new table\")\n",
    "\n",
    "# Create parent directory if needed\n",
    "gold_ml_parent = \"/\".join(GOLD_ML_PATH.split(\"/\")[:-1])\n",
    "create_directory_if_not_exists(gold_ml_parent)\n",
    "\n",
    "# Write Gold ML Delta table\n",
    "print(f\"\\nðŸ’¾ Writing Gold ML Delta table...\")\n",
    "try:\n",
    "    df_gold_ml_final.write.format(\"delta\").mode(\"overwrite\").save(GOLD_ML_PATH)\n",
    "    print(f\"âœ… Delta table written to: {GOLD_ML_PATH}\")\n",
    "    print(f\"âœ… Records written: {df_gold_ml_final.count():,}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ERROR: Could not write Delta table\")\n",
    "    print(f\"   Error: {str(e)}\")\n",
    "    print(f\"\\nðŸ’¡ Trying to clean and retry...\")\n",
    "    try:\n",
    "        dbutils.fs.rm(GOLD_ML_PATH, recurse=True)\n",
    "        df_gold_ml_final.write.format(\"delta\").mode(\"overwrite\").save(GOLD_ML_PATH)\n",
    "        print(f\"âœ… Successfully wrote Delta table after cleanup\")\n",
    "    except Exception as e2:\n",
    "        print(f\"âŒ Still failed: {str(e2)}\")\n",
    "        raise\n",
    "\n",
    "# Registering Delta table\n",
    "print(f\"\\nðŸ“Œ Registering Delta table as: {GOLD_ML_TABLE_NAME}\")\n",
    "try:\n",
    "    # Ensure database exists\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {DATABASE_NAME}\")\n",
    "    print(f\"âœ… Database '{DATABASE_NAME}' ready\")\n",
    "\n",
    "    # Drop table if it exists (to avoid conflicts)\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {GOLD_ML_TABLE_NAME}\")\n",
    "    print(f\"   Dropped existing table (if any)\")\n",
    "\n",
    "    # Create managed table\n",
    "    df_for_table = spark.read.format(\"delta\").load(GOLD_ML_PATH)\n",
    "    df_for_table.write.format(\"delta\").mode(\"overwrite\").saveAsTable(GOLD_ML_TABLE_NAME)\n",
    "\n",
    "    print(f\"âœ… Table registered successfully as '{GOLD_ML_TABLE_NAME}'!\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Could not create table with saveAsTable, trying alternative method...\")\n",
    "    try:\n",
    "        # Alternative: Create external table with explicit LOCATION\n",
    "        spark.sql(\n",
    "            f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {GOLD_ML_TABLE_NAME}\n",
    "            USING DELTA\n",
    "            LOCATION '{GOLD_ML_PATH}'\n",
    "        \"\"\"\n",
    "        )\n",
    "        print(f\"âœ… Table registered with LOCATION clause!\")\n",
    "    except Exception as e2:\n",
    "        print(f\"âš ï¸  Table registration failed: {str(e2)}\")\n",
    "        print(f\"ðŸ’¡ You can still access the data directly using:\")\n",
    "        print(f\"   spark.read.format('delta').load('{GOLD_ML_PATH}')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81c1458f-4e9b-4838-a266-3ebc9fc0a34c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Gold_table",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
